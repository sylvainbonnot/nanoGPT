{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try nanogpt on a prepared set of snips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nanogpt model is trained on a txt file made of all precomputed snips (using freesound dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 27,005\n",
      "all the unique characters: \n",
      " 0123456789:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāĂăĄąĆćĈĉĊċČčĎďđĒēĔĕĖėĘęĚěĜĝĞğĠġĢ\n",
      "vocab size: 163\n",
      "train has 24,304 tokens\n",
      "val has 2,701 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/slang_data/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60971) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_slang.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-slang'\n",
      "eval_interval = 250  # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10  # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False  # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'slang_data'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "# block_size = 256  # context of up to 256 previous characters\n",
      "block_size = 64  # context of up to 256 previous characters\n",
      "\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000  # make equal to max_iters usually\n",
      "min_lr = 1e-4  # learning_rate / 10 usually\n",
      "beta2 = 0.99  # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100  # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cpu\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 163 (inside data/slang_data/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.81M\n",
      "/Users/sylvain/opt/anaconda3/envs/drill_app_2/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "num decayed parameter tensors: 18, with 815,488 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.4568, val loss 4.4264\n",
      "iter 0: loss 4.4039, time 1112.46ms, mfu -100.00%\n",
      "iter 1: loss 4.5097, time 100.47ms, mfu -100.00%\n",
      "iter 2: loss 4.4477, time 109.28ms, mfu -100.00%\n",
      "iter 3: loss 4.3403, time 110.68ms, mfu -100.00%\n",
      "iter 4: loss 4.0500, time 107.60ms, mfu -100.00%\n",
      "iter 5: loss 4.2295, time 201.33ms, mfu 0.01%\n",
      "iter 6: loss 3.8197, time 98.42ms, mfu 0.01%\n",
      "iter 7: loss 3.5720, time 96.59ms, mfu 0.01%\n",
      "iter 8: loss 3.4856, time 98.54ms, mfu 0.01%\n",
      "iter 9: loss 3.4265, time 98.82ms, mfu 0.01%\n",
      "iter 10: loss 3.6590, time 96.39ms, mfu 0.01%\n",
      "iter 11: loss 3.4997, time 96.97ms, mfu 0.01%\n",
      "iter 12: loss 3.1218, time 96.88ms, mfu 0.01%\n",
      "iter 13: loss 3.3050, time 99.29ms, mfu 0.01%\n",
      "iter 14: loss 3.3066, time 102.02ms, mfu 0.01%\n",
      "iter 15: loss 3.4389, time 105.27ms, mfu 0.01%\n",
      "iter 16: loss 3.4334, time 106.29ms, mfu 0.01%\n",
      "iter 17: loss 3.2857, time 106.18ms, mfu 0.01%\n",
      "iter 18: loss 3.3097, time 105.25ms, mfu 0.01%\n",
      "iter 19: loss 3.3077, time 108.33ms, mfu 0.01%\n",
      "iter 20: loss 3.2989, time 104.89ms, mfu 0.01%\n",
      "iter 21: loss 3.4322, time 105.78ms, mfu 0.01%\n",
      "iter 22: loss 3.0788, time 105.34ms, mfu 0.01%\n",
      "iter 23: loss 3.2907, time 104.26ms, mfu 0.01%\n",
      "iter 24: loss 3.2328, time 138.82ms, mfu 0.01%\n",
      "iter 25: loss 3.0771, time 103.80ms, mfu 0.01%\n",
      "iter 26: loss 2.7966, time 96.30ms, mfu 0.01%\n",
      "iter 27: loss 3.4411, time 97.26ms, mfu 0.01%\n",
      "iter 28: loss 3.3199, time 98.07ms, mfu 0.01%\n",
      "iter 29: loss 3.2287, time 96.90ms, mfu 0.01%\n",
      "iter 30: loss 3.3237, time 96.07ms, mfu 0.01%\n",
      "iter 31: loss 3.0379, time 95.85ms, mfu 0.01%\n",
      "iter 32: loss 2.8126, time 94.86ms, mfu 0.01%\n",
      "iter 33: loss 3.0286, time 96.84ms, mfu 0.01%\n",
      "iter 34: loss 2.2421, time 92.80ms, mfu 0.01%\n",
      "iter 35: loss 3.0501, time 94.39ms, mfu 0.01%\n",
      "iter 36: loss 2.9993, time 95.77ms, mfu 0.01%\n",
      "iter 37: loss 3.1309, time 95.95ms, mfu 0.01%\n",
      "iter 38: loss 3.1843, time 98.95ms, mfu 0.01%\n",
      "iter 39: loss 3.0132, time 96.62ms, mfu 0.01%\n",
      "iter 40: loss 2.6327, time 95.46ms, mfu 0.01%\n",
      "iter 41: loss 2.9592, time 95.70ms, mfu 0.01%\n",
      "iter 42: loss 2.8798, time 97.29ms, mfu 0.01%\n",
      "iter 43: loss 2.6553, time 98.43ms, mfu 0.01%\n",
      "iter 44: loss 2.8876, time 105.04ms, mfu 0.01%\n",
      "iter 45: loss 2.2460, time 105.71ms, mfu 0.01%\n",
      "iter 46: loss 2.8589, time 106.39ms, mfu 0.01%\n",
      "iter 47: loss 2.0976, time 119.45ms, mfu 0.01%\n",
      "iter 48: loss 2.9959, time 103.23ms, mfu 0.01%\n",
      "iter 49: loss 2.0772, time 103.95ms, mfu 0.01%\n",
      "iter 50: loss 2.5137, time 117.29ms, mfu 0.01%\n",
      "iter 51: loss 2.1163, time 100.58ms, mfu 0.01%\n",
      "iter 52: loss 2.8854, time 96.32ms, mfu 0.01%\n",
      "iter 53: loss 2.2618, time 94.15ms, mfu 0.01%\n",
      "iter 54: loss 2.4369, time 93.56ms, mfu 0.01%\n",
      "iter 55: loss 1.3822, time 103.91ms, mfu 0.01%\n",
      "iter 56: loss 2.0732, time 107.74ms, mfu 0.01%\n",
      "iter 57: loss 1.4962, time 110.32ms, mfu 0.01%\n",
      "iter 58: loss 1.4188, time 105.56ms, mfu 0.01%\n",
      "iter 59: loss 1.8064, time 97.60ms, mfu 0.01%\n",
      "iter 60: loss 2.0457, time 96.74ms, mfu 0.01%\n",
      "iter 61: loss 2.0069, time 93.84ms, mfu 0.01%\n",
      "iter 62: loss 2.3562, time 95.85ms, mfu 0.01%\n",
      "iter 63: loss 2.0879, time 94.32ms, mfu 0.01%\n",
      "iter 64: loss 1.8370, time 93.24ms, mfu 0.01%\n",
      "iter 65: loss 1.5710, time 96.25ms, mfu 0.01%\n",
      "iter 66: loss 1.7301, time 95.94ms, mfu 0.01%\n",
      "iter 67: loss 2.4762, time 96.71ms, mfu 0.01%\n",
      "iter 68: loss 1.9817, time 98.23ms, mfu 0.01%\n",
      "iter 69: loss 1.7727, time 96.25ms, mfu 0.01%\n",
      "iter 70: loss 1.2838, time 95.40ms, mfu 0.01%\n",
      "iter 71: loss 1.6979, time 96.43ms, mfu 0.01%\n",
      "iter 72: loss 1.5520, time 95.73ms, mfu 0.01%\n",
      "iter 73: loss 1.8817, time 162.17ms, mfu 0.01%\n",
      "iter 74: loss 1.9986, time 95.86ms, mfu 0.01%\n",
      "iter 75: loss 1.8144, time 96.04ms, mfu 0.01%\n",
      "iter 76: loss 2.2545, time 97.24ms, mfu 0.01%\n",
      "iter 77: loss 1.7999, time 96.11ms, mfu 0.01%\n",
      "iter 78: loss 1.4891, time 96.45ms, mfu 0.01%\n",
      "iter 79: loss 2.0978, time 97.40ms, mfu 0.01%\n",
      "iter 80: loss 2.0499, time 96.41ms, mfu 0.01%\n",
      "iter 81: loss 2.2775, time 94.53ms, mfu 0.01%\n",
      "iter 82: loss 1.6599, time 96.09ms, mfu 0.01%\n",
      "iter 83: loss 1.3301, time 94.63ms, mfu 0.01%\n",
      "iter 84: loss 1.6605, time 94.51ms, mfu 0.01%\n",
      "iter 85: loss 1.9396, time 96.55ms, mfu 0.01%\n",
      "iter 86: loss 1.4859, time 95.02ms, mfu 0.01%\n",
      "iter 87: loss 1.5711, time 95.65ms, mfu 0.01%\n",
      "iter 88: loss 1.8453, time 95.80ms, mfu 0.01%\n",
      "iter 89: loss 1.1325, time 96.58ms, mfu 0.01%\n",
      "iter 90: loss 1.1115, time 94.83ms, mfu 0.01%\n",
      "iter 91: loss 1.5518, time 95.49ms, mfu 0.01%\n",
      "iter 92: loss 1.3161, time 95.41ms, mfu 0.01%\n",
      "iter 93: loss 1.8214, time 99.35ms, mfu 0.01%\n",
      "iter 94: loss 1.7261, time 95.38ms, mfu 0.01%\n",
      "iter 95: loss 1.7131, time 96.64ms, mfu 0.01%\n",
      "iter 96: loss 1.3714, time 96.01ms, mfu 0.01%\n",
      "iter 97: loss 1.9366, time 97.11ms, mfu 0.01%\n",
      "iter 98: loss 1.4948, time 96.61ms, mfu 0.01%\n",
      "iter 99: loss 1.4566, time 98.11ms, mfu 0.01%\n",
      "iter 100: loss 2.4455, time 99.23ms, mfu 0.01%\n",
      "iter 101: loss 1.6526, time 102.04ms, mfu 0.01%\n",
      "iter 102: loss 1.5268, time 105.45ms, mfu 0.01%\n",
      "iter 103: loss 1.4840, time 106.71ms, mfu 0.01%\n",
      "iter 104: loss 1.6572, time 106.44ms, mfu 0.01%\n",
      "iter 105: loss 1.4428, time 107.99ms, mfu 0.01%\n",
      "iter 106: loss 0.8432, time 106.09ms, mfu 0.01%\n",
      "iter 107: loss 1.4028, time 105.06ms, mfu 0.01%\n",
      "iter 108: loss 0.7569, time 107.97ms, mfu 0.01%\n",
      "iter 109: loss 1.5566, time 105.26ms, mfu 0.01%\n",
      "iter 110: loss 1.7345, time 108.05ms, mfu 0.01%\n",
      "iter 111: loss 1.0239, time 105.05ms, mfu 0.01%\n",
      "iter 112: loss 1.2566, time 104.67ms, mfu 0.01%\n",
      "iter 113: loss 1.1222, time 105.94ms, mfu 0.01%\n",
      "iter 114: loss 1.8041, time 106.14ms, mfu 0.01%\n",
      "iter 115: loss 1.4289, time 113.41ms, mfu 0.01%\n",
      "iter 116: loss 1.3615, time 108.04ms, mfu 0.01%\n",
      "iter 117: loss 1.6006, time 106.50ms, mfu 0.01%\n",
      "iter 118: loss 1.2359, time 99.59ms, mfu 0.01%\n",
      "iter 119: loss 1.1487, time 98.43ms, mfu 0.01%\n",
      "iter 120: loss 1.4970, time 96.92ms, mfu 0.01%\n",
      "iter 121: loss 1.1938, time 94.67ms, mfu 0.01%\n",
      "iter 122: loss 1.0302, time 95.87ms, mfu 0.01%\n",
      "iter 123: loss 1.1113, time 97.36ms, mfu 0.01%\n",
      "iter 124: loss 1.3709, time 96.06ms, mfu 0.01%\n",
      "iter 125: loss 1.3039, time 97.17ms, mfu 0.01%\n",
      "iter 126: loss 2.1349, time 95.61ms, mfu 0.01%\n",
      "iter 127: loss 1.5060, time 115.88ms, mfu 0.01%\n",
      "iter 128: loss 1.8595, time 96.43ms, mfu 0.01%\n",
      "iter 129: loss 1.4940, time 95.95ms, mfu 0.01%\n",
      "iter 130: loss 0.9528, time 97.62ms, mfu 0.01%\n",
      "iter 131: loss 1.6721, time 96.51ms, mfu 0.01%\n",
      "iter 132: loss 1.3096, time 98.16ms, mfu 0.01%\n",
      "iter 133: loss 1.0553, time 97.77ms, mfu 0.01%\n",
      "iter 134: loss 1.3919, time 98.91ms, mfu 0.01%\n",
      "iter 135: loss 1.7423, time 98.61ms, mfu 0.01%\n",
      "iter 136: loss 1.1266, time 100.32ms, mfu 0.01%\n",
      "iter 137: loss 1.3572, time 105.60ms, mfu 0.01%\n",
      "iter 138: loss 1.1672, time 107.35ms, mfu 0.01%\n",
      "iter 139: loss 0.9175, time 106.60ms, mfu 0.01%\n",
      "iter 140: loss 1.5347, time 107.61ms, mfu 0.01%\n",
      "iter 141: loss 0.7791, time 104.98ms, mfu 0.01%\n",
      "iter 142: loss 0.9002, time 104.77ms, mfu 0.01%\n",
      "iter 143: loss 1.2107, time 104.68ms, mfu 0.01%\n",
      "iter 144: loss 0.6921, time 105.26ms, mfu 0.01%\n",
      "iter 145: loss 1.7345, time 117.84ms, mfu 0.01%\n",
      "iter 146: loss 0.9168, time 102.45ms, mfu 0.01%\n",
      "iter 147: loss 0.9286, time 98.58ms, mfu 0.01%\n",
      "iter 148: loss 1.1086, time 97.79ms, mfu 0.01%\n",
      "iter 149: loss 1.0679, time 97.45ms, mfu 0.01%\n",
      "iter 150: loss 1.6602, time 99.09ms, mfu 0.01%\n",
      "iter 151: loss 1.3394, time 98.88ms, mfu 0.01%\n",
      "iter 152: loss 1.0941, time 193.03ms, mfu 0.01%\n",
      "iter 153: loss 0.7113, time 102.59ms, mfu 0.01%\n",
      "iter 154: loss 0.9765, time 98.69ms, mfu 0.01%\n",
      "iter 155: loss 1.1503, time 98.12ms, mfu 0.01%\n",
      "iter 156: loss 1.6416, time 97.52ms, mfu 0.01%\n",
      "iter 157: loss 0.8694, time 97.45ms, mfu 0.01%\n",
      "iter 158: loss 1.3854, time 99.98ms, mfu 0.01%\n",
      "iter 159: loss 0.7736, time 100.40ms, mfu 0.01%\n",
      "iter 160: loss 1.1485, time 108.62ms, mfu 0.01%\n",
      "iter 161: loss 0.8386, time 106.54ms, mfu 0.01%\n",
      "iter 162: loss 1.2428, time 107.32ms, mfu 0.01%\n",
      "iter 163: loss 0.8206, time 104.14ms, mfu 0.01%\n",
      "iter 164: loss 0.9194, time 108.17ms, mfu 0.01%\n",
      "iter 165: loss 1.2210, time 99.61ms, mfu 0.01%\n",
      "iter 166: loss 1.0666, time 96.86ms, mfu 0.01%\n",
      "iter 167: loss 1.3311, time 95.28ms, mfu 0.01%\n",
      "iter 168: loss 1.3791, time 97.27ms, mfu 0.01%\n",
      "iter 169: loss 0.9361, time 95.63ms, mfu 0.01%\n",
      "iter 170: loss 1.8276, time 95.62ms, mfu 0.01%\n",
      "iter 171: loss 1.0230, time 94.79ms, mfu 0.01%\n",
      "iter 172: loss 1.2075, time 94.86ms, mfu 0.01%\n",
      "iter 173: loss 1.0033, time 95.39ms, mfu 0.01%\n",
      "iter 174: loss 0.9709, time 97.22ms, mfu 0.01%\n",
      "iter 175: loss 1.0956, time 95.00ms, mfu 0.01%\n",
      "iter 176: loss 1.1736, time 96.17ms, mfu 0.01%\n",
      "iter 177: loss 1.0334, time 96.26ms, mfu 0.01%\n",
      "iter 178: loss 1.1300, time 96.46ms, mfu 0.01%\n",
      "iter 179: loss 1.4910, time 94.68ms, mfu 0.01%\n",
      "iter 180: loss 1.0881, time 95.58ms, mfu 0.01%\n",
      "iter 181: loss 1.0059, time 95.43ms, mfu 0.01%\n",
      "iter 182: loss 1.2075, time 93.51ms, mfu 0.01%\n",
      "iter 183: loss 1.2048, time 93.42ms, mfu 0.01%\n",
      "iter 184: loss 0.9846, time 95.69ms, mfu 0.01%\n",
      "iter 185: loss 1.3803, time 94.20ms, mfu 0.01%\n",
      "iter 186: loss 1.1115, time 94.47ms, mfu 0.01%\n",
      "iter 187: loss 1.2301, time 96.69ms, mfu 0.01%\n",
      "iter 188: loss 1.1632, time 94.98ms, mfu 0.01%\n",
      "iter 189: loss 1.3369, time 95.21ms, mfu 0.01%\n",
      "iter 190: loss 1.2746, time 95.19ms, mfu 0.01%\n",
      "iter 191: loss 1.4578, time 95.98ms, mfu 0.01%\n",
      "iter 192: loss 0.9285, time 97.16ms, mfu 0.01%\n",
      "iter 193: loss 1.5680, time 93.67ms, mfu 0.01%\n",
      "iter 194: loss 1.2090, time 97.29ms, mfu 0.01%\n",
      "iter 195: loss 1.3186, time 96.14ms, mfu 0.01%\n",
      "iter 196: loss 0.8441, time 94.38ms, mfu 0.01%\n",
      "iter 197: loss 0.9466, time 95.64ms, mfu 0.01%\n",
      "iter 198: loss 0.6413, time 95.63ms, mfu 0.01%\n",
      "iter 199: loss 0.9822, time 95.53ms, mfu 0.01%\n",
      "iter 200: loss 1.0055, time 95.77ms, mfu 0.01%\n",
      "iter 201: loss 1.1316, time 94.95ms, mfu 0.01%\n",
      "iter 202: loss 1.3118, time 95.29ms, mfu 0.01%\n",
      "iter 203: loss 1.2339, time 93.34ms, mfu 0.01%\n",
      "iter 204: loss 0.7108, time 94.79ms, mfu 0.01%\n",
      "iter 205: loss 1.0978, time 96.24ms, mfu 0.01%\n",
      "iter 206: loss 1.1844, time 94.24ms, mfu 0.01%\n",
      "iter 207: loss 0.8770, time 94.68ms, mfu 0.01%\n",
      "iter 208: loss 0.9742, time 97.45ms, mfu 0.01%\n",
      "iter 209: loss 1.2598, time 95.22ms, mfu 0.01%\n",
      "iter 210: loss 0.8931, time 97.39ms, mfu 0.01%\n",
      "iter 211: loss 0.8826, time 93.42ms, mfu 0.01%\n",
      "iter 212: loss 1.0961, time 94.94ms, mfu 0.01%\n",
      "iter 213: loss 1.1376, time 95.65ms, mfu 0.01%\n",
      "iter 214: loss 1.6395, time 93.48ms, mfu 0.01%\n",
      "iter 215: loss 1.4509, time 95.79ms, mfu 0.01%\n",
      "iter 216: loss 1.2867, time 97.75ms, mfu 0.01%\n",
      "iter 217: loss 0.7640, time 94.52ms, mfu 0.01%\n",
      "iter 218: loss 1.0596, time 95.14ms, mfu 0.01%\n",
      "iter 219: loss 0.8633, time 97.20ms, mfu 0.01%\n",
      "iter 220: loss 0.8445, time 95.30ms, mfu 0.01%\n",
      "iter 221: loss 0.7291, time 96.54ms, mfu 0.01%\n",
      "iter 222: loss 0.8529, time 95.61ms, mfu 0.01%\n",
      "iter 223: loss 0.9475, time 93.83ms, mfu 0.01%\n",
      "iter 224: loss 1.6024, time 92.52ms, mfu 0.01%\n",
      "iter 225: loss 1.2100, time 92.74ms, mfu 0.01%\n",
      "iter 226: loss 0.8372, time 95.71ms, mfu 0.01%\n",
      "iter 227: loss 1.2575, time 94.67ms, mfu 0.01%\n",
      "iter 228: loss 0.9430, time 161.42ms, mfu 0.01%\n",
      "iter 229: loss 1.0320, time 97.13ms, mfu 0.01%\n",
      "iter 230: loss 1.0001, time 94.36ms, mfu 0.01%\n",
      "iter 231: loss 1.8343, time 95.39ms, mfu 0.01%\n",
      "iter 232: loss 1.1942, time 94.34ms, mfu 0.01%\n",
      "iter 233: loss 0.4432, time 95.45ms, mfu 0.01%\n",
      "iter 234: loss 1.1424, time 94.05ms, mfu 0.01%\n",
      "iter 235: loss 1.5052, time 93.41ms, mfu 0.01%\n",
      "iter 236: loss 1.1954, time 113.21ms, mfu 0.01%\n",
      "iter 237: loss 0.8654, time 97.60ms, mfu 0.01%\n",
      "iter 238: loss 0.9842, time 96.98ms, mfu 0.01%\n",
      "iter 239: loss 1.0915, time 95.28ms, mfu 0.01%\n",
      "iter 240: loss 0.9031, time 96.08ms, mfu 0.01%\n",
      "iter 241: loss 1.2013, time 103.89ms, mfu 0.01%\n",
      "iter 242: loss 0.9751, time 107.59ms, mfu 0.01%\n",
      "iter 243: loss 0.9452, time 104.76ms, mfu 0.01%\n",
      "iter 244: loss 0.9892, time 97.49ms, mfu 0.01%\n",
      "iter 245: loss 1.0517, time 93.85ms, mfu 0.01%\n",
      "iter 246: loss 0.9826, time 95.54ms, mfu 0.01%\n",
      "iter 247: loss 0.6252, time 99.29ms, mfu 0.01%\n",
      "iter 248: loss 1.0781, time 98.16ms, mfu 0.01%\n",
      "iter 249: loss 1.3543, time 96.06ms, mfu 0.01%\n",
      "step 250: train loss 1.0137, val loss 1.3619\n",
      "saving checkpoint to out-slang\n",
      "iter 250: loss 1.5078, time 1294.94ms, mfu 0.01%\n",
      "iter 251: loss 1.3143, time 103.57ms, mfu 0.01%\n",
      "iter 252: loss 0.9320, time 94.91ms, mfu 0.01%\n",
      "iter 253: loss 1.0700, time 94.29ms, mfu 0.01%\n",
      "iter 254: loss 0.8500, time 95.64ms, mfu 0.01%\n",
      "iter 255: loss 1.1821, time 130.47ms, mfu 0.01%\n",
      "iter 256: loss 1.0870, time 101.78ms, mfu 0.01%\n",
      "iter 257: loss 0.7942, time 96.53ms, mfu 0.01%\n",
      "iter 258: loss 1.6479, time 96.93ms, mfu 0.01%\n",
      "iter 259: loss 0.8799, time 103.34ms, mfu 0.01%\n",
      "iter 260: loss 0.9588, time 113.60ms, mfu 0.01%\n",
      "iter 261: loss 0.5355, time 104.63ms, mfu 0.01%\n",
      "iter 262: loss 1.2266, time 94.25ms, mfu 0.01%\n",
      "iter 263: loss 0.9612, time 95.04ms, mfu 0.01%\n",
      "iter 264: loss 0.9748, time 96.63ms, mfu 0.01%\n",
      "iter 265: loss 1.0012, time 98.25ms, mfu 0.01%\n",
      "iter 266: loss 0.9187, time 95.87ms, mfu 0.01%\n",
      "iter 267: loss 1.3963, time 95.04ms, mfu 0.01%\n",
      "iter 268: loss 1.0573, time 98.33ms, mfu 0.01%\n",
      "iter 269: loss 0.9579, time 107.75ms, mfu 0.01%\n",
      "iter 270: loss 0.9725, time 101.26ms, mfu 0.01%\n",
      "iter 271: loss 1.0745, time 102.34ms, mfu 0.01%\n",
      "iter 272: loss 1.0135, time 96.71ms, mfu 0.01%\n",
      "iter 273: loss 0.6690, time 95.19ms, mfu 0.01%\n",
      "iter 274: loss 1.0931, time 96.47ms, mfu 0.01%\n",
      "iter 275: loss 0.7977, time 95.54ms, mfu 0.01%\n",
      "iter 276: loss 1.3455, time 95.70ms, mfu 0.01%\n",
      "iter 277: loss 1.2516, time 96.23ms, mfu 0.01%\n",
      "iter 278: loss 1.2878, time 96.83ms, mfu 0.01%\n",
      "iter 279: loss 0.8736, time 96.40ms, mfu 0.01%\n",
      "iter 280: loss 1.1414, time 94.85ms, mfu 0.01%\n",
      "iter 281: loss 0.7845, time 96.10ms, mfu 0.01%\n",
      "iter 282: loss 0.9027, time 93.45ms, mfu 0.01%\n",
      "iter 283: loss 1.3572, time 92.74ms, mfu 0.01%\n",
      "iter 284: loss 1.1533, time 92.87ms, mfu 0.01%\n",
      "iter 285: loss 1.1989, time 161.79ms, mfu 0.01%\n",
      "iter 286: loss 1.4564, time 96.24ms, mfu 0.01%\n",
      "iter 287: loss 0.9748, time 93.89ms, mfu 0.01%\n",
      "iter 288: loss 0.8869, time 95.53ms, mfu 0.01%\n",
      "iter 289: loss 1.1907, time 94.04ms, mfu 0.01%\n",
      "iter 290: loss 0.9790, time 100.22ms, mfu 0.01%\n",
      "iter 291: loss 1.0597, time 100.44ms, mfu 0.01%\n",
      "iter 292: loss 0.9738, time 103.20ms, mfu 0.01%\n",
      "iter 293: loss 0.8906, time 106.20ms, mfu 0.01%\n",
      "iter 294: loss 1.1904, time 116.65ms, mfu 0.01%\n",
      "iter 295: loss 0.8603, time 114.12ms, mfu 0.01%\n",
      "iter 296: loss 0.7765, time 105.61ms, mfu 0.01%\n",
      "iter 297: loss 1.1549, time 106.25ms, mfu 0.01%\n",
      "iter 298: loss 1.5155, time 113.64ms, mfu 0.01%\n",
      "iter 299: loss 0.8825, time 112.98ms, mfu 0.01%\n",
      "iter 300: loss 1.1224, time 149.79ms, mfu 0.01%\n",
      "iter 301: loss 0.8793, time 117.42ms, mfu 0.01%\n",
      "iter 302: loss 0.8002, time 99.16ms, mfu 0.01%\n",
      "iter 303: loss 0.5927, time 96.09ms, mfu 0.01%\n",
      "iter 304: loss 1.0649, time 102.66ms, mfu 0.01%\n",
      "iter 305: loss 1.0067, time 106.62ms, mfu 0.01%\n",
      "iter 306: loss 0.9292, time 102.44ms, mfu 0.01%\n",
      "iter 307: loss 0.6613, time 102.06ms, mfu 0.01%\n",
      "iter 308: loss 1.0941, time 98.61ms, mfu 0.01%\n",
      "iter 309: loss 0.8467, time 97.60ms, mfu 0.01%\n",
      "iter 310: loss 1.1468, time 130.28ms, mfu 0.01%\n",
      "iter 311: loss 0.8134, time 103.81ms, mfu 0.01%\n",
      "iter 312: loss 0.8047, time 98.22ms, mfu 0.01%\n",
      "iter 313: loss 1.1395, time 100.46ms, mfu 0.01%\n",
      "iter 314: loss 1.0825, time 98.08ms, mfu 0.01%\n",
      "iter 315: loss 0.9301, time 112.84ms, mfu 0.01%\n",
      "iter 316: loss 1.3767, time 103.70ms, mfu 0.01%\n",
      "iter 317: loss 1.2460, time 101.69ms, mfu 0.01%\n",
      "iter 318: loss 0.8710, time 101.15ms, mfu 0.01%\n",
      "iter 319: loss 0.8934, time 101.94ms, mfu 0.01%\n",
      "iter 320: loss 1.0756, time 99.94ms, mfu 0.01%\n",
      "iter 321: loss 1.1598, time 98.20ms, mfu 0.01%\n",
      "iter 322: loss 0.5202, time 98.77ms, mfu 0.01%\n",
      "iter 323: loss 0.6628, time 105.26ms, mfu 0.01%\n",
      "iter 324: loss 0.8780, time 99.93ms, mfu 0.01%\n",
      "iter 325: loss 0.8230, time 100.10ms, mfu 0.01%\n",
      "iter 326: loss 0.3881, time 99.38ms, mfu 0.01%\n",
      "iter 327: loss 0.6455, time 99.03ms, mfu 0.01%\n",
      "iter 328: loss 1.0880, time 98.67ms, mfu 0.01%\n",
      "iter 329: loss 0.8842, time 98.83ms, mfu 0.01%\n",
      "iter 330: loss 1.4198, time 99.46ms, mfu 0.01%\n",
      "iter 331: loss 0.9601, time 120.14ms, mfu 0.01%\n",
      "iter 332: loss 0.9005, time 99.75ms, mfu 0.01%\n",
      "iter 333: loss 1.0670, time 99.61ms, mfu 0.01%\n",
      "iter 334: loss 0.8452, time 125.43ms, mfu 0.01%\n",
      "iter 335: loss 0.6375, time 105.61ms, mfu 0.01%\n",
      "iter 336: loss 0.7887, time 99.60ms, mfu 0.01%\n",
      "iter 337: loss 0.9659, time 97.40ms, mfu 0.01%\n",
      "iter 338: loss 0.6312, time 97.38ms, mfu 0.01%\n",
      "iter 339: loss 1.0597, time 99.73ms, mfu 0.01%\n",
      "iter 340: loss 1.0921, time 101.16ms, mfu 0.01%\n",
      "iter 341: loss 1.2147, time 97.01ms, mfu 0.01%\n",
      "iter 342: loss 1.0844, time 98.25ms, mfu 0.01%\n",
      "iter 343: loss 1.0892, time 100.62ms, mfu 0.01%\n",
      "iter 344: loss 0.9086, time 98.98ms, mfu 0.01%\n",
      "iter 345: loss 1.0625, time 99.73ms, mfu 0.01%\n",
      "iter 346: loss 0.7437, time 98.41ms, mfu 0.01%\n",
      "iter 347: loss 0.7175, time 95.64ms, mfu 0.01%\n",
      "iter 348: loss 1.3884, time 104.81ms, mfu 0.01%\n",
      "iter 349: loss 0.7863, time 125.82ms, mfu 0.01%\n",
      "iter 350: loss 1.0394, time 99.64ms, mfu 0.01%\n",
      "iter 351: loss 1.1510, time 111.00ms, mfu 0.01%\n",
      "iter 352: loss 0.6871, time 147.76ms, mfu 0.01%\n",
      "iter 353: loss 0.8150, time 102.59ms, mfu 0.01%\n",
      "iter 354: loss 1.2246, time 100.92ms, mfu 0.01%\n",
      "iter 355: loss 0.9236, time 101.44ms, mfu 0.01%\n",
      "iter 356: loss 0.6668, time 100.49ms, mfu 0.01%\n",
      "iter 357: loss 0.9097, time 176.90ms, mfu 0.01%\n",
      "iter 358: loss 0.7967, time 108.68ms, mfu 0.01%\n",
      "iter 359: loss 0.6614, time 98.54ms, mfu 0.01%\n",
      "iter 360: loss 0.9287, time 99.11ms, mfu 0.01%\n",
      "iter 361: loss 0.7015, time 120.25ms, mfu 0.01%\n",
      "iter 362: loss 0.6261, time 114.60ms, mfu 0.01%\n",
      "iter 363: loss 0.8053, time 113.55ms, mfu 0.01%\n",
      "iter 364: loss 0.7398, time 113.91ms, mfu 0.01%\n",
      "iter 365: loss 0.8116, time 98.25ms, mfu 0.01%\n",
      "iter 366: loss 1.0783, time 97.77ms, mfu 0.01%\n",
      "iter 367: loss 1.3439, time 96.90ms, mfu 0.01%\n",
      "iter 368: loss 0.6949, time 98.39ms, mfu 0.01%\n",
      "iter 369: loss 1.1308, time 103.99ms, mfu 0.01%\n",
      "iter 370: loss 1.0997, time 107.45ms, mfu 0.01%\n",
      "iter 371: loss 0.8771, time 100.27ms, mfu 0.01%\n",
      "iter 372: loss 1.2146, time 108.37ms, mfu 0.01%\n",
      "iter 373: loss 1.2543, time 99.43ms, mfu 0.01%\n",
      "iter 374: loss 0.7513, time 98.51ms, mfu 0.01%\n",
      "iter 375: loss 0.9329, time 100.89ms, mfu 0.01%\n",
      "iter 376: loss 1.2149, time 102.66ms, mfu 0.01%\n",
      "iter 377: loss 1.4261, time 110.06ms, mfu 0.01%\n",
      "iter 378: loss 1.1441, time 84.91ms, mfu 0.01%\n",
      "iter 379: loss 0.7488, time 88.07ms, mfu 0.01%\n",
      "iter 380: loss 0.6423, time 100.88ms, mfu 0.01%\n",
      "iter 381: loss 0.8197, time 104.11ms, mfu 0.01%\n",
      "iter 382: loss 0.7091, time 114.77ms, mfu 0.01%\n",
      "iter 383: loss 0.9236, time 99.95ms, mfu 0.01%\n",
      "iter 384: loss 0.9906, time 97.72ms, mfu 0.01%\n",
      "iter 385: loss 0.7829, time 96.38ms, mfu 0.01%\n",
      "iter 386: loss 0.9848, time 97.84ms, mfu 0.01%\n",
      "iter 387: loss 1.3327, time 95.69ms, mfu 0.01%\n",
      "iter 388: loss 1.1240, time 96.24ms, mfu 0.01%\n",
      "iter 389: loss 0.8670, time 100.58ms, mfu 0.01%\n",
      "iter 390: loss 0.9453, time 106.76ms, mfu 0.01%\n",
      "iter 391: loss 0.9445, time 105.40ms, mfu 0.01%\n",
      "iter 392: loss 1.1811, time 96.77ms, mfu 0.01%\n",
      "iter 393: loss 1.2304, time 97.01ms, mfu 0.01%\n",
      "iter 394: loss 0.7735, time 94.32ms, mfu 0.01%\n",
      "iter 395: loss 0.5788, time 96.69ms, mfu 0.01%\n",
      "iter 396: loss 0.8172, time 98.55ms, mfu 0.01%\n",
      "iter 397: loss 0.8145, time 95.27ms, mfu 0.01%\n",
      "iter 398: loss 1.0468, time 98.86ms, mfu 0.01%\n",
      "iter 399: loss 0.5066, time 94.58ms, mfu 0.01%\n",
      "iter 400: loss 1.3976, time 109.04ms, mfu 0.01%\n",
      "iter 401: loss 1.4134, time 103.72ms, mfu 0.01%\n",
      "iter 402: loss 1.0820, time 100.10ms, mfu 0.01%\n",
      "iter 403: loss 0.8004, time 93.92ms, mfu 0.01%\n",
      "iter 404: loss 1.1150, time 92.78ms, mfu 0.01%\n",
      "iter 405: loss 1.2014, time 93.48ms, mfu 0.01%\n",
      "iter 406: loss 1.3529, time 93.69ms, mfu 0.01%\n",
      "iter 407: loss 1.0589, time 92.21ms, mfu 0.01%\n",
      "iter 408: loss 1.1794, time 97.75ms, mfu 0.01%\n",
      "iter 409: loss 0.9497, time 106.57ms, mfu 0.01%\n",
      "iter 410: loss 1.2636, time 111.44ms, mfu 0.01%\n",
      "iter 411: loss 0.9509, time 112.98ms, mfu 0.01%\n",
      "iter 412: loss 0.6937, time 113.90ms, mfu 0.01%\n",
      "iter 413: loss 0.9145, time 100.56ms, mfu 0.01%\n",
      "iter 414: loss 1.1096, time 100.57ms, mfu 0.01%\n",
      "iter 415: loss 0.7033, time 98.43ms, mfu 0.01%\n",
      "iter 416: loss 1.2581, time 97.93ms, mfu 0.01%\n",
      "iter 417: loss 0.8269, time 94.20ms, mfu 0.01%\n",
      "iter 418: loss 0.7742, time 96.10ms, mfu 0.01%\n",
      "iter 419: loss 1.2320, time 96.56ms, mfu 0.01%\n",
      "iter 420: loss 1.0047, time 94.63ms, mfu 0.01%\n",
      "iter 421: loss 0.9597, time 88.81ms, mfu 0.01%\n",
      "iter 422: loss 0.9219, time 98.00ms, mfu 0.01%\n",
      "iter 423: loss 0.7443, time 114.35ms, mfu 0.01%\n",
      "iter 424: loss 0.7962, time 108.73ms, mfu 0.01%\n",
      "iter 425: loss 0.8428, time 116.47ms, mfu 0.01%\n",
      "iter 426: loss 1.0258, time 188.47ms, mfu 0.01%\n",
      "iter 427: loss 1.2614, time 123.33ms, mfu 0.01%\n",
      "iter 428: loss 1.3570, time 104.81ms, mfu 0.01%\n",
      "iter 429: loss 0.6681, time 103.80ms, mfu 0.01%\n",
      "iter 430: loss 0.9111, time 123.25ms, mfu 0.01%\n",
      "iter 431: loss 0.9920, time 127.48ms, mfu 0.01%\n",
      "iter 432: loss 0.6866, time 117.83ms, mfu 0.01%\n",
      "iter 433: loss 1.1232, time 125.37ms, mfu 0.01%\n",
      "iter 434: loss 1.0163, time 113.87ms, mfu 0.01%\n",
      "iter 435: loss 1.1414, time 105.04ms, mfu 0.01%\n",
      "iter 436: loss 0.7015, time 105.27ms, mfu 0.01%\n",
      "iter 437: loss 0.6988, time 105.53ms, mfu 0.01%\n",
      "iter 438: loss 1.3506, time 108.39ms, mfu 0.01%\n",
      "iter 439: loss 1.2394, time 105.17ms, mfu 0.01%\n",
      "iter 440: loss 0.6861, time 107.26ms, mfu 0.01%\n",
      "iter 441: loss 0.8683, time 104.66ms, mfu 0.01%\n",
      "iter 442: loss 1.0175, time 100.99ms, mfu 0.01%\n",
      "iter 443: loss 1.1400, time 115.40ms, mfu 0.01%\n",
      "iter 444: loss 0.8101, time 103.59ms, mfu 0.01%\n",
      "iter 445: loss 0.5291, time 100.98ms, mfu 0.01%\n",
      "iter 446: loss 1.2678, time 103.64ms, mfu 0.01%\n",
      "iter 447: loss 1.0698, time 97.10ms, mfu 0.01%\n",
      "iter 448: loss 0.7048, time 96.86ms, mfu 0.01%\n",
      "iter 449: loss 0.9548, time 103.50ms, mfu 0.01%\n",
      "iter 450: loss 0.8719, time 149.06ms, mfu 0.01%\n",
      "iter 451: loss 0.9160, time 134.25ms, mfu 0.01%\n",
      "iter 452: loss 0.7805, time 119.24ms, mfu 0.01%\n",
      "iter 453: loss 0.7078, time 113.64ms, mfu 0.01%\n",
      "iter 454: loss 0.9732, time 98.49ms, mfu 0.01%\n",
      "iter 455: loss 1.4652, time 100.33ms, mfu 0.01%\n",
      "iter 456: loss 0.8707, time 96.95ms, mfu 0.01%\n",
      "iter 457: loss 1.2692, time 96.87ms, mfu 0.01%\n",
      "iter 458: loss 0.8471, time 95.91ms, mfu 0.01%\n",
      "iter 459: loss 0.7475, time 96.03ms, mfu 0.01%\n",
      "iter 460: loss 1.0280, time 96.45ms, mfu 0.01%\n",
      "iter 461: loss 0.8879, time 95.51ms, mfu 0.01%\n",
      "iter 462: loss 0.9493, time 96.20ms, mfu 0.01%\n",
      "iter 463: loss 1.2508, time 95.08ms, mfu 0.01%\n",
      "iter 464: loss 0.7548, time 98.47ms, mfu 0.01%\n",
      "iter 465: loss 0.7349, time 89.21ms, mfu 0.01%\n",
      "iter 466: loss 1.2366, time 79.83ms, mfu 0.01%\n",
      "iter 467: loss 0.7189, time 79.97ms, mfu 0.01%\n",
      "iter 468: loss 0.9523, time 81.38ms, mfu 0.01%\n",
      "iter 469: loss 0.9052, time 97.88ms, mfu 0.01%\n",
      "iter 470: loss 0.8650, time 100.17ms, mfu 0.01%\n",
      "iter 471: loss 1.1378, time 95.98ms, mfu 0.01%\n",
      "iter 472: loss 1.4187, time 96.09ms, mfu 0.01%\n",
      "iter 473: loss 1.0354, time 96.68ms, mfu 0.01%\n",
      "iter 474: loss 1.0006, time 95.26ms, mfu 0.01%\n",
      "iter 475: loss 1.1100, time 95.43ms, mfu 0.01%\n",
      "iter 476: loss 0.9069, time 95.11ms, mfu 0.01%\n",
      "iter 477: loss 0.9828, time 95.33ms, mfu 0.01%\n",
      "iter 478: loss 0.8215, time 100.49ms, mfu 0.01%\n",
      "iter 479: loss 1.2241, time 94.99ms, mfu 0.01%\n",
      "iter 480: loss 1.2819, time 92.12ms, mfu 0.01%\n",
      "iter 481: loss 1.2948, time 107.48ms, mfu 0.01%\n",
      "iter 482: loss 0.8328, time 147.27ms, mfu 0.01%\n",
      "iter 483: loss 1.0764, time 110.00ms, mfu 0.01%\n",
      "iter 484: loss 0.8991, time 93.41ms, mfu 0.01%\n",
      "iter 485: loss 1.2213, time 99.26ms, mfu 0.01%\n",
      "iter 486: loss 1.0148, time 105.20ms, mfu 0.01%\n",
      "iter 487: loss 0.9546, time 99.60ms, mfu 0.01%\n",
      "iter 488: loss 0.8026, time 94.99ms, mfu 0.01%\n",
      "iter 489: loss 1.3822, time 93.53ms, mfu 0.01%\n",
      "iter 490: loss 1.0228, time 92.50ms, mfu 0.01%\n",
      "iter 491: loss 1.3389, time 92.79ms, mfu 0.01%\n",
      "iter 492: loss 0.8556, time 93.20ms, mfu 0.01%\n",
      "iter 493: loss 0.6834, time 93.22ms, mfu 0.01%\n",
      "iter 494: loss 1.2733, time 95.97ms, mfu 0.01%\n",
      "iter 495: loss 0.6576, time 99.13ms, mfu 0.01%\n",
      "iter 496: loss 0.5937, time 94.40ms, mfu 0.01%\n",
      "iter 497: loss 0.8142, time 97.24ms, mfu 0.01%\n",
      "iter 498: loss 0.6199, time 97.43ms, mfu 0.01%\n",
      "iter 499: loss 1.4668, time 96.01ms, mfu 0.01%\n",
      "step 500: train loss 0.9521, val loss 1.4169\n",
      "iter 500: loss 1.2162, time 1268.34ms, mfu 0.01%\n",
      "iter 501: loss 0.5127, time 117.43ms, mfu 0.01%\n",
      "iter 502: loss 0.9901, time 118.36ms, mfu 0.01%\n",
      "iter 503: loss 1.0006, time 100.59ms, mfu 0.01%\n",
      "iter 504: loss 0.9727, time 98.34ms, mfu 0.01%\n",
      "iter 505: loss 0.6883, time 99.21ms, mfu 0.01%\n",
      "iter 506: loss 1.0148, time 99.02ms, mfu 0.01%\n",
      "iter 507: loss 0.8992, time 99.45ms, mfu 0.01%\n",
      "iter 508: loss 0.8835, time 99.86ms, mfu 0.01%\n",
      "iter 509: loss 1.1774, time 100.01ms, mfu 0.01%\n",
      "iter 510: loss 1.3355, time 98.90ms, mfu 0.01%\n",
      "iter 511: loss 1.0916, time 98.53ms, mfu 0.01%\n",
      "iter 512: loss 0.7833, time 98.70ms, mfu 0.01%\n",
      "iter 513: loss 1.0443, time 97.05ms, mfu 0.01%\n",
      "iter 514: loss 0.7316, time 96.14ms, mfu 0.01%\n",
      "iter 515: loss 1.0618, time 100.18ms, mfu 0.01%\n",
      "iter 516: loss 0.8044, time 102.15ms, mfu 0.01%\n",
      "iter 517: loss 0.7632, time 101.14ms, mfu 0.01%\n",
      "iter 518: loss 1.0243, time 100.99ms, mfu 0.01%\n",
      "iter 519: loss 1.2788, time 106.37ms, mfu 0.01%\n",
      "iter 520: loss 0.8998, time 102.09ms, mfu 0.01%\n",
      "iter 521: loss 0.6855, time 118.00ms, mfu 0.01%\n",
      "iter 522: loss 0.9959, time 115.52ms, mfu 0.01%\n",
      "iter 523: loss 0.7210, time 100.51ms, mfu 0.01%\n",
      "iter 524: loss 1.1417, time 99.34ms, mfu 0.01%\n",
      "iter 525: loss 0.7404, time 98.02ms, mfu 0.01%\n",
      "iter 526: loss 0.7911, time 98.63ms, mfu 0.01%\n",
      "iter 527: loss 1.4073, time 98.50ms, mfu 0.01%\n",
      "iter 528: loss 1.0510, time 100.27ms, mfu 0.01%\n",
      "iter 529: loss 0.6835, time 97.29ms, mfu 0.01%\n",
      "iter 530: loss 0.7900, time 118.09ms, mfu 0.01%\n",
      "iter 531: loss 1.2677, time 144.04ms, mfu 0.01%\n",
      "iter 532: loss 0.8584, time 116.25ms, mfu 0.01%\n",
      "iter 533: loss 0.5272, time 133.32ms, mfu 0.01%\n",
      "iter 534: loss 0.4431, time 112.66ms, mfu 0.01%\n",
      "iter 535: loss 1.4501, time 107.73ms, mfu 0.01%\n",
      "iter 536: loss 0.7096, time 89.85ms, mfu 0.01%\n",
      "iter 537: loss 0.6973, time 99.21ms, mfu 0.01%\n",
      "iter 538: loss 1.1235, time 101.25ms, mfu 0.01%\n",
      "iter 539: loss 1.0954, time 112.77ms, mfu 0.01%\n",
      "iter 540: loss 0.5261, time 100.56ms, mfu 0.01%\n",
      "iter 541: loss 0.9885, time 121.25ms, mfu 0.01%\n",
      "iter 542: loss 1.1162, time 114.31ms, mfu 0.01%\n",
      "iter 543: loss 0.8928, time 187.83ms, mfu 0.01%\n",
      "iter 544: loss 0.7096, time 107.90ms, mfu 0.01%\n",
      "iter 545: loss 0.8113, time 99.58ms, mfu 0.01%\n",
      "iter 546: loss 0.7855, time 100.93ms, mfu 0.01%\n",
      "iter 547: loss 1.4015, time 98.99ms, mfu 0.01%\n",
      "iter 548: loss 1.2818, time 106.27ms, mfu 0.01%\n",
      "iter 549: loss 0.8400, time 98.70ms, mfu 0.01%\n",
      "iter 550: loss 0.9024, time 97.95ms, mfu 0.01%\n",
      "iter 551: loss 1.4417, time 103.18ms, mfu 0.01%\n",
      "iter 552: loss 0.6995, time 101.13ms, mfu 0.01%\n",
      "iter 553: loss 0.8936, time 101.11ms, mfu 0.01%\n",
      "iter 554: loss 0.8847, time 98.55ms, mfu 0.01%\n",
      "iter 555: loss 1.1184, time 109.23ms, mfu 0.01%\n",
      "iter 556: loss 1.0001, time 106.56ms, mfu 0.01%\n",
      "iter 557: loss 0.7622, time 99.25ms, mfu 0.01%\n",
      "iter 558: loss 0.9027, time 99.87ms, mfu 0.01%\n",
      "iter 559: loss 0.8140, time 103.86ms, mfu 0.01%\n",
      "iter 560: loss 0.9510, time 101.66ms, mfu 0.01%\n",
      "iter 561: loss 1.0437, time 102.49ms, mfu 0.01%\n",
      "iter 562: loss 0.7702, time 124.19ms, mfu 0.01%\n",
      "iter 563: loss 0.7954, time 113.12ms, mfu 0.01%\n",
      "iter 564: loss 1.1666, time 101.34ms, mfu 0.01%\n",
      "iter 565: loss 0.6687, time 99.95ms, mfu 0.01%\n",
      "iter 566: loss 1.3449, time 111.74ms, mfu 0.01%\n",
      "iter 567: loss 0.9738, time 115.50ms, mfu 0.01%\n",
      "iter 568: loss 0.8737, time 113.18ms, mfu 0.01%\n",
      "iter 569: loss 0.7459, time 106.19ms, mfu 0.01%\n",
      "iter 570: loss 0.9508, time 111.77ms, mfu 0.01%\n",
      "iter 571: loss 1.1103, time 111.24ms, mfu 0.01%\n",
      "iter 572: loss 0.8027, time 106.03ms, mfu 0.01%\n",
      "iter 573: loss 1.0591, time 110.03ms, mfu 0.01%\n",
      "iter 574: loss 1.1539, time 104.88ms, mfu 0.01%\n",
      "iter 575: loss 0.7065, time 102.33ms, mfu 0.01%\n",
      "iter 576: loss 0.7610, time 102.05ms, mfu 0.01%\n",
      "iter 577: loss 0.8706, time 100.26ms, mfu 0.01%\n",
      "iter 578: loss 1.0154, time 110.21ms, mfu 0.01%\n",
      "iter 579: loss 0.6687, time 99.14ms, mfu 0.01%\n",
      "iter 580: loss 1.0630, time 99.08ms, mfu 0.01%\n",
      "iter 581: loss 0.6101, time 100.81ms, mfu 0.01%\n",
      "iter 582: loss 0.6749, time 104.81ms, mfu 0.01%\n",
      "iter 583: loss 0.8447, time 99.88ms, mfu 0.01%\n",
      "iter 584: loss 0.7954, time 99.03ms, mfu 0.01%\n",
      "iter 585: loss 0.7207, time 98.36ms, mfu 0.01%\n",
      "iter 586: loss 1.3051, time 99.94ms, mfu 0.01%\n",
      "iter 587: loss 0.8422, time 96.67ms, mfu 0.01%\n",
      "iter 588: loss 0.9081, time 97.67ms, mfu 0.01%\n",
      "iter 589: loss 0.7042, time 98.44ms, mfu 0.01%\n",
      "iter 590: loss 0.9403, time 98.02ms, mfu 0.01%\n",
      "iter 591: loss 0.9950, time 97.43ms, mfu 0.01%\n",
      "iter 592: loss 0.5616, time 96.56ms, mfu 0.01%\n",
      "iter 593: loss 0.9554, time 98.50ms, mfu 0.01%\n",
      "iter 594: loss 1.3869, time 97.94ms, mfu 0.01%\n",
      "iter 595: loss 0.7405, time 97.52ms, mfu 0.01%\n",
      "iter 596: loss 1.2070, time 97.37ms, mfu 0.01%\n",
      "iter 597: loss 1.2156, time 110.74ms, mfu 0.01%\n",
      "iter 598: loss 1.1325, time 97.99ms, mfu 0.01%\n",
      "iter 599: loss 0.9120, time 96.49ms, mfu 0.01%\n",
      "iter 600: loss 0.8814, time 162.24ms, mfu 0.01%\n",
      "iter 601: loss 1.0352, time 97.72ms, mfu 0.01%\n",
      "iter 602: loss 1.5154, time 97.36ms, mfu 0.01%\n",
      "iter 603: loss 0.8548, time 98.95ms, mfu 0.01%\n",
      "iter 604: loss 1.1831, time 97.17ms, mfu 0.01%\n",
      "iter 605: loss 1.2177, time 97.17ms, mfu 0.01%\n",
      "iter 606: loss 0.7492, time 96.86ms, mfu 0.01%\n",
      "iter 607: loss 0.6350, time 98.43ms, mfu 0.01%\n",
      "iter 608: loss 1.0812, time 97.69ms, mfu 0.01%\n",
      "iter 609: loss 0.7331, time 96.90ms, mfu 0.01%\n",
      "iter 610: loss 0.9673, time 98.25ms, mfu 0.01%\n",
      "iter 611: loss 0.6179, time 98.45ms, mfu 0.01%\n",
      "iter 612: loss 0.6822, time 106.52ms, mfu 0.01%\n",
      "iter 613: loss 0.7349, time 99.82ms, mfu 0.01%\n",
      "iter 614: loss 0.8070, time 98.66ms, mfu 0.01%\n",
      "iter 615: loss 0.9423, time 101.17ms, mfu 0.01%\n",
      "iter 616: loss 0.7443, time 98.66ms, mfu 0.01%\n",
      "iter 617: loss 0.8012, time 100.18ms, mfu 0.01%\n",
      "iter 618: loss 0.8327, time 102.44ms, mfu 0.01%\n",
      "iter 619: loss 0.7136, time 104.02ms, mfu 0.01%\n",
      "iter 620: loss 0.9739, time 98.72ms, mfu 0.01%\n",
      "iter 621: loss 0.7886, time 95.10ms, mfu 0.01%\n",
      "iter 622: loss 1.1990, time 96.29ms, mfu 0.01%\n",
      "iter 623: loss 1.0799, time 96.44ms, mfu 0.01%\n",
      "iter 624: loss 0.8429, time 97.19ms, mfu 0.01%\n",
      "iter 625: loss 1.1692, time 101.53ms, mfu 0.01%\n",
      "iter 626: loss 0.7651, time 107.19ms, mfu 0.01%\n",
      "iter 627: loss 0.8378, time 102.44ms, mfu 0.01%\n",
      "iter 628: loss 0.6169, time 99.37ms, mfu 0.01%\n",
      "iter 629: loss 1.1758, time 99.08ms, mfu 0.01%\n",
      "iter 630: loss 0.9222, time 97.94ms, mfu 0.01%\n",
      "iter 631: loss 0.8786, time 100.49ms, mfu 0.01%\n",
      "iter 632: loss 1.1564, time 104.70ms, mfu 0.01%\n",
      "iter 633: loss 0.9133, time 104.44ms, mfu 0.01%\n",
      "iter 634: loss 1.0598, time 107.57ms, mfu 0.01%\n",
      "iter 635: loss 1.1388, time 113.12ms, mfu 0.01%\n",
      "iter 636: loss 1.1732, time 106.75ms, mfu 0.01%\n",
      "iter 637: loss 0.9318, time 102.53ms, mfu 0.01%\n",
      "iter 638: loss 1.4718, time 98.53ms, mfu 0.01%\n",
      "iter 639: loss 1.0546, time 104.70ms, mfu 0.01%\n",
      "iter 640: loss 0.9145, time 105.24ms, mfu 0.01%\n",
      "iter 641: loss 1.1198, time 109.21ms, mfu 0.01%\n",
      "iter 642: loss 0.8471, time 103.53ms, mfu 0.01%\n",
      "iter 643: loss 0.4378, time 99.88ms, mfu 0.01%\n",
      "iter 644: loss 0.9742, time 99.23ms, mfu 0.01%\n",
      "iter 645: loss 0.8297, time 102.49ms, mfu 0.01%\n",
      "iter 646: loss 0.8090, time 103.79ms, mfu 0.01%\n",
      "iter 647: loss 1.2436, time 97.99ms, mfu 0.01%\n",
      "iter 648: loss 1.0098, time 98.31ms, mfu 0.01%\n",
      "iter 649: loss 1.0866, time 112.44ms, mfu 0.01%\n",
      "iter 650: loss 0.8990, time 188.12ms, mfu 0.01%\n",
      "iter 651: loss 0.5690, time 103.41ms, mfu 0.01%\n",
      "iter 652: loss 0.9398, time 105.15ms, mfu 0.01%\n",
      "iter 653: loss 0.7773, time 100.79ms, mfu 0.01%\n",
      "iter 654: loss 0.7024, time 98.24ms, mfu 0.01%\n",
      "iter 655: loss 0.8367, time 99.34ms, mfu 0.01%\n",
      "iter 656: loss 1.4659, time 97.81ms, mfu 0.01%\n",
      "iter 657: loss 0.6802, time 102.05ms, mfu 0.01%\n",
      "iter 658: loss 0.6581, time 101.00ms, mfu 0.01%\n",
      "iter 659: loss 0.8680, time 100.94ms, mfu 0.01%\n",
      "iter 660: loss 1.0810, time 97.36ms, mfu 0.01%\n",
      "iter 661: loss 1.1113, time 97.53ms, mfu 0.01%\n",
      "iter 662: loss 0.8475, time 96.95ms, mfu 0.01%\n",
      "iter 663: loss 0.6527, time 99.12ms, mfu 0.01%\n",
      "iter 664: loss 1.0590, time 96.60ms, mfu 0.01%\n",
      "iter 665: loss 0.8581, time 97.10ms, mfu 0.01%\n",
      "iter 666: loss 0.5561, time 102.24ms, mfu 0.01%\n",
      "iter 667: loss 1.0848, time 100.96ms, mfu 0.01%\n",
      "iter 668: loss 0.8915, time 97.79ms, mfu 0.01%\n",
      "iter 669: loss 0.7866, time 99.19ms, mfu 0.01%\n",
      "iter 670: loss 1.3650, time 98.12ms, mfu 0.01%\n",
      "iter 671: loss 0.7622, time 97.34ms, mfu 0.01%\n",
      "iter 672: loss 0.8127, time 95.31ms, mfu 0.01%\n",
      "iter 673: loss 1.1622, time 97.83ms, mfu 0.01%\n",
      "iter 674: loss 0.6583, time 96.76ms, mfu 0.01%\n",
      "iter 675: loss 1.1109, time 97.47ms, mfu 0.01%\n",
      "iter 676: loss 0.6708, time 97.15ms, mfu 0.01%\n",
      "iter 677: loss 0.9978, time 97.51ms, mfu 0.01%\n",
      "iter 678: loss 0.5209, time 98.27ms, mfu 0.01%\n",
      "iter 679: loss 0.6325, time 97.41ms, mfu 0.01%\n",
      "iter 680: loss 0.7402, time 97.39ms, mfu 0.01%\n",
      "iter 681: loss 0.8478, time 107.99ms, mfu 0.01%\n",
      "iter 682: loss 0.8655, time 100.21ms, mfu 0.01%\n",
      "iter 683: loss 1.2080, time 98.50ms, mfu 0.01%\n",
      "iter 684: loss 1.1039, time 95.42ms, mfu 0.01%\n",
      "iter 685: loss 1.3118, time 95.85ms, mfu 0.01%\n",
      "iter 686: loss 0.9091, time 96.66ms, mfu 0.01%\n",
      "iter 687: loss 0.9315, time 94.99ms, mfu 0.01%\n",
      "iter 688: loss 0.9432, time 99.42ms, mfu 0.01%\n",
      "iter 689: loss 0.5930, time 101.02ms, mfu 0.01%\n",
      "iter 690: loss 0.4809, time 101.20ms, mfu 0.01%\n",
      "iter 691: loss 0.8508, time 100.39ms, mfu 0.01%\n",
      "iter 692: loss 0.7118, time 97.76ms, mfu 0.01%\n",
      "iter 693: loss 0.6448, time 97.00ms, mfu 0.01%\n",
      "iter 694: loss 1.0965, time 97.10ms, mfu 0.01%\n",
      "iter 695: loss 0.5683, time 98.83ms, mfu 0.01%\n",
      "iter 696: loss 1.3010, time 101.79ms, mfu 0.01%\n",
      "iter 697: loss 0.9068, time 124.33ms, mfu 0.01%\n",
      "iter 698: loss 1.1658, time 100.05ms, mfu 0.01%\n",
      "iter 699: loss 0.8772, time 97.54ms, mfu 0.01%\n",
      "iter 700: loss 0.4128, time 95.60ms, mfu 0.01%\n",
      "iter 701: loss 0.8833, time 95.66ms, mfu 0.01%\n",
      "iter 702: loss 0.7319, time 96.77ms, mfu 0.01%\n",
      "iter 703: loss 0.9263, time 96.54ms, mfu 0.01%\n",
      "iter 704: loss 0.7493, time 150.29ms, mfu 0.01%\n",
      "iter 705: loss 1.3369, time 95.14ms, mfu 0.01%\n",
      "iter 706: loss 1.0102, time 91.01ms, mfu 0.01%\n",
      "iter 707: loss 0.6644, time 93.37ms, mfu 0.01%\n",
      "iter 708: loss 1.0601, time 97.16ms, mfu 0.01%\n",
      "iter 709: loss 0.8353, time 97.13ms, mfu 0.01%\n",
      "iter 710: loss 0.7546, time 103.63ms, mfu 0.01%\n",
      "iter 711: loss 1.1469, time 99.08ms, mfu 0.01%\n",
      "iter 712: loss 0.8136, time 95.45ms, mfu 0.01%\n",
      "iter 713: loss 0.7550, time 93.12ms, mfu 0.01%\n",
      "iter 714: loss 1.1708, time 94.63ms, mfu 0.01%\n",
      "iter 715: loss 0.6403, time 93.60ms, mfu 0.01%\n",
      "iter 716: loss 0.9150, time 94.40ms, mfu 0.01%\n",
      "iter 717: loss 1.0233, time 93.66ms, mfu 0.01%\n",
      "iter 718: loss 1.0278, time 96.16ms, mfu 0.01%\n",
      "iter 719: loss 0.8776, time 95.46ms, mfu 0.01%\n",
      "iter 720: loss 0.5612, time 94.72ms, mfu 0.01%\n",
      "iter 721: loss 1.1444, time 95.45ms, mfu 0.01%\n",
      "iter 722: loss 0.6485, time 94.49ms, mfu 0.01%\n",
      "iter 723: loss 0.3832, time 93.40ms, mfu 0.01%\n",
      "iter 724: loss 0.9391, time 94.74ms, mfu 0.01%\n",
      "iter 725: loss 0.9127, time 94.91ms, mfu 0.01%\n",
      "iter 726: loss 0.8069, time 97.37ms, mfu 0.01%\n",
      "iter 727: loss 0.8124, time 102.50ms, mfu 0.01%\n",
      "iter 728: loss 1.0119, time 100.77ms, mfu 0.01%\n",
      "iter 729: loss 1.0581, time 97.40ms, mfu 0.01%\n",
      "iter 730: loss 0.8088, time 100.04ms, mfu 0.01%\n",
      "iter 731: loss 0.9204, time 100.59ms, mfu 0.01%\n",
      "iter 732: loss 1.1425, time 102.62ms, mfu 0.01%\n",
      "iter 733: loss 0.7801, time 102.22ms, mfu 0.01%\n",
      "iter 734: loss 0.6425, time 101.51ms, mfu 0.01%\n",
      "iter 735: loss 1.2165, time 101.12ms, mfu 0.01%\n",
      "iter 736: loss 0.7894, time 99.79ms, mfu 0.01%\n",
      "iter 737: loss 0.7700, time 96.85ms, mfu 0.01%\n",
      "iter 738: loss 0.7227, time 97.39ms, mfu 0.01%\n",
      "iter 739: loss 0.6843, time 98.14ms, mfu 0.01%\n",
      "iter 740: loss 0.7666, time 96.02ms, mfu 0.01%\n",
      "iter 741: loss 0.8365, time 96.91ms, mfu 0.01%\n",
      "iter 742: loss 0.7029, time 111.22ms, mfu 0.01%\n",
      "iter 743: loss 0.9561, time 97.27ms, mfu 0.01%\n",
      "iter 744: loss 0.6190, time 98.53ms, mfu 0.01%\n",
      "iter 745: loss 1.1162, time 96.88ms, mfu 0.01%\n",
      "iter 746: loss 1.1660, time 93.52ms, mfu 0.01%\n",
      "iter 747: loss 0.9812, time 98.21ms, mfu 0.01%\n",
      "iter 748: loss 1.1223, time 98.97ms, mfu 0.01%\n",
      "iter 749: loss 0.7358, time 103.20ms, mfu 0.01%\n",
      "step 750: train loss 0.8348, val loss 1.2681\n",
      "saving checkpoint to out-slang\n",
      "iter 750: loss 0.5199, time 1279.19ms, mfu 0.01%\n",
      "iter 751: loss 1.1369, time 108.63ms, mfu 0.01%\n",
      "iter 752: loss 1.0355, time 102.40ms, mfu 0.01%\n",
      "iter 753: loss 0.6318, time 101.60ms, mfu 0.01%\n",
      "iter 754: loss 1.3893, time 100.97ms, mfu 0.01%\n",
      "iter 755: loss 1.0687, time 101.37ms, mfu 0.01%\n",
      "iter 756: loss 0.7379, time 101.47ms, mfu 0.01%\n",
      "iter 757: loss 1.3732, time 101.05ms, mfu 0.01%\n",
      "iter 758: loss 0.8484, time 102.18ms, mfu 0.01%\n",
      "iter 759: loss 0.8124, time 103.44ms, mfu 0.01%\n",
      "iter 760: loss 0.8389, time 101.85ms, mfu 0.01%\n",
      "iter 761: loss 0.6257, time 101.06ms, mfu 0.01%\n",
      "iter 762: loss 1.1441, time 101.09ms, mfu 0.01%\n",
      "iter 763: loss 0.8411, time 101.07ms, mfu 0.01%\n",
      "iter 764: loss 0.7030, time 97.31ms, mfu 0.01%\n",
      "iter 765: loss 1.0980, time 97.99ms, mfu 0.01%\n",
      "iter 766: loss 0.8939, time 97.87ms, mfu 0.01%\n",
      "iter 767: loss 0.9191, time 98.17ms, mfu 0.01%\n",
      "iter 768: loss 0.5187, time 98.57ms, mfu 0.01%\n",
      "iter 769: loss 0.6845, time 98.21ms, mfu 0.01%\n",
      "iter 770: loss 0.7953, time 97.90ms, mfu 0.01%\n",
      "iter 771: loss 0.6535, time 98.68ms, mfu 0.01%\n",
      "iter 772: loss 0.7573, time 101.85ms, mfu 0.01%\n",
      "iter 773: loss 1.1602, time 101.64ms, mfu 0.01%\n",
      "iter 774: loss 1.0175, time 100.05ms, mfu 0.01%\n",
      "iter 775: loss 0.6173, time 100.78ms, mfu 0.01%\n",
      "iter 776: loss 0.7817, time 100.39ms, mfu 0.01%\n",
      "iter 777: loss 0.9686, time 101.36ms, mfu 0.01%\n",
      "iter 778: loss 1.0852, time 102.95ms, mfu 0.01%\n",
      "iter 779: loss 0.7735, time 101.68ms, mfu 0.01%\n",
      "iter 780: loss 0.9020, time 101.97ms, mfu 0.01%\n",
      "iter 781: loss 0.5404, time 101.62ms, mfu 0.01%\n",
      "iter 782: loss 1.0035, time 101.06ms, mfu 0.01%\n",
      "iter 783: loss 0.6716, time 119.11ms, mfu 0.01%\n",
      "iter 784: loss 0.9728, time 100.70ms, mfu 0.01%\n",
      "iter 785: loss 0.8732, time 101.17ms, mfu 0.01%\n",
      "iter 786: loss 0.8114, time 100.60ms, mfu 0.01%\n",
      "iter 787: loss 1.0226, time 101.62ms, mfu 0.01%\n",
      "iter 788: loss 0.6449, time 101.90ms, mfu 0.01%\n",
      "iter 789: loss 0.5582, time 103.89ms, mfu 0.01%\n",
      "iter 790: loss 1.0731, time 102.66ms, mfu 0.01%\n",
      "iter 791: loss 0.8387, time 102.89ms, mfu 0.01%\n",
      "iter 792: loss 0.8449, time 102.25ms, mfu 0.01%\n",
      "iter 793: loss 0.8379, time 101.46ms, mfu 0.01%\n",
      "iter 794: loss 0.6470, time 102.19ms, mfu 0.01%\n",
      "iter 795: loss 0.8319, time 100.94ms, mfu 0.01%\n",
      "iter 796: loss 1.1068, time 102.06ms, mfu 0.01%\n",
      "iter 797: loss 0.9129, time 101.93ms, mfu 0.01%\n",
      "iter 798: loss 0.8682, time 103.47ms, mfu 0.01%\n",
      "iter 799: loss 0.8716, time 101.58ms, mfu 0.01%\n",
      "iter 800: loss 0.6985, time 152.79ms, mfu 0.01%\n",
      "iter 801: loss 0.6528, time 113.77ms, mfu 0.01%\n",
      "iter 802: loss 0.6641, time 101.95ms, mfu 0.01%\n",
      "iter 803: loss 0.7550, time 100.17ms, mfu 0.01%\n",
      "iter 804: loss 0.7862, time 96.97ms, mfu 0.01%\n",
      "iter 805: loss 1.5602, time 98.16ms, mfu 0.01%\n",
      "iter 806: loss 0.8507, time 98.04ms, mfu 0.01%\n",
      "iter 807: loss 0.8505, time 97.72ms, mfu 0.01%\n",
      "iter 808: loss 1.0181, time 97.40ms, mfu 0.01%\n",
      "iter 809: loss 0.5432, time 96.88ms, mfu 0.01%\n",
      "iter 810: loss 0.7467, time 98.42ms, mfu 0.01%\n",
      "iter 811: loss 1.0123, time 97.03ms, mfu 0.01%\n",
      "iter 812: loss 0.6701, time 98.47ms, mfu 0.01%\n",
      "iter 813: loss 0.7182, time 100.55ms, mfu 0.01%\n",
      "iter 814: loss 1.0669, time 97.20ms, mfu 0.01%\n",
      "iter 815: loss 0.7304, time 98.18ms, mfu 0.01%\n",
      "iter 816: loss 1.0184, time 102.80ms, mfu 0.01%\n",
      "iter 817: loss 0.6118, time 101.09ms, mfu 0.01%\n",
      "iter 818: loss 0.7328, time 99.37ms, mfu 0.01%\n",
      "iter 819: loss 0.9332, time 100.27ms, mfu 0.01%\n",
      "iter 820: loss 1.1762, time 100.52ms, mfu 0.01%\n",
      "iter 821: loss 1.3267, time 101.82ms, mfu 0.01%\n",
      "iter 822: loss 0.5974, time 100.31ms, mfu 0.01%\n",
      "iter 823: loss 0.9152, time 113.51ms, mfu 0.01%\n",
      "iter 824: loss 0.9734, time 97.87ms, mfu 0.01%\n",
      "iter 825: loss 0.6064, time 98.30ms, mfu 0.01%\n",
      "iter 826: loss 1.1291, time 99.37ms, mfu 0.01%\n",
      "iter 827: loss 1.0343, time 98.16ms, mfu 0.01%\n",
      "iter 828: loss 0.9182, time 97.70ms, mfu 0.01%\n",
      "iter 829: loss 1.1446, time 99.03ms, mfu 0.01%\n",
      "iter 830: loss 0.8638, time 98.81ms, mfu 0.01%\n",
      "iter 831: loss 0.5600, time 102.38ms, mfu 0.01%\n",
      "iter 832: loss 0.8518, time 99.99ms, mfu 0.01%\n",
      "iter 833: loss 1.0492, time 101.09ms, mfu 0.01%\n",
      "iter 834: loss 0.7384, time 101.60ms, mfu 0.01%\n",
      "iter 835: loss 0.6313, time 102.74ms, mfu 0.01%\n",
      "iter 836: loss 1.0016, time 102.55ms, mfu 0.01%\n",
      "iter 837: loss 0.4150, time 101.94ms, mfu 0.01%\n",
      "iter 838: loss 1.1059, time 101.05ms, mfu 0.01%\n",
      "iter 839: loss 1.3115, time 103.17ms, mfu 0.01%\n",
      "iter 840: loss 1.1370, time 105.05ms, mfu 0.01%\n",
      "iter 841: loss 0.8108, time 103.25ms, mfu 0.01%\n",
      "iter 842: loss 1.0879, time 102.13ms, mfu 0.01%\n",
      "iter 843: loss 0.8478, time 101.30ms, mfu 0.01%\n",
      "iter 844: loss 1.0341, time 159.70ms, mfu 0.01%\n",
      "iter 845: loss 1.1245, time 98.89ms, mfu 0.01%\n",
      "iter 846: loss 0.9819, time 99.34ms, mfu 0.01%\n",
      "iter 847: loss 0.9903, time 99.00ms, mfu 0.01%\n",
      "iter 848: loss 1.2956, time 98.21ms, mfu 0.01%\n",
      "iter 849: loss 0.5241, time 98.28ms, mfu 0.01%\n",
      "iter 850: loss 0.6807, time 97.80ms, mfu 0.01%\n",
      "iter 851: loss 0.6351, time 96.99ms, mfu 0.01%\n",
      "iter 852: loss 0.5617, time 96.77ms, mfu 0.01%\n",
      "iter 853: loss 0.9850, time 97.22ms, mfu 0.01%\n",
      "iter 854: loss 0.8165, time 97.97ms, mfu 0.01%\n",
      "iter 855: loss 0.7944, time 96.94ms, mfu 0.01%\n",
      "iter 856: loss 0.9365, time 97.06ms, mfu 0.01%\n",
      "iter 857: loss 0.6129, time 100.19ms, mfu 0.01%\n",
      "iter 858: loss 0.8857, time 100.79ms, mfu 0.01%\n",
      "iter 859: loss 0.9268, time 100.85ms, mfu 0.01%\n",
      "iter 860: loss 0.4816, time 98.91ms, mfu 0.01%\n",
      "iter 861: loss 1.3725, time 96.32ms, mfu 0.01%\n",
      "iter 862: loss 0.9835, time 110.64ms, mfu 0.01%\n",
      "iter 863: loss 0.5867, time 96.93ms, mfu 0.01%\n",
      "iter 864: loss 0.6493, time 96.18ms, mfu 0.01%\n",
      "iter 865: loss 0.8839, time 95.72ms, mfu 0.01%\n",
      "iter 866: loss 0.7314, time 95.97ms, mfu 0.01%\n",
      "iter 867: loss 0.7627, time 94.93ms, mfu 0.01%\n",
      "iter 868: loss 0.7842, time 96.92ms, mfu 0.01%\n",
      "iter 869: loss 0.8602, time 96.94ms, mfu 0.01%\n",
      "iter 870: loss 1.1203, time 96.13ms, mfu 0.01%\n",
      "iter 871: loss 0.5611, time 95.55ms, mfu 0.01%\n",
      "iter 872: loss 0.8371, time 94.62ms, mfu 0.01%\n",
      "iter 873: loss 0.6763, time 94.10ms, mfu 0.01%\n",
      "iter 874: loss 1.2618, time 99.36ms, mfu 0.01%\n",
      "iter 875: loss 0.8137, time 101.18ms, mfu 0.01%\n",
      "iter 876: loss 0.7328, time 103.20ms, mfu 0.01%\n",
      "iter 877: loss 0.9278, time 102.22ms, mfu 0.01%\n",
      "iter 878: loss 0.9123, time 99.54ms, mfu 0.01%\n",
      "iter 879: loss 0.8908, time 97.14ms, mfu 0.01%\n",
      "iter 880: loss 0.7225, time 96.23ms, mfu 0.01%\n",
      "iter 881: loss 0.7011, time 96.17ms, mfu 0.01%\n",
      "iter 882: loss 0.5031, time 95.85ms, mfu 0.01%\n",
      "iter 883: loss 0.7118, time 95.38ms, mfu 0.01%\n",
      "iter 884: loss 1.1910, time 93.57ms, mfu 0.01%\n",
      "iter 885: loss 1.0670, time 95.28ms, mfu 0.01%\n",
      "iter 886: loss 1.0224, time 95.17ms, mfu 0.01%\n",
      "iter 887: loss 1.1681, time 94.09ms, mfu 0.01%\n",
      "iter 888: loss 0.7793, time 94.20ms, mfu 0.01%\n",
      "iter 889: loss 0.8624, time 97.01ms, mfu 0.01%\n",
      "iter 890: loss 0.9798, time 96.15ms, mfu 0.01%\n",
      "iter 891: loss 0.9976, time 100.43ms, mfu 0.01%\n",
      "iter 892: loss 0.9345, time 94.29ms, mfu 0.01%\n",
      "iter 893: loss 0.8875, time 94.26ms, mfu 0.01%\n",
      "iter 894: loss 1.0203, time 93.06ms, mfu 0.01%\n",
      "iter 895: loss 0.8334, time 93.66ms, mfu 0.01%\n",
      "iter 896: loss 0.8627, time 135.92ms, mfu 0.01%\n",
      "iter 897: loss 0.5606, time 95.82ms, mfu 0.01%\n",
      "iter 898: loss 0.8492, time 95.03ms, mfu 0.01%\n",
      "iter 899: loss 1.0359, time 95.07ms, mfu 0.01%\n",
      "iter 900: loss 0.5048, time 109.05ms, mfu 0.01%\n",
      "iter 901: loss 0.7241, time 96.08ms, mfu 0.01%\n",
      "iter 902: loss 0.8554, time 92.53ms, mfu 0.01%\n",
      "iter 903: loss 0.5954, time 92.54ms, mfu 0.01%\n",
      "iter 904: loss 0.6501, time 95.03ms, mfu 0.01%\n",
      "iter 905: loss 0.7762, time 94.84ms, mfu 0.01%\n",
      "iter 906: loss 0.7848, time 94.84ms, mfu 0.01%\n",
      "iter 907: loss 0.8865, time 95.67ms, mfu 0.01%\n",
      "iter 908: loss 1.1379, time 94.87ms, mfu 0.01%\n",
      "iter 909: loss 0.8550, time 95.22ms, mfu 0.01%\n",
      "iter 910: loss 0.8804, time 95.19ms, mfu 0.01%\n",
      "iter 911: loss 1.0873, time 96.10ms, mfu 0.01%\n",
      "iter 912: loss 0.8496, time 94.09ms, mfu 0.01%\n",
      "iter 913: loss 1.1307, time 93.00ms, mfu 0.01%\n",
      "iter 914: loss 0.8313, time 93.96ms, mfu 0.01%\n",
      "iter 915: loss 1.0121, time 92.14ms, mfu 0.01%\n",
      "iter 916: loss 0.7835, time 94.40ms, mfu 0.01%\n",
      "iter 917: loss 0.8131, time 95.46ms, mfu 0.01%\n",
      "iter 918: loss 0.7812, time 97.73ms, mfu 0.01%\n",
      "iter 919: loss 0.8185, time 95.54ms, mfu 0.01%\n",
      "iter 920: loss 1.2109, time 96.98ms, mfu 0.01%\n",
      "iter 921: loss 1.1447, time 96.58ms, mfu 0.01%\n",
      "iter 922: loss 0.8406, time 96.48ms, mfu 0.01%\n",
      "iter 923: loss 0.4397, time 95.34ms, mfu 0.01%\n",
      "iter 924: loss 0.6160, time 95.66ms, mfu 0.01%\n",
      "iter 925: loss 0.9216, time 93.25ms, mfu 0.01%\n",
      "iter 926: loss 0.9569, time 93.51ms, mfu 0.01%\n",
      "iter 927: loss 0.8304, time 94.17ms, mfu 0.01%\n",
      "iter 928: loss 0.6420, time 94.59ms, mfu 0.01%\n",
      "iter 929: loss 0.8629, time 94.41ms, mfu 0.01%\n",
      "iter 930: loss 0.5835, time 95.34ms, mfu 0.01%\n",
      "iter 931: loss 1.2069, time 94.03ms, mfu 0.01%\n",
      "iter 932: loss 0.8093, time 94.07ms, mfu 0.01%\n",
      "iter 933: loss 0.7900, time 95.14ms, mfu 0.01%\n",
      "iter 934: loss 1.1314, time 95.55ms, mfu 0.01%\n",
      "iter 935: loss 0.7570, time 93.55ms, mfu 0.01%\n",
      "iter 936: loss 1.0383, time 108.93ms, mfu 0.01%\n",
      "iter 937: loss 1.0653, time 95.68ms, mfu 0.01%\n",
      "iter 938: loss 0.5264, time 93.84ms, mfu 0.01%\n",
      "iter 939: loss 0.5799, time 94.20ms, mfu 0.01%\n",
      "iter 940: loss 1.4410, time 94.10ms, mfu 0.01%\n",
      "iter 941: loss 0.8252, time 94.39ms, mfu 0.01%\n",
      "iter 942: loss 1.0991, time 94.99ms, mfu 0.01%\n",
      "iter 943: loss 0.7131, time 96.21ms, mfu 0.01%\n",
      "iter 944: loss 0.8639, time 145.92ms, mfu 0.01%\n",
      "iter 945: loss 0.8068, time 97.51ms, mfu 0.01%\n",
      "iter 946: loss 1.0149, time 94.61ms, mfu 0.01%\n",
      "iter 947: loss 0.9193, time 93.84ms, mfu 0.01%\n",
      "iter 948: loss 1.0535, time 94.62ms, mfu 0.01%\n",
      "iter 949: loss 0.4595, time 97.78ms, mfu 0.01%\n",
      "iter 950: loss 0.9469, time 99.63ms, mfu 0.01%\n",
      "iter 951: loss 1.2088, time 98.77ms, mfu 0.01%\n",
      "iter 952: loss 1.0631, time 95.05ms, mfu 0.01%\n",
      "iter 953: loss 0.9016, time 96.67ms, mfu 0.01%\n",
      "iter 954: loss 0.7817, time 97.93ms, mfu 0.01%\n",
      "iter 955: loss 0.8308, time 102.37ms, mfu 0.01%\n",
      "iter 956: loss 0.6273, time 101.18ms, mfu 0.01%\n",
      "iter 957: loss 0.7735, time 101.81ms, mfu 0.01%\n",
      "iter 958: loss 0.9166, time 103.38ms, mfu 0.01%\n",
      "iter 959: loss 0.6199, time 99.75ms, mfu 0.01%\n",
      "iter 960: loss 0.5821, time 98.10ms, mfu 0.01%\n",
      "iter 961: loss 1.1553, time 98.02ms, mfu 0.01%\n",
      "iter 962: loss 1.0688, time 97.21ms, mfu 0.01%\n",
      "iter 963: loss 0.9700, time 98.17ms, mfu 0.01%\n",
      "iter 964: loss 1.2008, time 98.06ms, mfu 0.01%\n",
      "iter 965: loss 0.7696, time 98.48ms, mfu 0.01%\n",
      "iter 966: loss 0.7367, time 95.78ms, mfu 0.01%\n",
      "iter 967: loss 0.9827, time 96.63ms, mfu 0.01%\n",
      "iter 968: loss 0.8416, time 94.41ms, mfu 0.01%\n",
      "iter 969: loss 0.8169, time 96.92ms, mfu 0.01%\n",
      "iter 970: loss 0.8036, time 97.47ms, mfu 0.01%\n",
      "iter 971: loss 0.8258, time 114.45ms, mfu 0.01%\n",
      "iter 972: loss 0.6335, time 101.67ms, mfu 0.01%\n",
      "iter 973: loss 0.8796, time 99.19ms, mfu 0.01%\n",
      "iter 974: loss 0.8917, time 95.98ms, mfu 0.01%\n",
      "iter 975: loss 0.5703, time 96.08ms, mfu 0.01%\n",
      "iter 976: loss 0.8306, time 99.73ms, mfu 0.01%\n",
      "iter 977: loss 0.6809, time 102.07ms, mfu 0.01%\n",
      "iter 978: loss 0.6727, time 101.52ms, mfu 0.01%\n",
      "iter 979: loss 0.9597, time 103.94ms, mfu 0.01%\n",
      "iter 980: loss 0.6289, time 100.77ms, mfu 0.01%\n",
      "iter 981: loss 0.6607, time 103.85ms, mfu 0.01%\n",
      "iter 982: loss 0.7437, time 101.94ms, mfu 0.01%\n",
      "iter 983: loss 1.2953, time 101.82ms, mfu 0.01%\n",
      "iter 984: loss 0.7094, time 99.91ms, mfu 0.01%\n",
      "iter 985: loss 0.7663, time 98.78ms, mfu 0.01%\n",
      "iter 986: loss 0.8909, time 98.49ms, mfu 0.01%\n",
      "iter 987: loss 0.9960, time 101.75ms, mfu 0.01%\n",
      "iter 988: loss 0.8942, time 101.53ms, mfu 0.01%\n",
      "iter 989: loss 1.0710, time 101.79ms, mfu 0.01%\n",
      "iter 990: loss 0.7952, time 103.53ms, mfu 0.01%\n",
      "iter 991: loss 1.0515, time 102.73ms, mfu 0.01%\n",
      "iter 992: loss 1.0669, time 103.64ms, mfu 0.01%\n",
      "iter 993: loss 0.7412, time 98.84ms, mfu 0.01%\n",
      "iter 994: loss 0.6434, time 97.61ms, mfu 0.01%\n",
      "iter 995: loss 0.4874, time 98.73ms, mfu 0.01%\n",
      "iter 996: loss 1.0576, time 156.92ms, mfu 0.01%\n",
      "iter 997: loss 0.9692, time 103.78ms, mfu 0.01%\n",
      "iter 998: loss 0.8803, time 102.46ms, mfu 0.01%\n",
      "iter 999: loss 0.7562, time 100.73ms, mfu 0.01%\n",
      "step 1000: train loss 0.8070, val loss 1.4578\n",
      "iter 1000: loss 0.8593, time 1116.34ms, mfu 0.01%\n",
      "iter 1001: loss 0.5558, time 97.98ms, mfu 0.01%\n",
      "iter 1002: loss 1.0272, time 100.14ms, mfu 0.01%\n",
      "iter 1003: loss 0.8694, time 102.77ms, mfu 0.01%\n",
      "iter 1004: loss 1.1257, time 99.62ms, mfu 0.01%\n",
      "iter 1005: loss 0.9673, time 96.18ms, mfu 0.01%\n",
      "iter 1006: loss 0.7171, time 110.49ms, mfu 0.01%\n",
      "iter 1007: loss 1.0409, time 97.34ms, mfu 0.01%\n",
      "iter 1008: loss 0.7870, time 94.79ms, mfu 0.01%\n",
      "iter 1009: loss 0.8116, time 94.40ms, mfu 0.01%\n",
      "iter 1010: loss 0.4522, time 97.94ms, mfu 0.01%\n",
      "iter 1011: loss 0.9032, time 96.29ms, mfu 0.01%\n",
      "iter 1012: loss 0.7985, time 97.20ms, mfu 0.01%\n",
      "iter 1013: loss 0.8292, time 97.46ms, mfu 0.01%\n",
      "iter 1014: loss 0.6584, time 95.54ms, mfu 0.01%\n",
      "iter 1015: loss 0.9457, time 95.50ms, mfu 0.01%\n",
      "iter 1016: loss 1.1604, time 99.70ms, mfu 0.01%\n",
      "iter 1017: loss 1.0596, time 101.13ms, mfu 0.01%\n",
      "iter 1018: loss 0.6068, time 99.01ms, mfu 0.01%\n",
      "iter 1019: loss 0.8191, time 97.89ms, mfu 0.01%\n",
      "iter 1020: loss 1.0624, time 95.38ms, mfu 0.01%\n",
      "iter 1021: loss 0.4694, time 96.40ms, mfu 0.01%\n",
      "iter 1022: loss 1.0534, time 95.57ms, mfu 0.01%\n",
      "iter 1023: loss 0.8750, time 95.83ms, mfu 0.01%\n",
      "iter 1024: loss 0.7306, time 95.37ms, mfu 0.01%\n",
      "iter 1025: loss 1.0575, time 93.86ms, mfu 0.01%\n",
      "iter 1026: loss 0.7152, time 93.18ms, mfu 0.01%\n",
      "iter 1027: loss 1.2142, time 94.54ms, mfu 0.01%\n",
      "iter 1028: loss 0.5072, time 93.69ms, mfu 0.01%\n",
      "iter 1029: loss 0.5967, time 97.57ms, mfu 0.01%\n",
      "iter 1030: loss 0.5111, time 96.57ms, mfu 0.01%\n",
      "iter 1031: loss 0.9209, time 97.18ms, mfu 0.01%\n",
      "iter 1032: loss 0.8215, time 97.83ms, mfu 0.01%\n",
      "iter 1033: loss 0.9078, time 97.07ms, mfu 0.01%\n",
      "iter 1034: loss 0.9022, time 96.01ms, mfu 0.01%\n",
      "iter 1035: loss 0.5540, time 95.52ms, mfu 0.01%\n",
      "iter 1036: loss 0.6731, time 94.30ms, mfu 0.01%\n",
      "iter 1037: loss 1.0021, time 94.28ms, mfu 0.01%\n",
      "iter 1038: loss 1.0924, time 94.82ms, mfu 0.01%\n",
      "iter 1039: loss 1.0322, time 171.03ms, mfu 0.01%\n",
      "iter 1040: loss 0.7086, time 96.80ms, mfu 0.01%\n",
      "iter 1041: loss 1.2744, time 95.34ms, mfu 0.01%\n",
      "iter 1042: loss 0.7165, time 95.23ms, mfu 0.01%\n",
      "iter 1043: loss 0.4775, time 94.57ms, mfu 0.01%\n",
      "iter 1044: loss 0.9249, time 95.57ms, mfu 0.01%\n",
      "iter 1045: loss 0.6043, time 96.62ms, mfu 0.01%\n",
      "iter 1046: loss 0.7754, time 91.32ms, mfu 0.01%\n",
      "iter 1047: loss 0.8923, time 93.46ms, mfu 0.01%\n",
      "iter 1048: loss 1.1048, time 94.11ms, mfu 0.01%\n",
      "iter 1049: loss 0.8818, time 95.10ms, mfu 0.01%\n",
      "iter 1050: loss 0.5267, time 95.72ms, mfu 0.01%\n",
      "iter 1051: loss 0.7225, time 95.06ms, mfu 0.01%\n",
      "iter 1052: loss 1.0045, time 95.25ms, mfu 0.01%\n",
      "iter 1053: loss 0.9358, time 95.61ms, mfu 0.01%\n",
      "iter 1054: loss 0.8254, time 93.95ms, mfu 0.01%\n",
      "iter 1055: loss 1.0057, time 94.63ms, mfu 0.01%\n",
      "iter 1056: loss 0.4825, time 93.49ms, mfu 0.01%\n",
      "iter 1057: loss 0.6920, time 94.94ms, mfu 0.01%\n",
      "iter 1058: loss 1.0342, time 93.21ms, mfu 0.01%\n",
      "iter 1059: loss 1.0208, time 94.71ms, mfu 0.01%\n",
      "iter 1060: loss 0.9289, time 94.37ms, mfu 0.01%\n",
      "iter 1061: loss 0.5615, time 94.43ms, mfu 0.01%\n",
      "iter 1062: loss 1.4831, time 96.62ms, mfu 0.01%\n",
      "iter 1063: loss 0.8936, time 96.17ms, mfu 0.01%\n",
      "iter 1064: loss 0.7547, time 95.32ms, mfu 0.01%\n",
      "iter 1065: loss 0.8000, time 95.59ms, mfu 0.01%\n",
      "iter 1066: loss 0.7384, time 94.21ms, mfu 0.01%\n",
      "iter 1067: loss 0.6964, time 92.89ms, mfu 0.01%\n",
      "iter 1068: loss 0.8483, time 95.00ms, mfu 0.01%\n",
      "iter 1069: loss 0.9908, time 93.49ms, mfu 0.01%\n",
      "iter 1070: loss 0.6239, time 110.00ms, mfu 0.01%\n",
      "iter 1071: loss 1.2247, time 97.40ms, mfu 0.01%\n",
      "iter 1072: loss 0.9474, time 95.86ms, mfu 0.01%\n",
      "iter 1073: loss 0.5833, time 96.57ms, mfu 0.01%\n",
      "iter 1074: loss 0.8788, time 96.47ms, mfu 0.01%\n",
      "iter 1075: loss 0.6278, time 95.67ms, mfu 0.01%\n",
      "iter 1076: loss 0.9276, time 94.34ms, mfu 0.01%\n",
      "iter 1077: loss 0.9434, time 93.50ms, mfu 0.01%\n",
      "iter 1078: loss 1.2360, time 94.17ms, mfu 0.01%\n",
      "iter 1079: loss 0.6959, time 94.27ms, mfu 0.01%\n",
      "iter 1080: loss 0.6395, time 92.14ms, mfu 0.01%\n",
      "iter 1081: loss 1.2194, time 94.62ms, mfu 0.01%\n",
      "iter 1082: loss 0.7754, time 94.42ms, mfu 0.01%\n",
      "iter 1083: loss 1.3812, time 92.24ms, mfu 0.01%\n",
      "iter 1084: loss 0.5177, time 148.45ms, mfu 0.01%\n",
      "iter 1085: loss 0.7652, time 96.08ms, mfu 0.01%\n",
      "iter 1086: loss 0.7247, time 92.68ms, mfu 0.01%\n",
      "iter 1087: loss 1.0118, time 92.64ms, mfu 0.01%\n",
      "iter 1088: loss 0.9020, time 93.06ms, mfu 0.01%\n",
      "iter 1089: loss 0.7786, time 92.89ms, mfu 0.01%\n",
      "iter 1090: loss 1.0715, time 94.38ms, mfu 0.01%\n",
      "iter 1091: loss 0.7903, time 95.03ms, mfu 0.01%\n",
      "iter 1092: loss 0.5961, time 93.79ms, mfu 0.01%\n",
      "iter 1093: loss 0.9338, time 96.15ms, mfu 0.01%\n",
      "iter 1094: loss 0.6800, time 93.87ms, mfu 0.01%\n",
      "iter 1095: loss 0.9900, time 93.59ms, mfu 0.01%\n",
      "iter 1096: loss 1.0962, time 92.59ms, mfu 0.01%\n",
      "iter 1097: loss 0.7008, time 92.53ms, mfu 0.01%\n",
      "iter 1098: loss 0.5956, time 94.21ms, mfu 0.01%\n",
      "iter 1099: loss 1.2247, time 106.78ms, mfu 0.01%\n",
      "iter 1100: loss 0.5332, time 95.85ms, mfu 0.01%\n",
      "iter 1101: loss 0.8855, time 95.51ms, mfu 0.01%\n",
      "iter 1102: loss 0.8723, time 97.59ms, mfu 0.01%\n",
      "iter 1103: loss 0.6869, time 93.59ms, mfu 0.01%\n",
      "iter 1104: loss 0.9892, time 92.80ms, mfu 0.01%\n",
      "iter 1105: loss 0.9676, time 94.40ms, mfu 0.01%\n",
      "iter 1106: loss 0.5727, time 92.49ms, mfu 0.01%\n",
      "iter 1107: loss 0.7535, time 99.60ms, mfu 0.01%\n",
      "iter 1108: loss 0.7296, time 96.33ms, mfu 0.01%\n",
      "iter 1109: loss 1.0373, time 92.19ms, mfu 0.01%\n",
      "iter 1110: loss 0.9427, time 95.09ms, mfu 0.01%\n",
      "iter 1111: loss 0.8717, time 93.34ms, mfu 0.01%\n",
      "iter 1112: loss 0.6900, time 92.82ms, mfu 0.01%\n",
      "iter 1113: loss 0.6142, time 96.01ms, mfu 0.01%\n",
      "iter 1114: loss 0.7493, time 93.92ms, mfu 0.01%\n",
      "iter 1115: loss 1.0172, time 96.10ms, mfu 0.01%\n",
      "iter 1116: loss 0.7326, time 94.70ms, mfu 0.01%\n",
      "iter 1117: loss 0.8880, time 94.58ms, mfu 0.01%\n",
      "iter 1118: loss 0.6395, time 95.44ms, mfu 0.01%\n",
      "iter 1119: loss 0.7935, time 93.76ms, mfu 0.01%\n",
      "iter 1120: loss 0.7717, time 93.92ms, mfu 0.01%\n",
      "iter 1121: loss 0.6683, time 94.22ms, mfu 0.01%\n",
      "iter 1122: loss 0.7013, time 95.08ms, mfu 0.01%\n",
      "iter 1123: loss 0.5873, time 94.62ms, mfu 0.01%\n",
      "iter 1124: loss 0.9113, time 92.95ms, mfu 0.01%\n",
      "iter 1125: loss 0.8147, time 95.32ms, mfu 0.01%\n",
      "iter 1126: loss 0.3930, time 93.48ms, mfu 0.01%\n",
      "iter 1127: loss 0.5976, time 93.47ms, mfu 0.01%\n",
      "iter 1128: loss 0.9315, time 149.21ms, mfu 0.01%\n",
      "iter 1129: loss 0.7207, time 95.69ms, mfu 0.01%\n",
      "iter 1130: loss 1.1535, time 110.80ms, mfu 0.01%\n",
      "iter 1131: loss 1.1071, time 92.82ms, mfu 0.01%\n",
      "iter 1132: loss 0.7417, time 94.95ms, mfu 0.01%\n",
      "iter 1133: loss 1.2659, time 94.55ms, mfu 0.01%\n",
      "iter 1134: loss 0.9509, time 93.21ms, mfu 0.01%\n",
      "iter 1135: loss 0.6922, time 93.97ms, mfu 0.01%\n",
      "iter 1136: loss 0.8359, time 93.46ms, mfu 0.01%\n",
      "iter 1137: loss 0.6084, time 96.11ms, mfu 0.01%\n",
      "iter 1138: loss 0.4501, time 92.90ms, mfu 0.01%\n",
      "iter 1139: loss 0.8473, time 92.21ms, mfu 0.01%\n",
      "iter 1140: loss 0.6465, time 91.93ms, mfu 0.01%\n",
      "iter 1141: loss 0.9537, time 90.75ms, mfu 0.01%\n",
      "iter 1142: loss 0.5824, time 95.54ms, mfu 0.01%\n",
      "iter 1143: loss 0.6949, time 95.37ms, mfu 0.01%\n",
      "iter 1144: loss 0.9313, time 93.77ms, mfu 0.01%\n",
      "iter 1145: loss 0.7326, time 94.36ms, mfu 0.01%\n",
      "iter 1146: loss 0.7262, time 92.92ms, mfu 0.01%\n",
      "iter 1147: loss 0.6837, time 93.09ms, mfu 0.01%\n",
      "iter 1148: loss 0.8865, time 92.04ms, mfu 0.01%\n",
      "iter 1149: loss 0.8874, time 90.51ms, mfu 0.01%\n",
      "iter 1150: loss 0.7933, time 91.55ms, mfu 0.01%\n",
      "iter 1151: loss 0.7722, time 92.12ms, mfu 0.01%\n",
      "iter 1152: loss 0.9922, time 92.10ms, mfu 0.01%\n",
      "iter 1153: loss 0.7524, time 93.74ms, mfu 0.01%\n",
      "iter 1154: loss 0.8821, time 94.68ms, mfu 0.01%\n",
      "iter 1155: loss 0.6072, time 94.83ms, mfu 0.01%\n",
      "iter 1156: loss 0.7659, time 93.36ms, mfu 0.01%\n",
      "iter 1157: loss 0.7835, time 95.54ms, mfu 0.01%\n",
      "iter 1158: loss 0.6808, time 95.19ms, mfu 0.01%\n",
      "iter 1159: loss 1.1439, time 93.68ms, mfu 0.01%\n",
      "iter 1160: loss 1.0917, time 110.87ms, mfu 0.01%\n",
      "iter 1161: loss 0.8401, time 94.99ms, mfu 0.01%\n",
      "iter 1162: loss 0.7527, time 94.34ms, mfu 0.01%\n",
      "iter 1163: loss 0.6609, time 94.32ms, mfu 0.01%\n",
      "iter 1164: loss 0.7310, time 95.34ms, mfu 0.01%\n",
      "iter 1165: loss 1.0709, time 94.43ms, mfu 0.01%\n",
      "iter 1166: loss 0.8285, time 93.18ms, mfu 0.01%\n",
      "iter 1167: loss 0.9640, time 92.59ms, mfu 0.01%\n",
      "iter 1168: loss 0.7951, time 94.43ms, mfu 0.01%\n",
      "iter 1169: loss 0.5460, time 93.18ms, mfu 0.01%\n",
      "iter 1170: loss 0.9301, time 94.00ms, mfu 0.01%\n",
      "iter 1171: loss 0.7671, time 92.90ms, mfu 0.01%\n",
      "iter 1172: loss 0.5698, time 93.63ms, mfu 0.01%\n",
      "iter 1173: loss 0.6577, time 93.90ms, mfu 0.01%\n",
      "iter 1174: loss 0.7865, time 138.33ms, mfu 0.01%\n",
      "iter 1175: loss 0.8740, time 94.62ms, mfu 0.01%\n",
      "iter 1176: loss 0.7372, time 93.57ms, mfu 0.01%\n",
      "iter 1177: loss 1.1089, time 92.32ms, mfu 0.01%\n",
      "iter 1178: loss 0.7560, time 92.59ms, mfu 0.01%\n",
      "iter 1179: loss 1.0193, time 93.17ms, mfu 0.01%\n",
      "iter 1180: loss 0.7431, time 92.39ms, mfu 0.01%\n",
      "iter 1181: loss 0.8436, time 92.40ms, mfu 0.01%\n",
      "iter 1182: loss 0.8899, time 94.17ms, mfu 0.01%\n",
      "iter 1183: loss 0.4211, time 93.58ms, mfu 0.01%\n",
      "iter 1184: loss 0.6154, time 93.86ms, mfu 0.01%\n",
      "iter 1185: loss 1.0006, time 99.12ms, mfu 0.01%\n",
      "iter 1186: loss 0.8914, time 100.63ms, mfu 0.01%\n",
      "iter 1187: loss 1.2764, time 101.02ms, mfu 0.01%\n",
      "iter 1188: loss 0.4737, time 96.79ms, mfu 0.01%\n",
      "iter 1189: loss 0.7970, time 109.67ms, mfu 0.01%\n",
      "iter 1190: loss 0.8909, time 98.33ms, mfu 0.01%\n",
      "iter 1191: loss 0.6969, time 100.25ms, mfu 0.01%\n",
      "iter 1192: loss 0.4563, time 100.39ms, mfu 0.01%\n",
      "iter 1193: loss 0.8773, time 99.12ms, mfu 0.01%\n",
      "iter 1194: loss 0.6957, time 99.87ms, mfu 0.01%\n",
      "iter 1195: loss 0.7218, time 102.09ms, mfu 0.01%\n",
      "iter 1196: loss 0.7375, time 106.70ms, mfu 0.01%\n",
      "iter 1197: loss 0.9744, time 98.84ms, mfu 0.01%\n",
      "iter 1198: loss 1.0815, time 97.17ms, mfu 0.01%\n",
      "iter 1199: loss 0.8910, time 95.86ms, mfu 0.01%\n",
      "iter 1200: loss 1.0126, time 96.65ms, mfu 0.01%\n",
      "iter 1201: loss 0.5375, time 96.44ms, mfu 0.01%\n",
      "iter 1202: loss 0.5885, time 94.99ms, mfu 0.01%\n",
      "iter 1203: loss 0.9023, time 96.99ms, mfu 0.01%\n",
      "iter 1204: loss 0.8506, time 100.26ms, mfu 0.01%\n",
      "iter 1205: loss 0.8295, time 103.99ms, mfu 0.01%\n",
      "iter 1206: loss 1.1200, time 99.26ms, mfu 0.01%\n",
      "iter 1207: loss 0.6800, time 96.13ms, mfu 0.01%\n",
      "iter 1208: loss 0.9372, time 94.77ms, mfu 0.01%\n",
      "iter 1209: loss 0.8417, time 102.30ms, mfu 0.01%\n",
      "iter 1210: loss 0.8448, time 99.05ms, mfu 0.01%\n",
      "iter 1211: loss 0.6385, time 103.37ms, mfu 0.01%\n",
      "iter 1212: loss 1.1354, time 99.30ms, mfu 0.01%\n",
      "iter 1213: loss 0.9796, time 99.40ms, mfu 0.01%\n",
      "iter 1214: loss 0.6347, time 101.25ms, mfu 0.01%\n",
      "iter 1215: loss 0.7395, time 99.88ms, mfu 0.01%\n",
      "iter 1216: loss 0.8091, time 195.94ms, mfu 0.01%\n",
      "iter 1217: loss 0.9971, time 115.75ms, mfu 0.01%\n",
      "iter 1218: loss 0.7605, time 114.52ms, mfu 0.01%\n",
      "iter 1219: loss 0.8207, time 133.24ms, mfu 0.01%\n",
      "iter 1220: loss 0.9007, time 103.79ms, mfu 0.01%\n",
      "iter 1221: loss 0.6945, time 99.30ms, mfu 0.01%\n",
      "iter 1222: loss 0.8682, time 97.92ms, mfu 0.01%\n",
      "iter 1223: loss 1.0437, time 100.10ms, mfu 0.01%\n",
      "iter 1224: loss 0.6672, time 98.06ms, mfu 0.01%\n",
      "iter 1225: loss 0.5967, time 97.15ms, mfu 0.01%\n",
      "iter 1226: loss 1.3127, time 99.34ms, mfu 0.01%\n",
      "iter 1227: loss 0.7025, time 97.42ms, mfu 0.01%\n",
      "iter 1228: loss 0.9555, time 95.27ms, mfu 0.01%\n",
      "iter 1229: loss 0.9807, time 96.30ms, mfu 0.01%\n",
      "iter 1230: loss 0.7276, time 95.01ms, mfu 0.01%\n",
      "iter 1231: loss 0.6460, time 101.51ms, mfu 0.01%\n",
      "iter 1232: loss 0.7079, time 102.73ms, mfu 0.01%\n",
      "iter 1233: loss 0.8876, time 98.76ms, mfu 0.01%\n",
      "iter 1234: loss 0.9172, time 96.30ms, mfu 0.01%\n",
      "iter 1235: loss 0.8068, time 97.88ms, mfu 0.01%\n",
      "iter 1236: loss 0.8678, time 99.10ms, mfu 0.01%\n",
      "iter 1237: loss 0.7390, time 98.09ms, mfu 0.01%\n",
      "iter 1238: loss 0.9685, time 97.05ms, mfu 0.01%\n",
      "iter 1239: loss 0.6770, time 100.68ms, mfu 0.01%\n",
      "iter 1240: loss 0.8834, time 104.49ms, mfu 0.01%\n",
      "iter 1241: loss 1.1103, time 100.12ms, mfu 0.01%\n",
      "iter 1242: loss 1.0709, time 100.59ms, mfu 0.01%\n",
      "iter 1243: loss 0.9280, time 107.87ms, mfu 0.01%\n",
      "iter 1244: loss 0.5065, time 99.07ms, mfu 0.01%\n",
      "iter 1245: loss 1.2198, time 97.42ms, mfu 0.01%\n",
      "iter 1246: loss 0.8895, time 97.37ms, mfu 0.01%\n",
      "iter 1247: loss 0.4550, time 114.51ms, mfu 0.01%\n",
      "iter 1248: loss 0.8511, time 97.86ms, mfu 0.01%\n",
      "iter 1249: loss 0.7455, time 96.54ms, mfu 0.01%\n",
      "step 1250: train loss 0.8395, val loss 1.4254\n",
      "iter 1250: loss 0.8601, time 1192.91ms, mfu 0.01%\n",
      "iter 1251: loss 0.8859, time 101.92ms, mfu 0.01%\n",
      "iter 1252: loss 0.9953, time 108.52ms, mfu 0.01%\n",
      "iter 1253: loss 0.5157, time 99.99ms, mfu 0.01%\n",
      "iter 1254: loss 1.0157, time 100.54ms, mfu 0.01%\n",
      "iter 1255: loss 0.5950, time 102.11ms, mfu 0.01%\n",
      "iter 1256: loss 0.8328, time 99.23ms, mfu 0.01%\n",
      "iter 1257: loss 0.8198, time 97.58ms, mfu 0.01%\n",
      "iter 1258: loss 0.7803, time 102.77ms, mfu 0.01%\n",
      "iter 1259: loss 0.4925, time 100.02ms, mfu 0.01%\n",
      "iter 1260: loss 0.9462, time 99.13ms, mfu 0.01%\n",
      "iter 1261: loss 0.9081, time 97.59ms, mfu 0.01%\n",
      "iter 1262: loss 1.0007, time 99.20ms, mfu 0.01%\n",
      "iter 1263: loss 0.7941, time 98.08ms, mfu 0.01%\n",
      "iter 1264: loss 1.1231, time 98.94ms, mfu 0.01%\n",
      "iter 1265: loss 0.6318, time 98.81ms, mfu 0.01%\n",
      "iter 1266: loss 0.9597, time 99.56ms, mfu 0.01%\n",
      "iter 1267: loss 1.0546, time 97.83ms, mfu 0.01%\n",
      "iter 1268: loss 0.9196, time 98.45ms, mfu 0.01%\n",
      "iter 1269: loss 0.6249, time 99.57ms, mfu 0.01%\n",
      "iter 1270: loss 0.8607, time 99.96ms, mfu 0.01%\n",
      "iter 1271: loss 0.7876, time 103.80ms, mfu 0.01%\n",
      "iter 1272: loss 0.8316, time 99.43ms, mfu 0.01%\n",
      "iter 1273: loss 0.5776, time 102.99ms, mfu 0.01%\n",
      "iter 1274: loss 1.0934, time 101.77ms, mfu 0.01%\n",
      "iter 1275: loss 0.5770, time 100.91ms, mfu 0.01%\n",
      "iter 1276: loss 0.6025, time 114.56ms, mfu 0.01%\n",
      "iter 1277: loss 1.0312, time 98.17ms, mfu 0.01%\n",
      "iter 1278: loss 1.0805, time 97.94ms, mfu 0.01%\n",
      "iter 1279: loss 0.8091, time 97.08ms, mfu 0.01%\n",
      "iter 1280: loss 0.7683, time 97.76ms, mfu 0.01%\n",
      "iter 1281: loss 0.9274, time 98.09ms, mfu 0.01%\n",
      "iter 1282: loss 1.2387, time 97.98ms, mfu 0.01%\n",
      "iter 1283: loss 0.9724, time 97.11ms, mfu 0.01%\n",
      "iter 1284: loss 0.7282, time 107.18ms, mfu 0.01%\n",
      "iter 1285: loss 0.6571, time 103.56ms, mfu 0.01%\n",
      "iter 1286: loss 1.3555, time 97.49ms, mfu 0.01%\n",
      "iter 1287: loss 0.8793, time 98.19ms, mfu 0.01%\n",
      "iter 1288: loss 0.8964, time 98.64ms, mfu 0.01%\n",
      "iter 1289: loss 0.9749, time 99.44ms, mfu 0.01%\n",
      "iter 1290: loss 0.6535, time 96.86ms, mfu 0.01%\n",
      "iter 1291: loss 0.8377, time 151.23ms, mfu 0.01%\n",
      "iter 1292: loss 0.3999, time 107.12ms, mfu 0.01%\n",
      "iter 1293: loss 0.8904, time 98.70ms, mfu 0.01%\n",
      "iter 1294: loss 0.8559, time 99.50ms, mfu 0.01%\n",
      "iter 1295: loss 0.6614, time 97.81ms, mfu 0.01%\n",
      "iter 1296: loss 0.6360, time 96.83ms, mfu 0.01%\n",
      "iter 1297: loss 0.6203, time 99.01ms, mfu 0.01%\n",
      "iter 1298: loss 1.0333, time 100.10ms, mfu 0.01%\n",
      "iter 1299: loss 0.9944, time 97.72ms, mfu 0.01%\n",
      "iter 1300: loss 0.8588, time 97.18ms, mfu 0.01%\n",
      "iter 1301: loss 0.5844, time 96.81ms, mfu 0.01%\n",
      "iter 1302: loss 0.8860, time 111.88ms, mfu 0.01%\n",
      "iter 1303: loss 0.6395, time 104.64ms, mfu 0.01%\n",
      "iter 1304: loss 0.6770, time 101.48ms, mfu 0.01%\n",
      "iter 1305: loss 0.5105, time 98.52ms, mfu 0.01%\n",
      "iter 1306: loss 1.0302, time 101.64ms, mfu 0.01%\n",
      "iter 1307: loss 0.5420, time 97.64ms, mfu 0.01%\n",
      "iter 1308: loss 0.7740, time 100.19ms, mfu 0.01%\n",
      "iter 1309: loss 0.9033, time 96.88ms, mfu 0.01%\n",
      "iter 1310: loss 1.0846, time 85.14ms, mfu 0.01%\n",
      "iter 1311: loss 0.7696, time 97.44ms, mfu 0.01%\n",
      "iter 1312: loss 0.5357, time 99.07ms, mfu 0.01%\n",
      "iter 1313: loss 1.1187, time 97.50ms, mfu 0.01%\n",
      "iter 1314: loss 1.0715, time 98.01ms, mfu 0.01%\n",
      "iter 1315: loss 0.7680, time 103.71ms, mfu 0.01%\n",
      "iter 1316: loss 0.5431, time 98.74ms, mfu 0.01%\n",
      "iter 1317: loss 0.8047, time 102.38ms, mfu 0.01%\n",
      "iter 1318: loss 0.6219, time 103.45ms, mfu 0.01%\n",
      "iter 1319: loss 0.7666, time 100.39ms, mfu 0.01%\n",
      "iter 1320: loss 0.5099, time 101.51ms, mfu 0.01%\n",
      "iter 1321: loss 0.6750, time 99.16ms, mfu 0.01%\n",
      "iter 1322: loss 0.5293, time 99.43ms, mfu 0.01%\n",
      "iter 1323: loss 0.6637, time 108.18ms, mfu 0.01%\n",
      "iter 1324: loss 0.6345, time 99.00ms, mfu 0.01%\n",
      "iter 1325: loss 0.8244, time 99.08ms, mfu 0.01%\n",
      "iter 1326: loss 0.7301, time 99.58ms, mfu 0.01%\n",
      "iter 1327: loss 0.4720, time 97.29ms, mfu 0.01%\n",
      "iter 1328: loss 0.6027, time 110.61ms, mfu 0.01%\n",
      "iter 1329: loss 0.7411, time 98.26ms, mfu 0.01%\n",
      "iter 1330: loss 0.6665, time 96.86ms, mfu 0.01%\n",
      "iter 1331: loss 0.9535, time 90.07ms, mfu 0.01%\n",
      "iter 1332: loss 0.8124, time 85.82ms, mfu 0.01%\n",
      "iter 1333: loss 0.6837, time 163.48ms, mfu 0.01%\n",
      "iter 1334: loss 0.9512, time 97.67ms, mfu 0.01%\n",
      "iter 1335: loss 0.7417, time 97.56ms, mfu 0.01%\n",
      "iter 1336: loss 0.7457, time 103.76ms, mfu 0.01%\n",
      "iter 1337: loss 0.9087, time 99.77ms, mfu 0.01%\n",
      "iter 1338: loss 0.5785, time 99.68ms, mfu 0.01%\n",
      "iter 1339: loss 0.7073, time 99.19ms, mfu 0.01%\n",
      "iter 1340: loss 1.0612, time 101.88ms, mfu 0.01%\n",
      "iter 1341: loss 0.7766, time 98.05ms, mfu 0.01%\n",
      "iter 1342: loss 1.2341, time 100.20ms, mfu 0.01%\n",
      "iter 1343: loss 0.7616, time 100.11ms, mfu 0.01%\n",
      "iter 1344: loss 0.5518, time 98.33ms, mfu 0.01%\n",
      "iter 1345: loss 1.0040, time 100.73ms, mfu 0.01%\n",
      "iter 1346: loss 0.1876, time 97.48ms, mfu 0.01%\n",
      "iter 1347: loss 0.6524, time 96.96ms, mfu 0.01%\n",
      "iter 1348: loss 0.8945, time 99.65ms, mfu 0.01%\n",
      "iter 1349: loss 0.8076, time 97.64ms, mfu 0.01%\n",
      "iter 1350: loss 0.6016, time 96.35ms, mfu 0.01%\n",
      "iter 1351: loss 0.9053, time 97.51ms, mfu 0.01%\n",
      "iter 1352: loss 1.1409, time 100.57ms, mfu 0.01%\n",
      "iter 1353: loss 0.6117, time 116.47ms, mfu 0.01%\n",
      "iter 1354: loss 1.1105, time 99.89ms, mfu 0.01%\n",
      "iter 1355: loss 0.7074, time 96.82ms, mfu 0.01%\n",
      "iter 1356: loss 0.8661, time 103.18ms, mfu 0.01%\n",
      "iter 1357: loss 1.1112, time 98.45ms, mfu 0.01%\n",
      "iter 1358: loss 0.5576, time 96.66ms, mfu 0.01%\n",
      "iter 1359: loss 0.7385, time 98.65ms, mfu 0.01%\n",
      "iter 1360: loss 0.6477, time 97.66ms, mfu 0.01%\n",
      "iter 1361: loss 0.6595, time 98.87ms, mfu 0.01%\n",
      "iter 1362: loss 0.9089, time 99.86ms, mfu 0.01%\n",
      "iter 1363: loss 0.7941, time 110.86ms, mfu 0.01%\n",
      "iter 1364: loss 0.7671, time 97.30ms, mfu 0.01%\n",
      "iter 1365: loss 0.8764, time 96.73ms, mfu 0.01%\n",
      "iter 1366: loss 1.0058, time 100.66ms, mfu 0.01%\n",
      "iter 1367: loss 1.0404, time 98.48ms, mfu 0.01%\n",
      "iter 1368: loss 0.7826, time 98.07ms, mfu 0.01%\n",
      "iter 1369: loss 0.8206, time 106.08ms, mfu 0.01%\n",
      "iter 1370: loss 0.8968, time 97.77ms, mfu 0.01%\n",
      "iter 1371: loss 0.8473, time 100.46ms, mfu 0.01%\n",
      "iter 1372: loss 0.9982, time 101.59ms, mfu 0.01%\n",
      "iter 1373: loss 0.5151, time 102.80ms, mfu 0.01%\n",
      "iter 1374: loss 0.4277, time 98.68ms, mfu 0.01%\n",
      "iter 1375: loss 0.9293, time 98.64ms, mfu 0.01%\n",
      "iter 1376: loss 0.8836, time 97.36ms, mfu 0.01%\n",
      "iter 1377: loss 0.9726, time 163.23ms, mfu 0.01%\n",
      "iter 1378: loss 1.0387, time 111.22ms, mfu 0.01%\n",
      "iter 1379: loss 1.0743, time 97.45ms, mfu 0.01%\n",
      "iter 1380: loss 0.6343, time 100.42ms, mfu 0.01%\n",
      "iter 1381: loss 0.6400, time 97.05ms, mfu 0.01%\n",
      "iter 1382: loss 1.1460, time 98.15ms, mfu 0.01%\n",
      "iter 1383: loss 0.7389, time 98.21ms, mfu 0.01%\n",
      "iter 1384: loss 1.1852, time 98.98ms, mfu 0.01%\n",
      "iter 1385: loss 0.6585, time 98.74ms, mfu 0.01%\n",
      "iter 1386: loss 0.7720, time 97.09ms, mfu 0.01%\n",
      "iter 1387: loss 1.0198, time 101.91ms, mfu 0.01%\n",
      "iter 1388: loss 0.4749, time 95.82ms, mfu 0.01%\n",
      "iter 1389: loss 0.9409, time 98.00ms, mfu 0.01%\n",
      "iter 1390: loss 0.5996, time 98.19ms, mfu 0.01%\n",
      "iter 1391: loss 0.8180, time 97.44ms, mfu 0.01%\n",
      "iter 1392: loss 0.8100, time 97.89ms, mfu 0.01%\n",
      "iter 1393: loss 0.8033, time 98.93ms, mfu 0.01%\n",
      "iter 1394: loss 0.6325, time 98.19ms, mfu 0.01%\n",
      "iter 1395: loss 1.0136, time 96.79ms, mfu 0.01%\n",
      "iter 1396: loss 0.9723, time 96.17ms, mfu 0.01%\n",
      "iter 1397: loss 0.5974, time 100.06ms, mfu 0.01%\n",
      "iter 1398: loss 0.6500, time 96.36ms, mfu 0.01%\n",
      "iter 1399: loss 1.0473, time 95.33ms, mfu 0.01%\n",
      "iter 1400: loss 0.7389, time 97.97ms, mfu 0.01%\n",
      "iter 1401: loss 0.7981, time 97.48ms, mfu 0.01%\n",
      "iter 1402: loss 0.5517, time 96.24ms, mfu 0.01%\n",
      "iter 1403: loss 0.6266, time 97.01ms, mfu 0.01%\n",
      "iter 1404: loss 0.7063, time 114.19ms, mfu 0.01%\n",
      "iter 1405: loss 0.7673, time 97.31ms, mfu 0.01%\n",
      "iter 1406: loss 0.8968, time 96.28ms, mfu 0.01%\n",
      "iter 1407: loss 1.0290, time 95.17ms, mfu 0.01%\n",
      "iter 1408: loss 0.5122, time 95.66ms, mfu 0.01%\n",
      "iter 1409: loss 0.7254, time 96.29ms, mfu 0.01%\n",
      "iter 1410: loss 0.4575, time 98.24ms, mfu 0.01%\n",
      "iter 1411: loss 0.9052, time 99.66ms, mfu 0.01%\n",
      "iter 1412: loss 0.8955, time 97.42ms, mfu 0.01%\n",
      "iter 1413: loss 0.7640, time 95.17ms, mfu 0.01%\n",
      "iter 1414: loss 0.8455, time 97.81ms, mfu 0.01%\n",
      "iter 1415: loss 0.5597, time 147.63ms, mfu 0.01%\n",
      "iter 1416: loss 0.9621, time 105.15ms, mfu 0.01%\n",
      "iter 1417: loss 0.9359, time 97.01ms, mfu 0.01%\n",
      "iter 1418: loss 0.9991, time 94.99ms, mfu 0.01%\n",
      "iter 1419: loss 0.7895, time 95.20ms, mfu 0.01%\n",
      "iter 1420: loss 0.6458, time 97.04ms, mfu 0.01%\n",
      "iter 1421: loss 1.1698, time 98.07ms, mfu 0.01%\n",
      "iter 1422: loss 0.8270, time 97.92ms, mfu 0.01%\n",
      "iter 1423: loss 0.7356, time 97.51ms, mfu 0.01%\n",
      "iter 1424: loss 0.4575, time 98.25ms, mfu 0.01%\n",
      "iter 1425: loss 0.7000, time 97.37ms, mfu 0.01%\n",
      "iter 1426: loss 0.8935, time 97.40ms, mfu 0.01%\n",
      "iter 1427: loss 0.2475, time 95.10ms, mfu 0.01%\n",
      "iter 1428: loss 0.6572, time 96.57ms, mfu 0.01%\n",
      "iter 1429: loss 0.6792, time 94.64ms, mfu 0.01%\n",
      "iter 1430: loss 0.5088, time 110.73ms, mfu 0.01%\n",
      "iter 1431: loss 0.5700, time 98.92ms, mfu 0.01%\n",
      "iter 1432: loss 1.2610, time 96.14ms, mfu 0.01%\n",
      "iter 1433: loss 0.9825, time 96.14ms, mfu 0.01%\n",
      "iter 1434: loss 0.6877, time 98.79ms, mfu 0.01%\n",
      "iter 1435: loss 1.0861, time 98.56ms, mfu 0.01%\n",
      "iter 1436: loss 0.9362, time 97.15ms, mfu 0.01%\n",
      "iter 1437: loss 1.1061, time 97.88ms, mfu 0.01%\n",
      "iter 1438: loss 0.7353, time 97.78ms, mfu 0.01%\n",
      "iter 1439: loss 0.5887, time 97.07ms, mfu 0.01%\n",
      "iter 1440: loss 0.9760, time 98.27ms, mfu 0.01%\n",
      "iter 1441: loss 0.8562, time 99.33ms, mfu 0.01%\n",
      "iter 1442: loss 0.8924, time 98.32ms, mfu 0.01%\n",
      "iter 1443: loss 0.8111, time 97.84ms, mfu 0.01%\n",
      "iter 1444: loss 0.8851, time 98.67ms, mfu 0.01%\n",
      "iter 1445: loss 0.9596, time 99.58ms, mfu 0.01%\n",
      "iter 1446: loss 1.0225, time 97.15ms, mfu 0.01%\n",
      "iter 1447: loss 0.5815, time 96.64ms, mfu 0.01%\n",
      "iter 1448: loss 1.0343, time 96.65ms, mfu 0.01%\n",
      "iter 1449: loss 0.9707, time 96.88ms, mfu 0.01%\n",
      "iter 1450: loss 0.9286, time 97.08ms, mfu 0.01%\n",
      "iter 1451: loss 0.9221, time 99.68ms, mfu 0.01%\n",
      "iter 1452: loss 1.0320, time 97.27ms, mfu 0.01%\n",
      "iter 1453: loss 0.4549, time 97.22ms, mfu 0.01%\n",
      "iter 1454: loss 0.6726, time 149.85ms, mfu 0.01%\n",
      "iter 1455: loss 0.9384, time 103.25ms, mfu 0.01%\n",
      "iter 1456: loss 0.7183, time 111.06ms, mfu 0.01%\n",
      "iter 1457: loss 0.7199, time 98.57ms, mfu 0.01%\n",
      "iter 1458: loss 0.7211, time 98.48ms, mfu 0.01%\n",
      "iter 1459: loss 0.9617, time 96.84ms, mfu 0.01%\n",
      "iter 1460: loss 0.7055, time 97.75ms, mfu 0.01%\n",
      "iter 1461: loss 0.6740, time 99.03ms, mfu 0.01%\n",
      "iter 1462: loss 0.7053, time 98.88ms, mfu 0.01%\n",
      "iter 1463: loss 0.5750, time 97.99ms, mfu 0.01%\n",
      "iter 1464: loss 0.6442, time 99.80ms, mfu 0.01%\n",
      "iter 1465: loss 0.8076, time 99.42ms, mfu 0.01%\n",
      "iter 1466: loss 0.8729, time 98.15ms, mfu 0.01%\n",
      "iter 1467: loss 0.8114, time 97.80ms, mfu 0.01%\n",
      "iter 1468: loss 0.5241, time 96.54ms, mfu 0.01%\n",
      "iter 1469: loss 0.8186, time 95.47ms, mfu 0.01%\n",
      "iter 1470: loss 0.7326, time 96.44ms, mfu 0.01%\n",
      "iter 1471: loss 0.8714, time 98.36ms, mfu 0.01%\n",
      "iter 1472: loss 0.7459, time 97.67ms, mfu 0.01%\n",
      "iter 1473: loss 0.9683, time 97.68ms, mfu 0.01%\n",
      "iter 1474: loss 0.5976, time 98.11ms, mfu 0.01%\n",
      "iter 1475: loss 0.7157, time 98.38ms, mfu 0.01%\n",
      "iter 1476: loss 0.7640, time 97.42ms, mfu 0.01%\n",
      "iter 1477: loss 0.8697, time 96.49ms, mfu 0.01%\n",
      "iter 1478: loss 0.7662, time 96.52ms, mfu 0.01%\n",
      "iter 1479: loss 0.6812, time 111.89ms, mfu 0.01%\n",
      "iter 1480: loss 0.7975, time 96.52ms, mfu 0.01%\n",
      "iter 1481: loss 0.7517, time 98.81ms, mfu 0.01%\n",
      "iter 1482: loss 0.9827, time 103.33ms, mfu 0.01%\n",
      "iter 1483: loss 0.8500, time 99.36ms, mfu 0.01%\n",
      "iter 1484: loss 0.8387, time 98.98ms, mfu 0.01%\n",
      "iter 1485: loss 0.2266, time 97.86ms, mfu 0.01%\n",
      "iter 1486: loss 0.9731, time 99.51ms, mfu 0.01%\n",
      "iter 1487: loss 0.9322, time 97.86ms, mfu 0.01%\n",
      "iter 1488: loss 0.8965, time 96.60ms, mfu 0.01%\n",
      "iter 1489: loss 0.7530, time 98.04ms, mfu 0.01%\n",
      "iter 1490: loss 0.7308, time 102.59ms, mfu 0.01%\n",
      "iter 1491: loss 0.2597, time 142.51ms, mfu 0.01%\n",
      "iter 1492: loss 0.7635, time 121.93ms, mfu 0.01%\n",
      "iter 1493: loss 0.7733, time 107.89ms, mfu 0.01%\n",
      "iter 1494: loss 0.7528, time 107.28ms, mfu 0.01%\n",
      "iter 1495: loss 0.8764, time 159.21ms, mfu 0.01%\n",
      "iter 1496: loss 0.6008, time 100.29ms, mfu 0.01%\n",
      "iter 1497: loss 0.5672, time 100.00ms, mfu 0.01%\n",
      "iter 1498: loss 0.4066, time 99.44ms, mfu 0.01%\n",
      "iter 1499: loss 0.6902, time 98.45ms, mfu 0.01%\n",
      "step 1500: train loss 0.7978, val loss 1.4074\n",
      "iter 1500: loss 0.6570, time 1151.46ms, mfu 0.01%\n",
      "iter 1501: loss 1.0006, time 116.44ms, mfu 0.01%\n",
      "iter 1502: loss 0.7992, time 99.35ms, mfu 0.01%\n",
      "iter 1503: loss 0.5934, time 97.79ms, mfu 0.01%\n",
      "iter 1504: loss 0.6241, time 98.58ms, mfu 0.01%\n",
      "iter 1505: loss 0.8381, time 98.55ms, mfu 0.01%\n",
      "iter 1506: loss 0.9558, time 100.97ms, mfu 0.01%\n",
      "iter 1507: loss 0.7864, time 98.92ms, mfu 0.01%\n",
      "iter 1508: loss 1.2959, time 99.78ms, mfu 0.01%\n",
      "iter 1509: loss 0.8945, time 101.70ms, mfu 0.01%\n",
      "iter 1510: loss 0.9430, time 99.99ms, mfu 0.01%\n",
      "iter 1511: loss 0.3326, time 97.68ms, mfu 0.01%\n",
      "iter 1512: loss 0.6540, time 97.05ms, mfu 0.01%\n",
      "iter 1513: loss 0.4891, time 101.12ms, mfu 0.01%\n",
      "iter 1514: loss 0.5883, time 97.87ms, mfu 0.01%\n",
      "iter 1515: loss 1.0292, time 101.43ms, mfu 0.01%\n",
      "iter 1516: loss 0.6115, time 101.37ms, mfu 0.01%\n",
      "iter 1517: loss 0.5405, time 97.82ms, mfu 0.01%\n",
      "iter 1518: loss 0.8393, time 97.53ms, mfu 0.01%\n",
      "iter 1519: loss 0.6781, time 97.94ms, mfu 0.01%\n",
      "iter 1520: loss 0.7702, time 98.01ms, mfu 0.01%\n",
      "iter 1521: loss 0.7218, time 96.81ms, mfu 0.01%\n",
      "iter 1522: loss 0.9984, time 99.07ms, mfu 0.01%\n",
      "iter 1523: loss 0.5079, time 101.66ms, mfu 0.01%\n",
      "iter 1524: loss 0.9709, time 115.97ms, mfu 0.01%\n",
      "iter 1525: loss 0.7215, time 101.02ms, mfu 0.01%\n",
      "iter 1526: loss 0.8003, time 99.25ms, mfu 0.01%\n",
      "iter 1527: loss 0.8272, time 100.44ms, mfu 0.01%\n",
      "iter 1528: loss 0.6322, time 100.32ms, mfu 0.01%\n",
      "iter 1529: loss 0.9165, time 101.70ms, mfu 0.01%\n",
      "iter 1530: loss 0.6287, time 99.66ms, mfu 0.01%\n",
      "iter 1531: loss 0.7319, time 97.73ms, mfu 0.01%\n",
      "iter 1532: loss 0.7466, time 166.59ms, mfu 0.01%\n",
      "iter 1533: loss 1.0677, time 98.15ms, mfu 0.01%\n",
      "iter 1534: loss 0.9599, time 100.20ms, mfu 0.01%\n",
      "iter 1535: loss 0.5791, time 97.81ms, mfu 0.01%\n",
      "iter 1536: loss 0.8689, time 100.88ms, mfu 0.01%\n",
      "iter 1537: loss 0.8392, time 99.85ms, mfu 0.01%\n",
      "iter 1538: loss 0.8691, time 97.92ms, mfu 0.01%\n",
      "iter 1539: loss 0.5582, time 97.98ms, mfu 0.01%\n",
      "iter 1540: loss 0.9539, time 112.14ms, mfu 0.01%\n",
      "iter 1541: loss 0.4528, time 105.95ms, mfu 0.01%\n",
      "iter 1542: loss 1.0183, time 104.23ms, mfu 0.01%\n",
      "iter 1543: loss 1.1803, time 98.17ms, mfu 0.01%\n",
      "iter 1544: loss 0.4956, time 100.17ms, mfu 0.01%\n",
      "iter 1545: loss 0.8687, time 97.86ms, mfu 0.01%\n",
      "iter 1546: loss 1.1156, time 96.98ms, mfu 0.01%\n",
      "iter 1547: loss 0.9543, time 116.22ms, mfu 0.01%\n",
      "iter 1548: loss 0.6995, time 97.45ms, mfu 0.01%\n",
      "iter 1549: loss 0.8716, time 99.12ms, mfu 0.01%\n",
      "iter 1550: loss 0.8988, time 98.19ms, mfu 0.01%\n",
      "iter 1551: loss 0.7255, time 97.46ms, mfu 0.01%\n",
      "iter 1552: loss 0.9215, time 100.60ms, mfu 0.01%\n",
      "iter 1553: loss 1.0150, time 98.66ms, mfu 0.01%\n",
      "iter 1554: loss 0.6869, time 97.30ms, mfu 0.01%\n",
      "iter 1555: loss 0.8695, time 99.82ms, mfu 0.01%\n",
      "iter 1556: loss 1.1282, time 99.69ms, mfu 0.01%\n",
      "iter 1557: loss 0.7829, time 96.54ms, mfu 0.01%\n",
      "iter 1558: loss 0.6967, time 97.77ms, mfu 0.01%\n",
      "iter 1559: loss 0.9399, time 97.90ms, mfu 0.01%\n",
      "iter 1560: loss 0.5966, time 97.83ms, mfu 0.01%\n",
      "iter 1561: loss 0.6326, time 97.70ms, mfu 0.01%\n",
      "iter 1562: loss 0.8758, time 100.84ms, mfu 0.01%\n",
      "iter 1563: loss 1.0594, time 97.01ms, mfu 0.01%\n",
      "iter 1564: loss 0.9442, time 100.18ms, mfu 0.01%\n",
      "iter 1565: loss 0.8368, time 97.37ms, mfu 0.01%\n",
      "iter 1566: loss 0.8364, time 97.65ms, mfu 0.01%\n",
      "iter 1567: loss 0.5116, time 97.53ms, mfu 0.01%\n",
      "iter 1568: loss 0.9203, time 98.51ms, mfu 0.01%\n",
      "iter 1569: loss 0.5895, time 100.44ms, mfu 0.01%\n",
      "iter 1570: loss 0.9335, time 193.62ms, mfu 0.01%\n",
      "iter 1571: loss 0.5489, time 97.92ms, mfu 0.01%\n",
      "iter 1572: loss 1.0210, time 101.62ms, mfu 0.01%\n",
      "iter 1573: loss 0.7997, time 97.40ms, mfu 0.01%\n",
      "iter 1574: loss 0.6190, time 98.13ms, mfu 0.01%\n",
      "iter 1575: loss 0.6109, time 98.17ms, mfu 0.01%\n",
      "iter 1576: loss 1.0507, time 97.65ms, mfu 0.01%\n",
      "iter 1577: loss 0.5211, time 96.31ms, mfu 0.01%\n",
      "iter 1578: loss 0.9504, time 99.57ms, mfu 0.01%\n",
      "iter 1579: loss 0.7082, time 97.30ms, mfu 0.01%\n",
      "iter 1580: loss 0.9700, time 97.06ms, mfu 0.01%\n",
      "iter 1581: loss 1.1257, time 97.37ms, mfu 0.01%\n",
      "iter 1582: loss 0.8285, time 96.80ms, mfu 0.01%\n",
      "iter 1583: loss 0.6960, time 97.83ms, mfu 0.01%\n",
      "iter 1584: loss 0.6177, time 97.07ms, mfu 0.01%\n",
      "iter 1585: loss 0.5257, time 100.21ms, mfu 0.01%\n",
      "iter 1586: loss 0.7630, time 96.65ms, mfu 0.01%\n",
      "iter 1587: loss 1.1734, time 97.62ms, mfu 0.01%\n",
      "iter 1588: loss 0.7950, time 98.48ms, mfu 0.01%\n",
      "iter 1589: loss 1.1312, time 99.25ms, mfu 0.01%\n",
      "iter 1590: loss 0.5544, time 102.42ms, mfu 0.01%\n",
      "iter 1591: loss 0.7391, time 98.35ms, mfu 0.01%\n",
      "iter 1592: loss 0.4264, time 101.83ms, mfu 0.01%\n",
      "iter 1593: loss 0.7205, time 111.23ms, mfu 0.01%\n",
      "iter 1594: loss 0.8272, time 99.10ms, mfu 0.01%\n",
      "iter 1595: loss 0.6614, time 98.34ms, mfu 0.01%\n",
      "iter 1596: loss 0.8029, time 97.43ms, mfu 0.01%\n",
      "iter 1597: loss 0.9827, time 97.48ms, mfu 0.01%\n",
      "iter 1598: loss 0.6929, time 97.54ms, mfu 0.01%\n",
      "iter 1599: loss 0.9123, time 100.94ms, mfu 0.01%\n",
      "iter 1600: loss 0.4737, time 97.79ms, mfu 0.01%\n",
      "iter 1601: loss 0.7630, time 98.26ms, mfu 0.01%\n",
      "iter 1602: loss 0.7635, time 99.61ms, mfu 0.01%\n",
      "iter 1603: loss 0.9723, time 104.39ms, mfu 0.01%\n",
      "iter 1604: loss 0.8658, time 106.64ms, mfu 0.01%\n",
      "iter 1605: loss 0.8089, time 104.80ms, mfu 0.01%\n",
      "iter 1606: loss 0.8296, time 101.39ms, mfu 0.01%\n",
      "iter 1607: loss 0.7635, time 152.18ms, mfu 0.01%\n",
      "iter 1608: loss 1.2277, time 98.60ms, mfu 0.01%\n",
      "iter 1609: loss 0.8414, time 97.77ms, mfu 0.01%\n",
      "iter 1610: loss 0.7906, time 98.67ms, mfu 0.01%\n",
      "iter 1611: loss 0.6915, time 97.47ms, mfu 0.01%\n",
      "iter 1612: loss 0.7061, time 97.80ms, mfu 0.01%\n",
      "iter 1613: loss 0.7706, time 96.90ms, mfu 0.01%\n",
      "iter 1614: loss 0.6167, time 97.39ms, mfu 0.01%\n",
      "iter 1615: loss 0.3692, time 98.75ms, mfu 0.01%\n",
      "iter 1616: loss 0.8736, time 110.35ms, mfu 0.01%\n",
      "iter 1617: loss 0.7604, time 96.85ms, mfu 0.01%\n",
      "iter 1618: loss 0.9034, time 97.51ms, mfu 0.01%\n",
      "iter 1619: loss 0.6360, time 96.50ms, mfu 0.01%\n",
      "iter 1620: loss 0.6949, time 96.65ms, mfu 0.01%\n",
      "iter 1621: loss 0.9785, time 97.58ms, mfu 0.01%\n",
      "iter 1622: loss 0.9569, time 97.29ms, mfu 0.01%\n",
      "iter 1623: loss 0.6600, time 97.71ms, mfu 0.01%\n",
      "iter 1624: loss 0.8009, time 106.15ms, mfu 0.01%\n",
      "iter 1625: loss 0.8642, time 100.48ms, mfu 0.01%\n",
      "iter 1626: loss 0.6610, time 100.58ms, mfu 0.01%\n",
      "iter 1627: loss 0.6201, time 101.77ms, mfu 0.01%\n",
      "iter 1628: loss 0.5582, time 98.34ms, mfu 0.01%\n",
      "iter 1629: loss 0.4882, time 96.83ms, mfu 0.01%\n",
      "iter 1630: loss 0.5745, time 97.23ms, mfu 0.01%\n",
      "iter 1631: loss 1.0582, time 99.34ms, mfu 0.01%\n",
      "iter 1632: loss 0.6746, time 103.79ms, mfu 0.01%\n",
      "iter 1633: loss 0.9134, time 98.60ms, mfu 0.01%\n",
      "iter 1634: loss 1.1182, time 96.50ms, mfu 0.01%\n",
      "iter 1635: loss 0.7625, time 98.00ms, mfu 0.01%\n",
      "iter 1636: loss 0.4411, time 97.53ms, mfu 0.01%\n",
      "iter 1637: loss 0.8599, time 109.73ms, mfu 0.01%\n",
      "iter 1638: loss 0.7362, time 108.10ms, mfu 0.01%\n",
      "iter 1639: loss 0.5653, time 107.20ms, mfu 0.01%\n",
      "iter 1640: loss 0.9587, time 104.46ms, mfu 0.01%\n",
      "iter 1641: loss 0.8081, time 103.08ms, mfu 0.01%\n",
      "iter 1642: loss 1.1142, time 106.46ms, mfu 0.01%\n",
      "iter 1643: loss 0.8229, time 163.65ms, mfu 0.01%\n",
      "iter 1644: loss 0.7969, time 98.50ms, mfu 0.01%\n",
      "iter 1645: loss 0.6092, time 99.25ms, mfu 0.01%\n",
      "iter 1646: loss 0.8706, time 105.38ms, mfu 0.01%\n",
      "iter 1647: loss 0.5222, time 116.85ms, mfu 0.01%\n",
      "iter 1648: loss 0.7702, time 113.37ms, mfu 0.01%\n",
      "iter 1649: loss 0.8429, time 104.04ms, mfu 0.01%\n",
      "iter 1650: loss 1.0245, time 119.37ms, mfu 0.01%\n",
      "iter 1651: loss 0.6168, time 104.12ms, mfu 0.01%\n",
      "iter 1652: loss 0.7227, time 115.06ms, mfu 0.01%\n",
      "iter 1653: loss 0.5999, time 106.88ms, mfu 0.01%\n",
      "iter 1654: loss 1.1173, time 112.44ms, mfu 0.01%\n",
      "iter 1655: loss 0.8250, time 104.11ms, mfu 0.01%\n",
      "iter 1656: loss 0.7119, time 98.72ms, mfu 0.01%\n",
      "iter 1657: loss 0.5330, time 97.93ms, mfu 0.01%\n",
      "iter 1658: loss 0.7212, time 98.55ms, mfu 0.01%\n",
      "iter 1659: loss 0.8779, time 98.89ms, mfu 0.01%\n",
      "iter 1660: loss 0.7886, time 112.93ms, mfu 0.01%\n",
      "iter 1661: loss 0.8915, time 98.67ms, mfu 0.01%\n",
      "iter 1662: loss 0.8161, time 98.34ms, mfu 0.01%\n",
      "iter 1663: loss 0.9795, time 96.89ms, mfu 0.01%\n",
      "iter 1664: loss 0.6667, time 98.47ms, mfu 0.01%\n",
      "iter 1665: loss 0.7417, time 105.33ms, mfu 0.01%\n",
      "iter 1666: loss 0.8992, time 99.45ms, mfu 0.01%\n",
      "iter 1667: loss 0.5754, time 100.89ms, mfu 0.01%\n",
      "iter 1668: loss 0.6946, time 98.63ms, mfu 0.01%\n",
      "iter 1669: loss 0.7367, time 100.60ms, mfu 0.01%\n",
      "iter 1670: loss 0.6244, time 101.52ms, mfu 0.01%\n",
      "iter 1671: loss 1.0597, time 97.75ms, mfu 0.01%\n",
      "iter 1672: loss 0.5069, time 96.95ms, mfu 0.01%\n",
      "iter 1673: loss 0.5859, time 97.35ms, mfu 0.01%\n",
      "iter 1674: loss 0.8577, time 97.05ms, mfu 0.01%\n",
      "iter 1675: loss 0.7262, time 97.54ms, mfu 0.01%\n",
      "iter 1676: loss 0.8196, time 96.50ms, mfu 0.01%\n",
      "iter 1677: loss 0.5414, time 96.63ms, mfu 0.01%\n",
      "iter 1678: loss 0.5513, time 161.16ms, mfu 0.01%\n",
      "iter 1679: loss 0.7446, time 97.53ms, mfu 0.01%\n",
      "iter 1680: loss 0.5522, time 97.95ms, mfu 0.01%\n",
      "iter 1681: loss 0.6647, time 101.31ms, mfu 0.01%\n",
      "iter 1682: loss 0.8597, time 112.48ms, mfu 0.01%\n",
      "iter 1683: loss 0.7859, time 96.45ms, mfu 0.01%\n",
      "iter 1684: loss 0.8332, time 96.75ms, mfu 0.01%\n",
      "iter 1685: loss 0.6032, time 97.09ms, mfu 0.01%\n",
      "iter 1686: loss 0.6837, time 97.27ms, mfu 0.01%\n",
      "iter 1687: loss 0.7493, time 99.89ms, mfu 0.01%\n",
      "iter 1688: loss 0.8130, time 103.48ms, mfu 0.01%\n",
      "iter 1689: loss 0.7506, time 104.95ms, mfu 0.01%\n",
      "iter 1690: loss 0.9335, time 101.97ms, mfu 0.01%\n",
      "iter 1691: loss 1.1849, time 100.78ms, mfu 0.01%\n",
      "iter 1692: loss 1.0030, time 100.08ms, mfu 0.01%\n",
      "iter 1693: loss 0.9108, time 97.49ms, mfu 0.01%\n",
      "iter 1694: loss 0.9019, time 97.59ms, mfu 0.01%\n",
      "iter 1695: loss 0.9317, time 96.98ms, mfu 0.01%\n",
      "iter 1696: loss 0.8581, time 100.48ms, mfu 0.01%\n",
      "iter 1697: loss 0.7737, time 103.17ms, mfu 0.01%\n",
      "iter 1698: loss 0.8774, time 98.94ms, mfu 0.01%\n",
      "iter 1699: loss 0.6542, time 100.24ms, mfu 0.01%\n",
      "iter 1700: loss 0.7458, time 97.65ms, mfu 0.01%\n",
      "iter 1701: loss 0.8226, time 97.33ms, mfu 0.01%\n",
      "iter 1702: loss 0.9310, time 95.97ms, mfu 0.01%\n",
      "iter 1703: loss 0.9862, time 95.68ms, mfu 0.01%\n",
      "iter 1704: loss 0.5947, time 95.75ms, mfu 0.01%\n",
      "iter 1705: loss 0.7411, time 110.44ms, mfu 0.01%\n",
      "iter 1706: loss 0.9441, time 96.07ms, mfu 0.01%\n",
      "iter 1707: loss 0.9683, time 95.97ms, mfu 0.01%\n",
      "iter 1708: loss 0.6082, time 96.97ms, mfu 0.01%\n",
      "iter 1709: loss 0.7136, time 96.41ms, mfu 0.01%\n",
      "iter 1710: loss 0.5787, time 96.76ms, mfu 0.01%\n",
      "iter 1711: loss 0.9200, time 154.38ms, mfu 0.01%\n",
      "iter 1712: loss 0.7397, time 96.32ms, mfu 0.01%\n",
      "iter 1713: loss 0.6526, time 94.43ms, mfu 0.01%\n",
      "iter 1714: loss 0.4674, time 96.13ms, mfu 0.01%\n",
      "iter 1715: loss 0.8164, time 95.63ms, mfu 0.01%\n",
      "iter 1716: loss 0.7043, time 95.66ms, mfu 0.01%\n",
      "iter 1717: loss 0.8346, time 95.50ms, mfu 0.01%\n",
      "iter 1718: loss 0.8165, time 94.49ms, mfu 0.01%\n",
      "iter 1719: loss 0.6879, time 95.32ms, mfu 0.01%\n",
      "iter 1720: loss 0.8519, time 95.61ms, mfu 0.01%\n",
      "iter 1721: loss 0.7631, time 96.81ms, mfu 0.01%\n",
      "iter 1722: loss 0.7980, time 100.06ms, mfu 0.01%\n",
      "iter 1723: loss 0.8068, time 100.99ms, mfu 0.01%\n",
      "iter 1724: loss 0.9929, time 101.43ms, mfu 0.01%\n",
      "iter 1725: loss 0.4355, time 102.59ms, mfu 0.01%\n",
      "iter 1726: loss 0.5594, time 100.25ms, mfu 0.01%\n",
      "iter 1727: loss 0.6031, time 100.59ms, mfu 0.01%\n",
      "iter 1728: loss 0.6240, time 124.83ms, mfu 0.01%\n",
      "iter 1729: loss 0.6140, time 99.03ms, mfu 0.01%\n",
      "iter 1730: loss 0.6145, time 103.69ms, mfu 0.01%\n",
      "iter 1731: loss 0.5690, time 146.05ms, mfu 0.01%\n",
      "iter 1732: loss 0.5869, time 109.98ms, mfu 0.01%\n",
      "iter 1733: loss 0.9963, time 103.77ms, mfu 0.01%\n",
      "iter 1734: loss 0.6285, time 98.38ms, mfu 0.01%\n",
      "iter 1735: loss 0.8041, time 103.62ms, mfu 0.01%\n",
      "iter 1736: loss 0.6479, time 98.69ms, mfu 0.01%\n",
      "iter 1737: loss 0.6500, time 96.63ms, mfu 0.01%\n",
      "iter 1738: loss 0.4143, time 97.03ms, mfu 0.01%\n",
      "iter 1739: loss 1.1333, time 96.14ms, mfu 0.01%\n",
      "iter 1740: loss 0.8714, time 96.25ms, mfu 0.01%\n",
      "iter 1741: loss 0.5848, time 96.36ms, mfu 0.01%\n",
      "iter 1742: loss 0.7624, time 96.26ms, mfu 0.01%\n",
      "iter 1743: loss 0.7897, time 96.88ms, mfu 0.01%\n",
      "iter 1744: loss 0.9204, time 155.44ms, mfu 0.01%\n",
      "iter 1745: loss 0.6557, time 97.49ms, mfu 0.01%\n",
      "iter 1746: loss 0.6896, time 96.94ms, mfu 0.01%\n",
      "iter 1747: loss 0.7389, time 96.44ms, mfu 0.01%\n",
      "iter 1748: loss 0.8740, time 96.98ms, mfu 0.01%\n",
      "iter 1749: loss 0.4807, time 96.64ms, mfu 0.01%\n",
      "step 1750: train loss 0.6964, val loss 1.6481\n",
      "iter 1750: loss 0.7939, time 1197.80ms, mfu 0.01%\n",
      "iter 1751: loss 0.9526, time 197.63ms, mfu 0.01%\n",
      "iter 1752: loss 0.7962, time 142.99ms, mfu 0.01%\n",
      "iter 1753: loss 0.8562, time 124.27ms, mfu 0.01%\n",
      "iter 1754: loss 0.7645, time 111.99ms, mfu 0.01%\n",
      "iter 1755: loss 0.7710, time 105.71ms, mfu 0.01%\n",
      "iter 1756: loss 0.8852, time 97.30ms, mfu 0.01%\n",
      "iter 1757: loss 1.0903, time 96.91ms, mfu 0.01%\n",
      "iter 1758: loss 0.6114, time 97.74ms, mfu 0.01%\n",
      "iter 1759: loss 0.9121, time 103.11ms, mfu 0.01%\n",
      "iter 1760: loss 0.8867, time 99.06ms, mfu 0.01%\n",
      "iter 1761: loss 0.6016, time 98.21ms, mfu 0.01%\n",
      "iter 1762: loss 0.5093, time 144.01ms, mfu 0.01%\n",
      "iter 1763: loss 0.7510, time 97.96ms, mfu 0.01%\n",
      "iter 1764: loss 0.8231, time 97.78ms, mfu 0.01%\n",
      "iter 1765: loss 0.7032, time 97.05ms, mfu 0.01%\n",
      "iter 1766: loss 0.9460, time 95.63ms, mfu 0.01%\n",
      "iter 1767: loss 0.7433, time 94.72ms, mfu 0.01%\n",
      "iter 1768: loss 0.8044, time 95.68ms, mfu 0.01%\n",
      "iter 1769: loss 0.8648, time 96.22ms, mfu 0.01%\n",
      "iter 1770: loss 0.5416, time 96.43ms, mfu 0.01%\n",
      "iter 1771: loss 0.7100, time 96.58ms, mfu 0.01%\n",
      "iter 1772: loss 0.2927, time 96.00ms, mfu 0.01%\n",
      "iter 1773: loss 0.6017, time 112.88ms, mfu 0.01%\n",
      "iter 1774: loss 0.9387, time 95.31ms, mfu 0.01%\n",
      "iter 1775: loss 0.6711, time 94.86ms, mfu 0.01%\n",
      "iter 1776: loss 0.9981, time 96.21ms, mfu 0.01%\n",
      "iter 1777: loss 0.5046, time 96.59ms, mfu 0.01%\n",
      "iter 1778: loss 0.4617, time 95.31ms, mfu 0.01%\n",
      "iter 1779: loss 0.9787, time 94.76ms, mfu 0.01%\n",
      "iter 1780: loss 0.4574, time 95.17ms, mfu 0.01%\n",
      "iter 1781: loss 0.8651, time 107.41ms, mfu 0.01%\n",
      "iter 1782: loss 0.9805, time 104.58ms, mfu 0.01%\n",
      "iter 1783: loss 0.9478, time 98.99ms, mfu 0.01%\n",
      "iter 1784: loss 0.6834, time 96.79ms, mfu 0.01%\n",
      "iter 1785: loss 0.4371, time 96.67ms, mfu 0.01%\n",
      "iter 1786: loss 0.8434, time 96.31ms, mfu 0.01%\n",
      "iter 1787: loss 0.8285, time 95.16ms, mfu 0.01%\n",
      "iter 1788: loss 0.7220, time 93.95ms, mfu 0.01%\n",
      "iter 1789: loss 0.9228, time 95.10ms, mfu 0.01%\n",
      "iter 1790: loss 0.8166, time 95.24ms, mfu 0.01%\n",
      "iter 1791: loss 0.7871, time 97.59ms, mfu 0.01%\n",
      "iter 1792: loss 0.7176, time 110.61ms, mfu 0.01%\n",
      "iter 1793: loss 0.9339, time 119.45ms, mfu 0.01%\n",
      "iter 1794: loss 1.0129, time 172.17ms, mfu 0.01%\n",
      "iter 1795: loss 0.4751, time 102.28ms, mfu 0.01%\n",
      "iter 1796: loss 0.7437, time 97.49ms, mfu 0.01%\n",
      "iter 1797: loss 0.7624, time 97.61ms, mfu 0.01%\n",
      "iter 1798: loss 0.8543, time 98.22ms, mfu 0.01%\n",
      "iter 1799: loss 0.5682, time 97.35ms, mfu 0.01%\n",
      "iter 1800: loss 0.6893, time 103.50ms, mfu 0.01%\n",
      "iter 1801: loss 1.1027, time 101.98ms, mfu 0.01%\n",
      "iter 1802: loss 0.8956, time 98.38ms, mfu 0.01%\n",
      "iter 1803: loss 0.9617, time 99.03ms, mfu 0.01%\n",
      "iter 1804: loss 0.9273, time 98.84ms, mfu 0.01%\n",
      "iter 1805: loss 1.0024, time 95.96ms, mfu 0.01%\n",
      "iter 1806: loss 0.4681, time 99.44ms, mfu 0.01%\n",
      "iter 1807: loss 0.6954, time 97.66ms, mfu 0.01%\n",
      "iter 1808: loss 0.8000, time 97.20ms, mfu 0.01%\n",
      "iter 1809: loss 1.0528, time 95.51ms, mfu 0.01%\n",
      "iter 1810: loss 0.4520, time 94.27ms, mfu 0.01%\n",
      "iter 1811: loss 0.7955, time 99.09ms, mfu 0.01%\n",
      "iter 1812: loss 0.6016, time 96.85ms, mfu 0.01%\n",
      "iter 1813: loss 0.8626, time 98.07ms, mfu 0.01%\n",
      "iter 1814: loss 0.9047, time 111.33ms, mfu 0.01%\n",
      "iter 1815: loss 0.8189, time 98.37ms, mfu 0.01%\n",
      "iter 1816: loss 0.9664, time 97.14ms, mfu 0.01%\n",
      "iter 1817: loss 0.5789, time 99.15ms, mfu 0.01%\n",
      "iter 1818: loss 1.0591, time 97.56ms, mfu 0.01%\n",
      "iter 1819: loss 0.7932, time 98.54ms, mfu 0.01%\n",
      "iter 1820: loss 0.7118, time 97.78ms, mfu 0.01%\n",
      "iter 1821: loss 0.9237, time 98.27ms, mfu 0.01%\n",
      "iter 1822: loss 0.8176, time 97.84ms, mfu 0.01%\n",
      "iter 1823: loss 0.6848, time 98.44ms, mfu 0.01%\n",
      "iter 1824: loss 0.6118, time 98.60ms, mfu 0.01%\n",
      "iter 1825: loss 0.8061, time 98.08ms, mfu 0.01%\n",
      "iter 1826: loss 0.8880, time 97.90ms, mfu 0.01%\n",
      "iter 1827: loss 0.9447, time 97.58ms, mfu 0.01%\n",
      "iter 1828: loss 1.1341, time 97.91ms, mfu 0.01%\n",
      "iter 1829: loss 0.5980, time 96.87ms, mfu 0.01%\n",
      "iter 1830: loss 0.7581, time 97.57ms, mfu 0.01%\n",
      "iter 1831: loss 0.6761, time 160.04ms, mfu 0.01%\n",
      "iter 1832: loss 0.7710, time 97.42ms, mfu 0.01%\n",
      "iter 1833: loss 0.8293, time 112.35ms, mfu 0.01%\n",
      "iter 1834: loss 0.7457, time 97.26ms, mfu 0.01%\n",
      "iter 1835: loss 0.8505, time 98.28ms, mfu 0.01%\n",
      "iter 1836: loss 0.9417, time 96.93ms, mfu 0.01%\n",
      "iter 1837: loss 0.7846, time 97.54ms, mfu 0.01%\n",
      "iter 1838: loss 0.8516, time 95.48ms, mfu 0.01%\n",
      "iter 1839: loss 0.8328, time 95.40ms, mfu 0.01%\n",
      "iter 1840: loss 0.6774, time 97.75ms, mfu 0.01%\n",
      "iter 1841: loss 0.6243, time 97.55ms, mfu 0.01%\n",
      "iter 1842: loss 0.7553, time 99.22ms, mfu 0.01%\n",
      "iter 1843: loss 0.5147, time 98.13ms, mfu 0.01%\n",
      "iter 1844: loss 0.8366, time 97.58ms, mfu 0.01%\n",
      "iter 1845: loss 0.7524, time 97.37ms, mfu 0.01%\n",
      "iter 1846: loss 0.8086, time 96.88ms, mfu 0.01%\n",
      "iter 1847: loss 0.6835, time 98.00ms, mfu 0.01%\n",
      "iter 1848: loss 0.5271, time 96.74ms, mfu 0.01%\n",
      "iter 1849: loss 0.8103, time 97.10ms, mfu 0.01%\n",
      "iter 1850: loss 0.6480, time 97.59ms, mfu 0.01%\n",
      "iter 1851: loss 0.9917, time 101.81ms, mfu 0.01%\n",
      "iter 1852: loss 0.8412, time 99.12ms, mfu 0.01%\n",
      "iter 1853: loss 0.7758, time 99.05ms, mfu 0.01%\n",
      "iter 1854: loss 1.0783, time 97.97ms, mfu 0.01%\n",
      "iter 1855: loss 0.9190, time 132.63ms, mfu 0.01%\n",
      "iter 1856: loss 0.7257, time 98.44ms, mfu 0.01%\n",
      "iter 1857: loss 0.7903, time 98.28ms, mfu 0.01%\n",
      "iter 1858: loss 0.7956, time 96.09ms, mfu 0.01%\n",
      "iter 1859: loss 0.7344, time 97.25ms, mfu 0.01%\n",
      "iter 1860: loss 0.7347, time 97.28ms, mfu 0.01%\n",
      "iter 1861: loss 0.9630, time 96.57ms, mfu 0.01%\n",
      "iter 1862: loss 0.4265, time 96.61ms, mfu 0.01%\n",
      "iter 1863: loss 0.8074, time 97.28ms, mfu 0.01%\n",
      "iter 1864: loss 0.9539, time 146.00ms, mfu 0.01%\n",
      "iter 1865: loss 0.9218, time 97.26ms, mfu 0.01%\n",
      "iter 1866: loss 0.8921, time 97.10ms, mfu 0.01%\n",
      "iter 1867: loss 0.7826, time 98.97ms, mfu 0.01%\n",
      "iter 1868: loss 0.6551, time 97.12ms, mfu 0.01%\n",
      "iter 1869: loss 0.9028, time 96.03ms, mfu 0.01%\n",
      "iter 1870: loss 0.4134, time 97.76ms, mfu 0.01%\n",
      "iter 1871: loss 0.7941, time 98.74ms, mfu 0.01%\n",
      "iter 1872: loss 0.6921, time 97.24ms, mfu 0.01%\n",
      "iter 1873: loss 0.6875, time 97.05ms, mfu 0.01%\n",
      "iter 1874: loss 0.5270, time 113.86ms, mfu 0.01%\n",
      "iter 1875: loss 0.6479, time 99.09ms, mfu 0.01%\n",
      "iter 1876: loss 0.7405, time 97.16ms, mfu 0.01%\n",
      "iter 1877: loss 0.9286, time 98.07ms, mfu 0.01%\n",
      "iter 1878: loss 0.7610, time 98.97ms, mfu 0.01%\n",
      "iter 1879: loss 0.8767, time 101.45ms, mfu 0.01%\n",
      "iter 1880: loss 0.5780, time 103.34ms, mfu 0.01%\n",
      "iter 1881: loss 0.4504, time 95.02ms, mfu 0.01%\n",
      "iter 1882: loss 0.7667, time 96.35ms, mfu 0.01%\n",
      "iter 1883: loss 0.6437, time 83.61ms, mfu 0.01%\n",
      "iter 1884: loss 0.7129, time 79.81ms, mfu 0.01%\n",
      "iter 1885: loss 0.6559, time 80.39ms, mfu 0.01%\n",
      "iter 1886: loss 0.5639, time 80.68ms, mfu 0.01%\n",
      "iter 1887: loss 1.0255, time 107.44ms, mfu 0.01%\n",
      "iter 1888: loss 0.9579, time 93.09ms, mfu 0.01%\n",
      "iter 1889: loss 0.9910, time 96.39ms, mfu 0.01%\n",
      "iter 1890: loss 0.4679, time 95.99ms, mfu 0.01%\n",
      "iter 1891: loss 0.7991, time 96.22ms, mfu 0.01%\n",
      "iter 1892: loss 0.9730, time 96.68ms, mfu 0.01%\n",
      "iter 1893: loss 0.6383, time 96.38ms, mfu 0.01%\n",
      "iter 1894: loss 0.7543, time 95.89ms, mfu 0.01%\n",
      "iter 1895: loss 0.4869, time 95.76ms, mfu 0.01%\n",
      "iter 1896: loss 1.1956, time 151.83ms, mfu 0.01%\n",
      "iter 1897: loss 0.9493, time 106.30ms, mfu 0.01%\n",
      "iter 1898: loss 0.6595, time 93.15ms, mfu 0.01%\n",
      "iter 1899: loss 0.8924, time 95.68ms, mfu 0.01%\n",
      "iter 1900: loss 0.5987, time 95.27ms, mfu 0.01%\n",
      "iter 1901: loss 0.4563, time 97.34ms, mfu 0.01%\n",
      "iter 1902: loss 0.7945, time 95.85ms, mfu 0.01%\n",
      "iter 1903: loss 0.7153, time 95.95ms, mfu 0.01%\n",
      "iter 1904: loss 1.0190, time 96.99ms, mfu 0.01%\n",
      "iter 1905: loss 0.5820, time 94.70ms, mfu 0.01%\n",
      "iter 1906: loss 0.8718, time 95.43ms, mfu 0.01%\n",
      "iter 1907: loss 0.7369, time 95.72ms, mfu 0.01%\n",
      "iter 1908: loss 0.5784, time 95.15ms, mfu 0.01%\n",
      "iter 1909: loss 0.7975, time 95.51ms, mfu 0.01%\n",
      "iter 1910: loss 1.0023, time 95.65ms, mfu 0.01%\n",
      "iter 1911: loss 0.4327, time 97.06ms, mfu 0.01%\n",
      "iter 1912: loss 1.0285, time 95.61ms, mfu 0.01%\n",
      "iter 1913: loss 0.4370, time 97.39ms, mfu 0.01%\n",
      "iter 1914: loss 0.9354, time 96.54ms, mfu 0.01%\n",
      "iter 1915: loss 0.6587, time 111.22ms, mfu 0.01%\n",
      "iter 1916: loss 0.7053, time 95.13ms, mfu 0.01%\n",
      "iter 1917: loss 0.5811, time 95.52ms, mfu 0.01%\n",
      "iter 1918: loss 0.8376, time 95.96ms, mfu 0.01%\n",
      "iter 1919: loss 0.7010, time 96.38ms, mfu 0.01%\n",
      "iter 1920: loss 0.8199, time 96.71ms, mfu 0.01%\n",
      "iter 1921: loss 0.8581, time 95.43ms, mfu 0.01%\n",
      "iter 1922: loss 0.7856, time 94.41ms, mfu 0.01%\n",
      "iter 1923: loss 0.5820, time 96.85ms, mfu 0.01%\n",
      "iter 1924: loss 0.9289, time 96.00ms, mfu 0.01%\n",
      "iter 1925: loss 0.5917, time 95.70ms, mfu 0.01%\n",
      "iter 1926: loss 0.7368, time 95.93ms, mfu 0.01%\n",
      "iter 1927: loss 0.7068, time 95.75ms, mfu 0.01%\n",
      "iter 1928: loss 1.0351, time 133.87ms, mfu 0.01%\n",
      "iter 1929: loss 0.3820, time 99.96ms, mfu 0.01%\n",
      "iter 1930: loss 0.5526, time 95.75ms, mfu 0.01%\n",
      "iter 1931: loss 0.6694, time 96.18ms, mfu 0.01%\n",
      "iter 1932: loss 0.5728, time 99.81ms, mfu 0.01%\n",
      "iter 1933: loss 0.7423, time 109.73ms, mfu 0.01%\n",
      "iter 1934: loss 0.9693, time 97.69ms, mfu 0.01%\n",
      "iter 1935: loss 0.8550, time 96.47ms, mfu 0.01%\n",
      "iter 1936: loss 0.7010, time 96.50ms, mfu 0.01%\n",
      "iter 1937: loss 0.8355, time 96.24ms, mfu 0.01%\n",
      "iter 1938: loss 0.5993, time 94.44ms, mfu 0.01%\n",
      "iter 1939: loss 0.9224, time 94.92ms, mfu 0.01%\n",
      "iter 1940: loss 0.9318, time 93.88ms, mfu 0.01%\n",
      "iter 1941: loss 0.4842, time 94.42ms, mfu 0.01%\n",
      "iter 1942: loss 0.6835, time 95.75ms, mfu 0.01%\n",
      "iter 1943: loss 0.9851, time 98.21ms, mfu 0.01%\n",
      "iter 1944: loss 0.9424, time 99.13ms, mfu 0.01%\n",
      "iter 1945: loss 0.5295, time 99.82ms, mfu 0.01%\n",
      "iter 1946: loss 0.8938, time 98.40ms, mfu 0.01%\n",
      "iter 1947: loss 1.0353, time 99.07ms, mfu 0.01%\n",
      "iter 1948: loss 0.6418, time 90.92ms, mfu 0.01%\n",
      "iter 1949: loss 0.7282, time 97.48ms, mfu 0.01%\n",
      "iter 1950: loss 0.9631, time 97.00ms, mfu 0.01%\n",
      "iter 1951: loss 0.6817, time 114.06ms, mfu 0.01%\n",
      "iter 1952: loss 0.7475, time 96.44ms, mfu 0.01%\n",
      "iter 1953: loss 0.6669, time 97.82ms, mfu 0.01%\n",
      "iter 1954: loss 0.8456, time 102.06ms, mfu 0.01%\n",
      "iter 1955: loss 0.6131, time 99.02ms, mfu 0.01%\n",
      "iter 1956: loss 0.5597, time 97.87ms, mfu 0.01%\n",
      "iter 1957: loss 0.7815, time 97.66ms, mfu 0.01%\n",
      "iter 1958: loss 0.7681, time 95.38ms, mfu 0.01%\n",
      "iter 1959: loss 0.5788, time 97.66ms, mfu 0.01%\n",
      "iter 1960: loss 1.1037, time 154.19ms, mfu 0.01%\n",
      "iter 1961: loss 0.5538, time 98.37ms, mfu 0.01%\n",
      "iter 1962: loss 0.6469, time 95.88ms, mfu 0.01%\n",
      "iter 1963: loss 0.9851, time 96.56ms, mfu 0.01%\n",
      "iter 1964: loss 0.5761, time 96.59ms, mfu 0.01%\n",
      "iter 1965: loss 0.7594, time 96.92ms, mfu 0.01%\n",
      "iter 1966: loss 0.9436, time 97.51ms, mfu 0.01%\n",
      "iter 1967: loss 0.8020, time 97.09ms, mfu 0.01%\n",
      "iter 1968: loss 0.5639, time 101.77ms, mfu 0.01%\n",
      "iter 1969: loss 0.5803, time 98.54ms, mfu 0.01%\n",
      "iter 1970: loss 0.7736, time 108.33ms, mfu 0.01%\n",
      "iter 1971: loss 0.8169, time 95.72ms, mfu 0.01%\n",
      "iter 1972: loss 0.5372, time 96.26ms, mfu 0.01%\n",
      "iter 1973: loss 0.6871, time 96.74ms, mfu 0.01%\n",
      "iter 1974: loss 0.7924, time 97.20ms, mfu 0.01%\n",
      "iter 1975: loss 0.5727, time 96.77ms, mfu 0.01%\n",
      "iter 1976: loss 0.5487, time 95.80ms, mfu 0.01%\n",
      "iter 1977: loss 0.8522, time 95.34ms, mfu 0.01%\n",
      "iter 1978: loss 0.6217, time 95.99ms, mfu 0.01%\n",
      "iter 1979: loss 0.2901, time 98.23ms, mfu 0.01%\n",
      "iter 1980: loss 0.8595, time 92.99ms, mfu 0.01%\n",
      "iter 1981: loss 0.6385, time 94.25ms, mfu 0.01%\n",
      "iter 1982: loss 0.7578, time 95.52ms, mfu 0.01%\n",
      "iter 1983: loss 0.5417, time 94.89ms, mfu 0.01%\n",
      "iter 1984: loss 0.5204, time 95.90ms, mfu 0.01%\n",
      "iter 1985: loss 0.4201, time 95.49ms, mfu 0.01%\n",
      "iter 1986: loss 0.6964, time 96.46ms, mfu 0.01%\n",
      "iter 1987: loss 0.6654, time 95.75ms, mfu 0.01%\n",
      "iter 1988: loss 0.6841, time 108.62ms, mfu 0.01%\n",
      "iter 1989: loss 0.7293, time 97.14ms, mfu 0.01%\n",
      "iter 1990: loss 1.0780, time 142.98ms, mfu 0.01%\n",
      "iter 1991: loss 0.6923, time 96.02ms, mfu 0.01%\n",
      "iter 1992: loss 0.7157, time 94.81ms, mfu 0.01%\n",
      "iter 1993: loss 0.8576, time 96.19ms, mfu 0.01%\n",
      "iter 1994: loss 0.8472, time 95.09ms, mfu 0.01%\n",
      "iter 1995: loss 0.6176, time 96.50ms, mfu 0.01%\n",
      "iter 1996: loss 0.7257, time 96.59ms, mfu 0.01%\n",
      "iter 1997: loss 0.7227, time 96.14ms, mfu 0.01%\n",
      "iter 1998: loss 0.5830, time 94.70ms, mfu 0.01%\n",
      "iter 1999: loss 0.8089, time 95.73ms, mfu 0.01%\n",
      "step 2000: train loss 0.7081, val loss 1.7199\n",
      "iter 2000: loss 0.6679, time 1012.37ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "# This step has already been done: retrain only if needed, takes 4minutes.\n",
    "#!python train.py config/train_slang.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do with that model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some strings from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-slang\n",
      "Overriding: device = cpu\n",
      "number of parameters: 0.81M\n",
      "Loading meta from data/slang_data/meta.pkl...\n",
      "\n",
      "Zėėė666ÿL66ČLxZZÑÑÑăÿăÿÿÿ6ÿÿđ66ă6Č6ZZZÑÿÐÑZZă666ăÿėăă6ăÑăÿÿăăăÿ6Ñ66666ăÿÿÿÿ6ĕăÿÿÿÿZZă666ÿÿZăÿăă6ÿ6666666ė6ÿ666666Ză6ÿę66ă66ėČ6ČÑ6Vÿ66666666æeeæææææDDDDDææææææææeeeæDDDDææDDææææææææææDDDDbbbbbbbbææææeeeeeeeææææææeeæææææææææææeeeeeeeeeeeeeeeeeeeeeeæDbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "\n",
      "ÄøbbbbbbbbbbbbbbbbbbbÉbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbÄøĖbbbbbbbbbbbbÉËbbbbbÉbbbbbbbÉbbbbbbbbbbbbbbbbbbbbËËbbbbbbbbbbbbbËËbbbbbbbbbbbbbbbbbbbbbbËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "\n",
      "ïïėÄÉbbbbËbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "bbbbbbbbbbbbbbbbbbÄÄÄËĕāāÄËËËbbbbÕÕÕÕÕÕÕÕbbÄÄbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbâbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "\n",
      "ÿėėėxxėăxāăxėăă6ėėėėėăăÑZėxÿ6ėė666ă6ÑÑÑėïDĘÑÑÑÑÑZėėėăÑÑÑČÑăėāÑÑėėėČė66ææ6xăăÿ66ÑÿxėČ66666ČLăÑ6ėÿăÑZÿăÿÿ66ÿxČďÿ666ÿ6666ČÑ6ăxČ66ÿăÿÿăăă6æ6ăė6ÿæææ 66666ÿÑėÿė666666ČDææeææææDDDææææææææææææææææææææææææææææææææDDËDDDbbbbbbbbbDDDææeææeeeæææææææææææææeææææææeeeeeeeeeeeeeeeeeeeeeeeeeeæææeeeeææææææææææææeeeææææææeeeeeæææææeæeeeeeeæbbbbææææææeeeeeeeæææeeæDDææDDDDDææææeeeæææbbbbbbDæeeeeeæeeæDDDDDDDDDDDDDDËbbbbbbDDDDDDDææææææææææeeeeeeeeeeeeææææææææææeeeeeeeeeeeeeeeeeeeeeeeeeeeeæææææeeeææeeeeeeeeeæDDDæææDDDD\n",
      "---------------\n",
      "\n",
      "ĢîwwwwwwwwwĢĢwwwĢwwwwwøĢwwwĢwwwwwwwĢôôôøøĢwĢwwwĢwwÎwĢwwñóĖøwĢĢwwwwÉÉÉÉĖwĢwmwĢwwwwĢĢwwwĢwĢôĢwwwwwwøĖÍÍEËËËËbËËÎ×ÉbbbbbbSwwwøøøøÄĎðøønĖĔĖąĖđđHěĔĖGđøøøđąÄøðbKøđËËbbbbbbbbbKËbbbËbbbbbïbbbbbbbbbbbbbbbbėÄđøăāðđđđøSðËËbbbbbðËËðbbbbËËbbbbbbbbbbbÉÐĕđËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "\n",
      "iiiiiiùiiiiùùùùùùùùùùùiiiiiiiùùùùiiiiiiiùùùùùùùùùiiùùùùùiiùiiiiiiiùùùùùùùùùiiiiiiiiiiiiiiùùùùùùùùùùùùùùùiiiiiiiiiiiiiùùùùùùùùùùùùùùiiiiiùùùùùùùùiùùùùùùùùùùùùiiiiùùùùùùùùùùùùùùùùùùùùùùùùùùùùùiùùùùùùùùùùùùùùùùùùùùiùùùùùùùùùùùùùùùùùùùùùùùùùùùùùiiiiiiiiùùùùùùiiiiiiiiiiiùùùiiùiiiiiiiiiiiiiiiiiiiiiiiiiùùùùùùùùùùùùùiiiiiiiiiiiùùùùùùùùùùùùùùùùùùùùùiiiiùùùùùùùiiiiiiiiiiiiiiiùùùùùùùùùùùùùùùùùùùùùùùiiiiùùùùùiiùùùùùùùùiiùùùùùiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiùùiiiiiiùùùùùiiiiiiiiiiiùiiùiiiiiiiiiiiiiiiiiiiùùùù\n",
      "---------------\n",
      "\n",
      "ĘėėėėÑăăăÿÿăăāăÿÿVăăăZÿăÿ66ï6666ČėėėÿÑăZZZă6666ÿ6÷ÿăÑZÑÑxėxăÑăāČÿ6ÑėxZÿÿÿă66ÿ6ė66ÿxÿă4kėÿ666÷J666ÑÑ666ÿÿăÿÿăÿÿ66ė66Čė6ZÿÿÿÿÿÿÇÿėZZăÿăx6666666666ïï6ÿ6ÿ666xėÇxÿZă66666ï66ÿÿÎ6666ąÿ66666ÿäăÎăï6666x6ČZÑăėxČČėĘ6æïČææÇZ66æææææææææææææė6æææeeææeeææææææææææææææeeææææææææææeDDææææææææææeeeeeeeeeeeeææeeeææææææDbbDæDDDDDDDææææææææææææææææææææææææeeeeeeeeeeeeeeeeeeææææææææeeeæDæeeææDæDDbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "\n",
      "đđððđïLxLęÇïïNĘEęNĘïąLęEĘHÇÄÄÄLÄĕLĘĘïÄEÄÄÇLęÄÄÇÇÇÇLLęLðÄĘÇĘÇÇÇÇĀđÇÎÇĀęęÇÇÇÇÄïïÄęÄęęÇÇÇđÇÇÇęÇÇïÇÇÄïÇÇÇæÇïÇïÇąLïEÇÇÇÇÇÎęÄÄÇÇÇLÇĘÄāĘïÄLïÄLÇÇÇÇÇÄĘÇĘÇÇÄÇÄāÄÇęxÇÇÇÇLęÄÄÇÇÄLxÄÇLĘÄĘÄÇÇÇïÄÇÇÇÇÄÇÄïNĘĘLÄÇÇÇÎĘÄĘÇÄÇÄĘĘąÄÇÇïHĘĘLÄÄĘąÄÇÄĘÄÄĘbĘÄÄÄĘÄÄĘÄÇęðEÇÇÇÇÇÇÇEÄÄÇĘbÄÇâÄÇLėÄĘÄÄęÇĀęąÇÇęĘėÇÄbËLEÇÇĘÇËäÇÇęęÇąęęÇęÄÎęÇęEÇÄĘÇÄÇÇÇĘLęÄÄLÄLEÄęÄÄĘïÇÄÄÇEÇÇÄÇÇEEÄÇÇEÇÇÇLNSÄÇÇEÇÇÇĘÇÇïÇÄïÇÇLÇïĘĘĘEĘÇÇĀÇÇEÇęÇÄÄÄÄ÷ÎEÇĀÇĘÇęÎLĘLÇąEEÇLEęĘĘbĘĘÇĘęÇïLÇÇĘLSąĘâĀĀĀEÇÇÇĘÇÇLLĕïEĀęĘÎLęLĀĘĘÇĘÇÇÄÇEęLÇÇEÇEĘEĘĘïÇLEÇÇęęęEĕÇĘÇęÎĘEÇLđÇÇEÄÄÇEÇÇĀÇęę\n",
      "---------------\n",
      "\n",
      "ėđÎÄėėėėÑZÑÑ6Ðăÿÿÿÿ6ăăă6666666Č666ÐOăÿÿă6ÎÿÿÿăČăă66ăÿ6x6eė6ęÿx66ÿÿÿÿăČxăÿ66ÿė66ăāČČ666ėă6ă6ă66x6ÿăăÿ6ăÑăăăÿ6ė666ÎxÿėÿĘă6ÿÿ66ÿÿ6ėČ66666ėėėxăZÿăÑÿÿÐÿăăăÿăZxx666xėČ66ææ6ČČÿėZăėėėėė666666xėėx666ę666ÿėėzÿZL66ęėėė666xZÑeæDDDææe6ZZČ6DææææææDææeeeeeeeæeeeeeeæææææDDæææææDDææeeææææDDDæïbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbâĖbbbbbbbbbbbĖóbbbbbbËbbb\n",
      "---------------\n",
      "\n",
      "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbâðbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-slang --device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the model and get info about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.81M\n",
      "Loading meta from data/slang_data/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "out_dir='out-slang'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many params in the model\n",
    "model.get_num_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood of a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 69, 121,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40,  74,  40,  40,  40,  40,  40,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode a string\n",
    "start_ids =encode('ÄøbbbbbbbbbbbbbbbbbbbÉbbbbbbbbbbbbb')\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(start_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 163])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits\n",
    "logits, _ = model(x)\n",
    "logits.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 69, 121,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40,  74,  40,  40,  40,  40,  40,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = 'ÄøbbbbbbbbbbbbbbbbbbbÉbbbbbbbbbbbbb'\n",
    "\n",
    "start_ids = encode(input_string)\n",
    "#token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "token_ids = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.483938694000244\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_log_likelihood(model, input_string):\n",
    "    # Tokenize the input string\n",
    "    start_ids = encode(input_string)\n",
    "    #token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "    token_ids = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "    # Get logits from the model\n",
    "    logits, _ = model(token_ids)\n",
    "    token_ids = token_ids.squeeze().tolist()\n",
    "\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs = log_probs.squeeze().tolist()\n",
    "\n",
    "    # Compute log likelihood of the input string\n",
    "    log_likelihood = 0.0\n",
    "    for i in range(1, len(token_ids)):\n",
    "        log_likelihood += log_probs[token_ids[i]]\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "# Usage\n",
    "#input_string = 'ÄøbbbbbbbbbbbbbbbbbbbÉbbbbbbbbbbbbb'\n",
    "input_string = 'Äø'\n",
    "\n",
    "log_likelihood = compute_log_likelihood(model, input_string)\n",
    "print(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1-------log_likelihood=-6.543567657470703\n",
      "i=2-------log_likelihood=-6.702705383300781\n",
      "i=3-------log_likelihood=-6.861843109130859\n",
      "i=4-------log_likelihood=-7.0209808349609375\n",
      "i=5-------log_likelihood=-7.180118560791016\n",
      "i=6-------log_likelihood=-7.339256286621094\n",
      "i=7-------log_likelihood=-7.498394012451172\n",
      "i=8-------log_likelihood=-7.65753173828125\n",
      "i=9-------log_likelihood=-7.816669464111328\n",
      "i=10-------log_likelihood=-7.975807189941406\n",
      "i=11-------log_likelihood=-8.134944915771484\n",
      "i=12-------log_likelihood=-8.294082641601562\n",
      "i=13-------log_likelihood=-8.45322036743164\n",
      "i=14-------log_likelihood=-8.612358093261719\n",
      "i=15-------log_likelihood=-8.771495819091797\n",
      "i=16-------log_likelihood=-8.930633544921875\n",
      "i=17-------log_likelihood=-9.089771270751953\n",
      "i=18-------log_likelihood=-9.248908996582031\n",
      "i=19-------log_likelihood=-9.40804672241211\n",
      "i=20-------log_likelihood=-9.567184448242188\n",
      "i=21-------log_likelihood=-14.31873893737793\n",
      "i=22-------log_likelihood=-14.477876663208008\n",
      "i=23-------log_likelihood=-14.637014389038086\n",
      "i=24-------log_likelihood=-14.796152114868164\n",
      "i=25-------log_likelihood=-14.955289840698242\n",
      "i=26-------log_likelihood=-15.11442756652832\n",
      "i=27-------log_likelihood=-15.273565292358398\n",
      "i=28-------log_likelihood=-15.432703018188477\n",
      "i=29-------log_likelihood=-15.591840744018555\n",
      "i=30-------log_likelihood=-15.750978469848633\n",
      "i=31-------log_likelihood=-15.910116195678711\n",
      "i=32-------log_likelihood=-16.06925392150879\n",
      "i=33-------log_likelihood=-16.228391647338867\n",
      "i=34-------log_likelihood=-16.387529373168945\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "# Tokenize the input string\n",
    "input_string = 'Äøbbbbbbbbbb'\n",
    "\n",
    "token_ids = encode(input_string)\n",
    "#token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "token_ids = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "# Get logits from the model\n",
    "logits, _ = model(token_ids)\n",
    "token_ids = token_ids.squeeze().tolist()\n",
    "\n",
    "# Convert logits to log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "log_probs = log_probs.squeeze().tolist()\n",
    "\n",
    "# Compute log likelihood of the input string\n",
    "log_likelihood = 0.0\n",
    "for i in range(1, len(token_ids)):\n",
    "    log_likelihood += log_probs[token_ids[i]]\n",
    "    print(f'{i=}-------{log_likelihood=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.49934697151184"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = 'Äøbbbbbbbbbb'\n",
    "log_likelihood = compute_log_likelihood(model, input_string)\n",
    "log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([69,\n",
       "  121,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  74,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40,\n",
       "  40],\n",
       " [-5.950631141662598,\n",
       "  -7.802065372467041,\n",
       "  -13.482702255249023,\n",
       "  -8.664899826049805,\n",
       "  -8.303033828735352,\n",
       "  -8.459487915039062,\n",
       "  -8.544755935668945,\n",
       "  -8.036310195922852,\n",
       "  -8.63289737701416,\n",
       "  -7.842120170593262,\n",
       "  -13.253167152404785,\n",
       "  -8.583927154541016,\n",
       "  -9.431646347045898,\n",
       "  -9.803473472595215,\n",
       "  -9.308578491210938,\n",
       "  -6.665337085723877,\n",
       "  -8.988578796386719,\n",
       "  -8.000788688659668,\n",
       "  -10.165093421936035,\n",
       "  -8.545632362365723,\n",
       "  -9.102828979492188,\n",
       "  -7.873270034790039,\n",
       "  -10.757134437561035,\n",
       "  -5.889583110809326,\n",
       "  -7.671297073364258,\n",
       "  -11.372381210327148,\n",
       "  -6.976910591125488,\n",
       "  -7.4980788230896,\n",
       "  -7.716301918029785,\n",
       "  -8.631196022033691,\n",
       "  -11.004348754882812,\n",
       "  -5.41005802154541,\n",
       "  -10.959341049194336,\n",
       "  -10.98293399810791,\n",
       "  -7.680006504058838,\n",
       "  -8.742183685302734,\n",
       "  -11.81546688079834,\n",
       "  -8.332889556884766,\n",
       "  -8.378704071044922,\n",
       "  -8.227534294128418,\n",
       "  -0.15913772583007812,\n",
       "  -8.025323867797852,\n",
       "  -8.808104515075684,\n",
       "  -9.910619735717773,\n",
       "  -10.528595924377441,\n",
       "  -10.060989379882812,\n",
       "  -11.483312606811523,\n",
       "  -8.316913604736328,\n",
       "  -10.217599868774414,\n",
       "  -7.987351417541504,\n",
       "  -8.242897033691406,\n",
       "  -9.658035278320312,\n",
       "  -9.755733489990234,\n",
       "  -9.022960662841797,\n",
       "  -6.266380786895752,\n",
       "  -10.658675193786621,\n",
       "  -7.667608261108398,\n",
       "  -9.16586685180664,\n",
       "  -7.564515590667725,\n",
       "  -8.931997299194336,\n",
       "  -11.418488502502441,\n",
       "  -6.427118301391602,\n",
       "  -7.902199745178223,\n",
       "  -9.170424461364746,\n",
       "  -7.033017158508301,\n",
       "  -11.820178985595703,\n",
       "  -8.379581451416016,\n",
       "  -9.338068008422852,\n",
       "  -9.419045448303223,\n",
       "  -5.3011579513549805,\n",
       "  -7.680788040161133,\n",
       "  -6.180191993713379,\n",
       "  -7.5618414878845215,\n",
       "  -8.13213062286377,\n",
       "  -4.751554489135742,\n",
       "  -8.532227516174316,\n",
       "  -3.4638442993164062,\n",
       "  -10.518830299377441,\n",
       "  -8.767194747924805,\n",
       "  -7.440390110015869,\n",
       "  -10.94239616394043,\n",
       "  -7.361095905303955,\n",
       "  -7.775007247924805,\n",
       "  -6.6193671226501465,\n",
       "  -7.656801223754883,\n",
       "  -6.481744766235352,\n",
       "  -7.6984148025512695,\n",
       "  -10.487643241882324,\n",
       "  -6.422869682312012,\n",
       "  -8.589497566223145,\n",
       "  -8.803001403808594,\n",
       "  -5.7674126625061035,\n",
       "  -11.756107330322266,\n",
       "  -9.224717140197754,\n",
       "  -6.646552085876465,\n",
       "  -8.821013450622559,\n",
       "  -7.993476867675781,\n",
       "  -9.321496963500977,\n",
       "  -9.062085151672363,\n",
       "  -5.764608383178711,\n",
       "  -7.909576416015625,\n",
       "  -7.212730884552002,\n",
       "  -9.369081497192383,\n",
       "  -10.498621940612793,\n",
       "  -6.365996360778809,\n",
       "  -8.668899536132812,\n",
       "  -10.80052375793457,\n",
       "  -7.366962432861328,\n",
       "  -10.872920989990234,\n",
       "  -9.644855499267578,\n",
       "  -8.521404266357422,\n",
       "  -9.799703598022461,\n",
       "  -5.558866500854492,\n",
       "  -5.132552146911621,\n",
       "  -8.177952766418457,\n",
       "  -10.477681159973145,\n",
       "  -7.751117706298828,\n",
       "  -9.92990493774414,\n",
       "  -8.425687789916992,\n",
       "  -8.837491989135742,\n",
       "  -7.898916244506836,\n",
       "  -6.543567657470703,\n",
       "  -9.20556354522705,\n",
       "  -5.296974182128906,\n",
       "  -7.145254135131836,\n",
       "  -8.526975631713867,\n",
       "  -10.62103271484375,\n",
       "  -9.84132194519043,\n",
       "  -8.553725242614746,\n",
       "  -8.044427871704102,\n",
       "  -5.538974761962891,\n",
       "  -10.11452865600586,\n",
       "  -6.7956013679504395,\n",
       "  -6.302101135253906,\n",
       "  -7.75233268737793,\n",
       "  -7.981353759765625,\n",
       "  -10.290728569030762,\n",
       "  -8.927913665771484,\n",
       "  -10.584684371948242,\n",
       "  -10.064664840698242,\n",
       "  -11.330522537231445,\n",
       "  -7.981705665588379,\n",
       "  -8.67103385925293,\n",
       "  -5.693831443786621,\n",
       "  -7.559401035308838,\n",
       "  -5.937882900238037,\n",
       "  -13.435747146606445,\n",
       "  -6.618038654327393,\n",
       "  -8.750913619995117,\n",
       "  -7.694700241088867,\n",
       "  -5.092756271362305,\n",
       "  -5.482172966003418,\n",
       "  -6.30391788482666,\n",
       "  -7.282887935638428,\n",
       "  -12.149431228637695,\n",
       "  -7.126071929931641,\n",
       "  -10.010489463806152,\n",
       "  -7.281721115112305,\n",
       "  -13.187739372253418,\n",
       "  -11.936113357543945,\n",
       "  -8.24740219116211,\n",
       "  -9.22597885131836,\n",
       "  -7.936367034912109])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids, log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage in outlier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a long string, make a moving window and analyze the variations of log-likelihood along the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plausible string followed by a \"wrong one\" \n",
    "plausible_string = 'ÜèÜèèèbêêèÜèfèööêbÜêbêêêêêêèöêêêbbèöêbbêêêêöêbbbbbbbbbbbbbbbbbbbbbbbbbbbêêêêöCèÜÜÜööÜÜööêööêđèffÜfÜÜfÜÜÜöööööööêÜ3fÜööÜÜÜöÜööêöêêêêêêêêêêööööööêêêêêêêêêêêêêêêêêêËbËêbêèöèööööêêöêêêêêêêêêêêöêêêêêêêêêêêöêêêêêêêêêêêbêêbêbËËËËöÜbËbêĘbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËËËËËbbbSïbbbbËāËbbbÄËbbbbbbbbbbNbbËbbzËbbËbĘËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbÔËbÆĘËbbāêËbËbbbbbbbbbËËbbbËbbbbËbbbbÄbËËbËbbbËbbbbbbbbāêðbïNËËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbÄbêėÄÄðĘĘbbËËbËðĘÄêËËËbbbbbbbËbbbbðbbbbÆbbbËÄËĘËbbbbbbbbbbbātËËËbbËbbbbbĘbbbbbË'\n",
    "wrong_string = 'bbêêêê'*40\n",
    "input_string = plausible_string+wrong_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# size of context window: right now it is 64: take 3 windows\n",
    "# outlierness : if supervised\n",
    "# deliverable: \n",
    "* generative model: create plausible text (compare train with generated)\n",
    "* verify the outlier example: random generated versus natural one. \n",
    "* grab some other sounds and create snips (may be look at test sounds or other sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def moving_window_log_likelihoods(input_string, window_size, model):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihoods of all moving windows of size window_size across the entire input_string.\n",
    "\n",
    "    Args:\n",
    "    - input_string (str): The input string to compute the likelihoods for.\n",
    "    - window_size (int): The size of the moving window.\n",
    "    - model (GPT): An instance of the GPT model.\n",
    "    - tokenizer: A tokenizer that can convert the input string to token IDs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of log-likelihoods for each window.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the log-likelihoods\n",
    "    log_likelihoods = []\n",
    "\n",
    "    # Slide a window of size window_size across the input_string\n",
    "    for i in range(0, len(input_string) - window_size + 1):\n",
    "        window_string = input_string[i:i+window_size]\n",
    "        log_likelihood = compute_log_likelihood(model, window_string)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    return np.array(log_likelihoods)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.3496747 , -20.03447247, -20.52007389, -21.59149623,\n",
       "       -20.52059722, -19.0582062 , -16.07400775, -21.26709992,\n",
       "       -21.87084675, -18.70573139, -19.14659035, -18.95660639,\n",
       "       -15.4932701 , -13.21155405, -11.92269629, -10.59679806,\n",
       "       -10.4375267 ,  -9.15566492, -22.41205692, -11.70778525,\n",
       "        -8.82194322,  -8.14262274,  -8.1021139 , -12.41041684,\n",
       "       -12.994169  , -24.13353395, -17.13440233, -13.15799737,\n",
       "       -12.9057613 , -12.86275363, -14.32159573, -14.27828735,\n",
       "       -13.03158402, -11.83986926, -12.73487008,  -8.93803596,\n",
       "       -10.55286092, -10.99016857, -11.03837538, -10.65971494,\n",
       "        -9.87672672,  -9.09061527,  -7.53251314,  -4.35966837,\n",
       "        -1.54343265,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -9.87375581,\n",
       "       -10.66006196, -10.39861339,  -9.90464854, -15.56685328,\n",
       "       -21.09625709, -34.9593091 , -31.25606745, -26.26073921,\n",
       "       -23.47232962, -19.72706759, -18.76490366, -18.71587777,\n",
       "       -18.29332   , -14.47341448, -13.05910873, -17.87304908,\n",
       "       -12.02263933, -10.87906158, -15.34284103, -16.67613029,\n",
       "       -20.39381099, -23.50698757, -23.2901597 , -21.89056641,\n",
       "       -21.7613225 , -19.94813013, -19.7786293 , -18.54837227,\n",
       "       -14.05204272, -11.82005751, -11.05435503, -15.10643351,\n",
       "       -14.72758937, -13.96352845, -13.24906182, -12.60210687,\n",
       "       -11.47226357, -10.30530858, -13.49056125, -11.62591743,\n",
       "       -16.80159616, -20.44084597, -20.38060522, -20.05909455,\n",
       "       -20.13904119, -20.32907832, -20.08862293, -17.62218761,\n",
       "       -20.76257002, -11.43522418, -12.98463678, -12.01892418,\n",
       "       -18.21189493, -13.19157946, -16.64049762, -14.55988699,\n",
       "       -11.89518583, -10.89900762,  -7.5748601 ,  -6.07366097,\n",
       "        -4.45559263,  -4.47710896,  -2.66096619,  -2.6100069 ,\n",
       "        -5.21675777,  -6.39141691,  -7.27858007,  -7.94690526,\n",
       "        -8.38370872,  -8.58498287, -11.07661128, -11.23577571,\n",
       "       -11.23173299, -10.01773131,  -8.88044173,  -7.5719651 ,\n",
       "        -6.11006144,  -4.45057678,  -2.66096619,  -2.6100069 ,\n",
       "        -2.6100069 ,  -2.6100069 ,  -2.6100069 ,  -2.6100069 ,\n",
       "        -2.6100069 ,  -2.6100069 ,  -2.6100069 ,  -2.6100069 ,\n",
       "       -13.85069895, -14.22620672, -14.59630013, -11.43834984,\n",
       "       -15.25118297, -12.19347358, -29.02149379, -22.57828039,\n",
       "       -31.16986132, -21.44571793, -19.83912706, -15.11181235,\n",
       "       -15.21318746, -15.79379141, -15.78066874, -10.51249361,\n",
       "       -13.03027758, -10.14823508,  -8.86636212,  -7.59280908,\n",
       "        -6.08624923,  -4.45459867,  -4.48652983,  -4.47710896,\n",
       "        -2.66096619,  -2.6100069 ,  -2.6100069 ,  -5.21675777,\n",
       "        -4.47237897,  -4.48261213,  -4.47001576,  -4.48068357,\n",
       "        -4.47377372,  -4.46882558,  -4.48652983,  -4.47710896,\n",
       "        -2.66096619,  -2.6100069 ,  -2.6100069 ,  -5.21675777,\n",
       "        -4.47237897,  -4.48261213,  -4.47001576,  -4.48068357,\n",
       "        -4.47377372,  -4.46882558,  -4.48652983,  -4.47710896,\n",
       "        -2.66096619,  -2.6100069 ,  -2.6100069 ,  -8.98149943,\n",
       "        -5.57516146,  -5.58037448,  -9.37238157,  -7.01443219,\n",
       "        -9.60573936, -14.11366981, -14.00460124, -13.0954864 ,\n",
       "       -12.57180595, -18.55723417, -22.17048001, -22.77373415,\n",
       "       -15.17200065, -23.24935472, -20.75618798, -19.33222127,\n",
       "       -22.87280273, -20.52175093, -16.99115753, -11.51270586,\n",
       "       -11.6249308 ,  -9.63304499,  -9.33889827,  -5.93527222,\n",
       "        -1.42255408,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -6.46397275,  -7.5200929 ,  -7.65611386,\n",
       "        -7.54203868,  -7.0019809 , -10.44659042, -10.6566779 ,\n",
       "       -11.38177568, -10.53512096, -18.89436164, -20.43510377,\n",
       "       -16.46497272, -12.42391588,  -6.96173021, -10.24697691,\n",
       "       -26.98469052, -17.70262718, -16.04355219, -13.74399877,\n",
       "       -12.38807511, -18.96595097, -13.79985487, -17.32416189,\n",
       "       -15.43750137, -11.35388723,  -7.8737216 ,  -7.76766136,\n",
       "        -8.01111609,  -8.07906783,  -4.12784302,  -0.70616093,\n",
       "        -0.25761905, -14.5370574 ,  -7.31402105,  -7.7152732 ,\n",
       "       -12.7167387 , -10.48256458, -11.04906196, -43.25350228,\n",
       "       -18.1689238 , -20.36226411, -13.63518783, -12.67564186,\n",
       "       -17.00112031, -17.47578847, -16.69232726, -21.00265235,\n",
       "       -14.36074099, -10.8610386 , -11.90952905, -11.41451657,\n",
       "        -8.86125394,  -8.94377698,  -4.29895079,  -0.70616093,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905, -23.32740603,\n",
       "       -11.59518528, -10.50378615, -21.36852026, -22.98864114,\n",
       "       -17.91727209, -20.74437654, -20.37716198, -27.22318965,\n",
       "       -19.21585315, -20.62383866, -24.07453692, -16.93981713,\n",
       "       -17.54355669, -15.24488124, -15.78754883, -15.84201056,\n",
       "       -10.8807787 ,  -6.29531699,  -3.63100117,  -3.91170403,\n",
       "        -0.70616093,  -6.46397275,  -7.5200929 ,  -6.10279062,\n",
       "        -6.18976296,  -6.25224038,  -7.23187566,  -8.53711729,\n",
       "        -8.63485323,  -8.98051375,  -6.10599624,  -7.40825522,\n",
       "        -6.43630496,  -6.47641227,  -6.58151004,  -3.57199347,\n",
       "       -11.88791239,  -7.72733204,  -9.3263247 ,  -9.99698114,\n",
       "       -10.2430833 ,  -9.7882905 , -12.54261963, -13.31780951,\n",
       "       -13.19305582,  -7.215388  , -10.69517564,  -7.99856526,\n",
       "        -6.14148388,  -6.44863028,  -3.59578687,  -3.88068348,\n",
       "        -3.93976417,  -3.91170403, -16.33085281, -14.55980301,\n",
       "       -17.14103651, -15.52286673, -19.84293556, -24.53440332,\n",
       "       -25.03716278, -25.31031704, -29.52768683, -23.83362478,\n",
       "       -20.58471107, -19.13329078, -20.08813868, -15.11951035,\n",
       "        -6.69229282,  -3.61621481,  -0.70616093,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -0.25761905,  -0.25761905,  -0.25761905,\n",
       "        -0.25761905,  -7.96548307,  -4.27798247, -11.25439262,\n",
       "       -30.22919774, -19.04010797, -18.18245661, -20.02645731,\n",
       "       -21.82230783, -21.67585075, -26.58475947, -27.67286164,\n",
       "       -24.44419181, -19.61171079, -20.16060895, -16.16698265,\n",
       "       -15.83955789, -16.27470994, -16.2544719 , -17.61742175,\n",
       "       -18.92082453, -18.42966938, -18.55190253, -23.26868841,\n",
       "       -22.17498296, -19.56753522, -15.70909989, -12.16133741,\n",
       "        -8.19431502,  -6.30289429,  -7.11611807,  -3.57167512,\n",
       "        -3.68726987,  -3.81922489,  -3.71415102,  -8.79948997,\n",
       "        -9.34048273,  -9.01261872,  -9.29001467,  -4.83759564,\n",
       "       -16.68437332, -11.64447892, -11.38476304, -11.68244339,\n",
       "        -9.54577589, -16.43794155, -13.50777268, -19.63400817,\n",
       "       -17.4805429 , -16.16863668, -16.64139456, -16.79508203,\n",
       "       -17.96243513, -14.99719204, -11.59454048,  -8.81186676,\n",
       "        -4.29895079,  -0.70616093,  -0.25761905,  -0.25761905,\n",
       "       -29.8355498 , -29.36230356, -16.63194871, -15.07476676,\n",
       "       -14.8402071 , -19.66002941, -19.42655706, -13.86205506,\n",
       "       -20.8106606 , -17.56544709, -10.82411809,  -8.39181568,\n",
       "        -6.20106073, -13.80978298,  -8.75761227,  -8.59078673,\n",
       "        -5.78978741,  -5.6238085 ,  -5.75909472, -10.1009922 ,\n",
       "        -8.92947009,  -8.84372631,  -9.9925437 , -10.45022821,\n",
       "       -11.20707339, -11.13267851, -13.12785375, -13.21859908,\n",
       "        -8.87044197,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236,\n",
       "        -9.54472625,  -9.25612736,  -9.01047158,  -8.10382676,\n",
       "        -7.09630495,  -7.07929236,  -9.54472625,  -9.25612736,\n",
       "        -9.01047158,  -8.10382676,  -7.09630495,  -7.07929236])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs = moving_window_log_likelihoods(input_string, window_size=10, model=model)\n",
    "all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d87dd90>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACMZklEQVR4nO29eZgcVb3//+5lepbMlmQmkz1kDyQkZIGQQFgkEhAXXBBlkU0QhZ8giARRQL2YK/pF0esV8Qp4BQVUUK+CJCwiYggQEiBAQgIhiUlmsmf26a1+f3SfqlOnT1VXVdfSU/15PU+eTHdXV51aus67PmtEURQFBEEQBEEQISEa9AAIgiAIgiDchMQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChIh70AEolm81i165daGhoQCQSCXo4BEEQBEFYQFEUdHV1YfTo0YhG3bW1DHpxs2vXLowbNy7oYRAEQRAE4YAdO3Zg7Nixrq5z0IubhoYGALmD09jYGPBoCIIgCIKwQmdnJ8aNG6fO424y6MUNc0U1NjaSuCEIgiCIQYYXISUUUEwQBEEQRKggcUMQBEEQRKggcUMQBEEQRKggcUMQBEEQRKggcUMQBEEQRKggcUMQBEEQRKggcUMQBEEQRKgoC3Hz05/+FEcccQRqamqwcOFCvPTSS0EPiSAIgiCIQUrg4ubhhx/Gddddh1tvvRWvvvoq5syZg2XLlmHPnj1BD40gCIIgiEFI4OLmzjvvxOWXX45LLrkERx11FO6++27U1dXh3nvvDXpoBEEQBEEMQgIVN8lkEmvXrsXSpUvV96LRKJYuXYrVq1dLvzMwMIDOzk7dP4IgCIIgCEag4mbfvn3IZDJoa2vTvd/W1ob29nbpd1asWIGmpib1H3UEJwiCIAiCZ9A1zrzppptw3XXXqa9ZV9HBzAtb9uGptzssLTu5tR4XHD/B4xERxXjgxW14d2+34+9HEMGH54zCvPFDSxpHOpPFvS9sxZKprThylL3GsY+8vANvt9u3fJ42ow0nTm2x/T2/eeqtDrzw7j5P1v2BGSOwZGqrJ+u2yivvH8CTb7bjI3NGY/bY5kDHQhDlRqDipqWlBbFYDB0d+om9o6MDI0eOlH6nuroa1dXVfgzPN659eD32dg1YXv74ScMxZUS9hyMizNjc0YVv/HFDyetZs3U//vrlJSWt47///i7uXPUOYtFNePe7H7L8vff2duNrf3jd0TYff2M31nx9afEFA6Q/lcGXfvMqkumsJ+v/v9d245VvBHsMzv+fNRhIZ/H85n3427UnBToWgig3AhU3iUQC8+fPx9NPP42zzz4bAJDNZvH000/j6quvDnJovpHNKtjXnRM2l54wEbUJY0/h//5rG7oG0ugZSPs1PEJCR2fufLXUJ3DusfathrsP9+PRV3ei24Xz+OfXdgEAMlnF1vfe3t0FABjTXIuz54629J3DfSk88OJ29Axk7A0yALbs6UYynUV9dRwXLXbP0tnZl8avX9wW+G+wqz+Fgbxw6+xLBToWgihHAndLXXfddbjooouwYMECHHfccfjRj36Enp4eXHLJJUEPzRd6kmko+Xnpa2dMR01VzHDZP63fha6BNLKKvYmMcJfO/txkMrFlCG5YNsP299dtP4hHX91pW5DI2LLHmWtsU0dO3JwwZbjlfdi2vwcPvLgdis3rL5NVEI0AkUjE9jidsnlPbv+OGtXo6BwZsfNQH3794jZkAv4NbubOe2NtVYAjIYjyJHBxc+6552Lv3r245ZZb0N7ejmOOOQZ/+9vfCoKMw0pXf+4JsCoWQXXcPL47mp8cXJgTiRI4nH9SbnI4qcSi+fMY4Inckp/8p7U1WP5OBPbFSX8qg2U/+gemtTXgF59bYPv7TnmnIzf5T21z132bP3W2BZ7bbOnQxA096xBEIYGLGwC4+uqrK8YNJcLETUNNVdEn23K5sVY6zA3g9ImZidS0i+KmscbeT3l/dxIA0NZYY/k77PK0M+p/vbsP2/b3Ytv+XhvfKp2DPbn9G2lj/6wQyx8EN6xupbA/v38EQcgJvIhfpdOVd3E0WJicyHJTHjDLTWNNiZYbF0VqPGbvp8yEVZXN7wH2LAUZb+J5i8ICiRNFrKF2iZTJbzDNHVjFltwkiMqAxE3AaJYbC+LGg0mRsE+pbql41P2nf7ZOq7DJsSpm39VkZzIN6lpNZrwRNzHuOAfpVkxx26bbAUEUQuImYFhwakN18YmS3VdJ3ARLZ16QOnZLeSBuYjbFTTJj33KjuqVsDDsoAcAsN04sU2bwhznI32EqKJMYQQwSSNwEjC3LTX52IW0TLCUHFLvk2uDFkV1xwyw3cRuWG+aSsTNsPqvIz1ixlEeWmyh3nIPMmNK7pQiCECFxEzB8QHExNH8/3c6CxE6clIyYS5Yb/undrlsqlbFv2VC3YMdywy3rZxCu6pZy3XKjHecgf4apTDCikSAGCyRuAqZ7wE5Ace7/oIMZK51SrQKqW6rESSldguUmVYJbyg68W8rN7LBipNK5bbkec8MdhCAzpsgtRRDmkLgJGBYbUKzGDcBnS5G6CZJ0XhjYtZYwVLdUqZabNG+5sZstlXdLOdgHpwHFfl63Aw4sU1aIlEnMTZq33AQ2CoIoX0jcBIydlFWqc1MesCd2u4KCwb5WquUmldXEjV2rCrPc2LFssCJ+9lLBFenfXpPyKBVcny3l6qptobPc0O2AIAogcRMwdrJW1JgbskgHCnOv2AnG5YlxgeGlWG90T+82V8MmRzuWGydF/HjrRhAxN05S3c3gY24CzZYi3zRBmELiJmCcWG7ILRUszKVjN86FEXMp44YXN3avibSTmJv8/3Ysh7yBwVfLTca6u9cO/CmnbCmCKF9I3ASMnawVqlBcHmSYMHDoltKJmxJOZrKECc5JthQcWG4ynJkxCLeU+zE3EdWCVS51bshNTRCFkLgJGHuWG+bOoJtZkDCXgBuWm1ImyDQnHOysR1EUR641J40zk5x1yU9Lh1cVigE+INz1VVuGTwUnCKKQsmicWcmoacUWJhntidHLEdlHURTs7R5Aa321afPP7oE0dhzQN1CMRSOY3FrvSCj0pzLYcaAXk1vrdcXVgJxofH9/Dya1DCnou5TOZLF1Xw8mDB/iaPLLlBhzE3UpnZh3S9kxp/ATo9e9pXgLg68xNx5ZbgB2/pTysdwENgqCKF9I3ASMnSfMckwFVxQF1zy0Hn9+bRduOnMGvnDyZOlyL763H5f/7ytq0UKeT8wbgzs/fYyt7b61qxMX/nIN9vckcf7C8bj940ern+040IvP3PMidh7qw7KZbfj5hQvUz/Z3D+Dce17Elj3dmD9hKP7wxcW2tgto8Q6uWG5KePp3OsHxFh87AbdO6tzw6ephKOIH5LPdMsHWuSklmJwgKgESNwFj5wmThXiUk7g53JfCn1/bBQDYsKvTcLlHX/23KmxaG6oBAAOpDDr703ino8v2dv/6xi7s70kCADa267//1Nsd2HmoDwDw9m79Z6vf248te7oBAO+0298uwHXUdhpzE3EnoDjlMKCY/56ddHZe2yiKYmql07YVUMyNg1R3q5RDG5QUpUwShCkkbgLGzhNmOdxURXqSGfVvswm2ZyC33C0fPgqXnjgRAPD3TXtw8X0vO9oftj5AE4iM3qTxZz0DmuXIqbBg4ibm1C3lUkCxLmPGoavInuVG33rAiiUnWUJGl1MyWUU9rl64pZg4DTJbSm+1K6MbAkGUCRRQHDBq1oqFJ8xy7C3Vl9TEwkDK+GmyN79cfbWmp0vJ/urjBMxAOqP7rJcfk/AZ/z2nwkIr4ue8hgpzTZVyLvlaJ3YmOL7CshXrC0NnubH4HX4S9qv9Ar9NTyw3Lpy7UiG3FEGYQ+ImYNT2C5YsN7n/yymgWGclMel305fKLVebiKnvlZL91ZtyZrnhv+dkclIUxR1xk9/3UiZ83nJjx0vhKA0cDmNuAnBLDaSdWaasov4OA/whmv3WCIIgcRM4agPDQRpQ3KcTEpmiy9VW8eIm97+T/dFZjAQB058yFlylWm54MeK0/QLAWW5KmCCdNk9UqxOXMPFbFaQphwKsFHSWGy/cUlHnFke3IMsNQZhD4iZgbAUUl2FvKd4SIooM3XJ5UVHHWW7UonBO3FIWLTepjKITEH26GCH7x5IXRE5jbgBtgizFmpHSTXA23FIO41H4OjdWt5ZMa0umfVI32m/KntvNKmydwWZLkeWGIMwgcRMwdgKKI2VYobjPxAXEwwSHzC3lxHLTmzQWVfxngN56w4ux3LbtbVdvuXE+cbKvltR+QVfEz/r3+MnfFtziVoe985BW18joPKcyWax44m38c/M+w/X0pzL43Ss70H64v+g2Ux6mgQNcEb8SHzL+tWUf/t/KTTpLo1WSVMSPIEwhcRMw6o04XnyiKcfeUn0mIkO3XIpZbgoDih1ZbkxElThZ8IHOfYLwsfv0zT8xuxJQ7Jblxk5AscOu5rwRxMr23tx1GC++d0B9bWRs+M2a7fj5c+/hgl+uMVzXQy9txw2/fx0f+vHzRberijcPgokB936H1zy8Hj95Zgtm3foknn67w9Z3eWFbTpZcgigXSNwEjNp+IRYrsmR59pYyC+zl6ZO4pUrp0SMGMvM3eNFyM5DJcJ/piwjabjjJu6VcEDel1blxlgqetlE4kkdf56b48qve0k/YRm6prft6iq7rtX8fBgAc6EkWdcl4WcAP0LKlSnVL7e0aAJC7pi771SsFmX1mpMlyQxCmkLgJGC0V3Irlpvx6S/WZpF0zsllFtdzUSAKKnexNn2id4YRVgVuK+6xPSFe3O0Gx5WM206hFoi7Ebei7glv/Hpv87Vqe7O7vmOZa3WujkBsrsThTRtRrf9/8BF7dftBwWS9bLwDuPWSMH1ane21m+RQppWkqQVQCJG4CJJtVtEqqlmJutO+VC31J7SZrZLnp50SP3nLjPHZBdC/xN/sCtxQvbgTLjV3LScqhMBDR3FLO16HPlnJQ58ajyZ8hNnc0EjFWBJ5oqVj7vrG4Ydut9sgt5UaNIqAwKNjO79ppAUeCqBRI3AQIX0LdXiq4Z0OyTW9KEwtG4oa3pOhTwZ1N8IqiFLiX+Lga8bOkiVUnY9O870aNG0Db91IyiHgXmZMKxVaatfLYdUuJljzjgOLiK0tm9Osyi/nx2nLDHjL4atdOEIOCrdY8ymSVsroHEEQ5QuImQPhJ11r7hdz/5RRQ3G8hoJhZWWqqorrWA07lQTKTLbi585Yb0aqjs9wIVh27lps055YqBVcqFHP75aS3lF3Ljd2AYlHsGoXKWLHciOsy+0rKYUyRVVi21MX3vYzuEgSOKGytuijF+kbUfoEgCqHeUgHCP7EOxt5S6UwWv1q9TXud7+kjTvyyTCnAWSp490AaX3rwVfV1bVUMfakMrnt4PSa2DEEyk0VnvkEn++yrv3sN88cPRTqr4L29+uBVuy4+t1w6cTUo1fk69O0XrJNxKNB0dW4sbLBQ3Dh3S4nWHbPtDzhNdbdIlFN5r+84hMVTWhytR3S1WbXciMuVy/2AIMoJEjcBwm7+8WhEZ9Ewotx6S72yrTDuIZnO6mrZAFyNmyr9+06ypV7auh//eGcvAKCptgqH+1IAgDVbD2DNVi3tuCoWQSIeRV8qgy17utVO4CL2LTfuxNy4kXGjb79gJxXc2eSvt9wUR6wObSTkrLjmRKug2TXjteVGZ30sIai84PhYdJHadaUSRCVC4iZA7Pb4KbfeUl39hSZ5mbhhsQlDqvXvO4kh4rd5z4Xzce49L6qvrzltKhLxKDr7Uzh+4nBc+cBa9bNrl05FVSz32dFjmnDdI68hmc46zpYqOaDYBaGadmi5YRaDWAntI6xk7ImCxEhIWklrNiszYLSsVzE3PKW4J8WAYqtCWxSDZXI7IIiygsRNgLCbv9UnzHLrLcVEywlThuNf7+6HorCaMlXS5YZU6y839tBrZ3eYFWjpkSOwcNJw9f2aqii+8sFpumX5yfXapfrPbvjd6wDsBzOrMTclujzcsNzoJnwbq2HbrLKdCm5rcVfdUqKVw8xS5XWFYn7bTjchCwrOZLNY8fjbaKytwlWnTjH9Lk+Z3A4IoqyggOIAsW25yS9WLnVuevJZSUMScTXtdiBVOIHxy/E4qduj9ajSr6u5NmF5HYDzInrMylBVgtUjt3042r5uLLr2C/YrFJdidbCytQLLjaFbykLMTdq6tcJrtxR/rJ26pfig4Jqq3Di37OnBz//xHr7/5Cbz75aL6ZYgyhgSNwHCnmyt1uMot95SvEWGPSWLT9i55TL55US3VO5/OxNzr4GLq7muSra4IU4bVzJBUXK2lJoG704RP3sBxc66gpcaUGy0r1ZibpKCYDG7ZgY8dkvxgjTmirjJXcud+fgxwFzwF8bclMkNgSDKCBI3AaJZbqzdIMstFZwXLYl47gYttdwMyC03qlvKzjYNLDdD64wtNzLh4zQV22mmkYgrbimn7RfUfXCeCm7lpIl1bowsNHZibtiDgNn+qoUxvbLcZHnLjbN18Ptck//tWD2fBTE35XE7IIiygsRNgCQdx9x4NiRb8KKlNpHbh75UYZCxKkiqxWwp+9aLXtXFlVvXkqm5NNzPL5lYsOyn5o8FAHz19OkFnzltf6ClgrtjuXGv/YKd6rbOgqL12sZBnRuDMdqpc8OsHGaWDa8Divn9cHr+2INNNKL9/nlrjtn5tJoyThCVDAUUB0jSdrZUefWW4q0ojTVVAPrUGjM8vQYBxU7q9jBrUV1+Xb/43ALsONCLqW0NBcuu+MTRuHzJJExrqy/4TI15se2WYsKg1Jib0htn6jpD2/qewzo3EZtuKYtBwFYma7auatUtZbxsKmPP3WsX3nDi1IrK4mbisagqMpO6oozG3xUtXeVxNyCI8oIsNwFi13JTShdtL1CtKNUxNNXmXD983ADDOKA497+tmBvBclNTFZMKGyAnGqePbJAGfTLLyYd/8k/8Zs12y9vPuFTnxmnMD4+usJ2tbCmHdW64vy3VuREsN0Yixo7lRnVLmbVfsOnutUtWZ7lxtg4WIJ2IRdVrQdfl3WT/CrOlih+/d/d2Y8Xjb+NAT9LJcAli0EHiJkDY5GTXclMuVmk+oDhnuTEQN2psjhBzk58u7eyOUbaUXfhCbA+8uM1kST2pjDOrh4gr7RcsujFE3GohUQwW2MuapbphuWFuKbOv2H1osAsvLpyevzQX1B2TWG7sxNxY4aoHX8XP//Eevvq712x/lyAGIyRuAsRuPY7yDSiOq5abw1Jxo7e2MJxYonhrUSnwJfTtbF8t4ldqnRs15sb5OpxnSzlzrekqFFs4ZklB3BgHFFvIlrIRUGzX3WsX/npxmu2WTGvngImbAYcxN8VGsGVPFza2dwEAntm4B/1CfzWCCCMkbgLEaUBxmWgbzt0UQ2NtzpIiFTf55erEmJuofUuUGnNTouXG+RO3s0wjEdVyU5Jbin/S995yo4u5sbA8u75ZxWqjY27PLVU8oDjlseWGH67TmClmfUnEIqqLUydWLcTcsNNXbAj7uvWuKFm5BoIIGxRQHCB2YwOcZBd5Ce+W0mJuJNlSeUFSb1Dnxo7ZwS3LjVOBmM5ok1IpqJYbl9ov2Lkk2D6UEjdkJ6CY9RT7/pOb0FATx+cWHYH+VAZf+PVanDSttegx2HmoD+2d/QCA6ioWc1N8u15VKObFWKnZUnEu5kYfUFw85iYeixZtS9GfyhTGPlFvKqICIHETIJrlxtpEXW69pVS3VCKORjO3FLPciHVu4KQruDuWG35SslNl1m5VaSOcZmvJxmKXUmJuIpGcsLGSCj6Qd3/Ucufqlj+9CUXJWVWee2cvnntnL8Y015qu52u/1+JEWE0YS+0XfKhz41SbbtvfCyD3YMPcg7qYG5Pvprig9iSMrVi/+tf7uPXPb+KT88bq3ncSs0MQgw1ySwWI/SJ+ZdZbSnU3cdlS/YXiple13JSeLdU9kFt/Q02J4kbhJyjr209mtKfmUnAnoNh+1gygf/K3i3ql2rLc6LfzX89u0U3kOw/1qX/L9uGFLfvVvy1Zbsq8zo2iKLjukZxg60tm1LYqOjejif7ICIkIRiO49c9vAgD+8Oq/de+T5YaoBEjcBIjd9guajz34m5OiKGrmUn21Zrk51GscUFxXEFBsL+YmlcmiP18BuaHaXrsFEf4Y2imKlnYpzZgJ1VImGjEQ1+puaLV6vM2WYtupFiyT+7sHDIWH7NKeNaZR/Zu5msxEIROgnjXO5MWNg98i33Nr1+F+1XJjN6DY6TVYirWQIAYLJG4CxK6Lo5x6Sw2ks+pNsi4RQ8uQagAoqKOhKIpq4TGy3LDlitHNFQgsNeaGv8FbydZhuNVxmgm9vhIyVwqyZqxabkpIZ2fXoJUtse2IVrasUtiaQfuscM3D8tcWALQ11uS2b5oKnlt3lQ8BxU7i38RsJbXOjUW3VEELEJtDcOrOJIjBBImbABmwnQpePm4pZo0BcvEvw+pzvZ329wzoJtn+VFadDMRsKbsVb7vz26ytipXsFuLFjejeMSNpszaREc35XlgHSyiqVtCY0gfLjRoDbmFbbDst9dUFn8kCzwG5JaQ/byH87/PnWbJeqr2lvCri5zCQmyEKWnYeBiwGFKez+ociu0Mgyw1RCZC4CZBUvtaF1SfMcgooZsHEtVUxxKIRDB+Sm6xTGQVdnPBhVhsAqKsyyJaCNcHG4nlKjbfJbU/7206ApZppVOLEyZp5HpIEYAM5y0b3gFwAqGMRGyhanOZK6WyuNTstvq2MKm4Km5qy7CcR2WXQm+9XVpuIqQMw+w1olg0fYm4cPGgwdy5DrXPDWbNMxU2JAtuOmCeIwQqJmwBJZnI3M8uWm2j59JZSa9zkrTE1VTHV7bSfq6vBx9tEhck0oiukV3ybzC1V74q40TbY0TmAR17eYel7brmlmmtzE/6OA704/39eLGgBseR7z2LWrU+aCpyCHkO+WG6s11piImq4xHLTYSBuZJN6X1IT0lEL4oqtw6uQIn6ITtxSfYbihhOrlsSbs/uBzHKTTGexsb1TFVjth/vVQO90JvcZc6ft6x7Atv09AHL7v6m9S71OD/UmsWVPNxRFgaIo2LKnS82g7B5I452OLnX7W/f1YH/3AIDcMdnU3qU+POw40KteI7Kx/ftgr25s7Jju6x7A+/t61OPCj+1wb6pwbL3Fx9afcja2TFbxZGzvC2Pb2N6p3pecjm1/kbE99VaHLvB/MECp4AHCLDeDsbeUVuNGs8YMr0+geyCN/d0DmNgyJL+cceq2ruKtBUsA+7E1VJd+2Yo3+K/94XV8+thxRb9nt2WGEcxys2brAQC5jKDzFo5XP9/Tlbt5bdh5GMdPGm4wFsFyY/GyyJSS8WUxzCObVVTByqx6PLy4qYpF1OMqnpd93QNcy40YVz7AZNtM3HgcMA04c/EYxdwMpKw1zkyV2N8sJbFUnvPz1XhtxyFcePwEfGTOaHz656sRjQBPX38Kbv/r23jq7Q6cMXMkvrpsGk7/4T+QVYA/fHExfr92B3770g4smDAU/33BPCz53rMYSGfx8wvn461dnbjr6c2YMLwOT1yzBCd+7xkc6k3h9o/PQgQRfP2xNzBsSAL/vPFULPvRP7DjQB+uXToVR45qxBd+vRY1VVG8cOMHcNmvXsH6HYdwwfHj8bFjxuCcu1cjEgGeuf4UfPfxt7HqrQ4sm9mGG5bNwOk/fC4/tkX4/dqd+O1L2zF/wlDcfcF8nPi9ZzCQzuLuC+ZjY3snfvSUNrYl33sGB3tT+I+zZyEWjeCmR9/A0LoqvLD8AzjjR89j+4FeXHPaVMwc3Ygrfr0W1fEo/rX8A/j8/76CddsP4fyF4/HxuWPwqfzYnr7uZPznExux8q0OnH5UG752xgws+9E/kMkq+P2Vi/Doup34zZrtmDe+GT+/cAGW3PEM+lNZ3H3BPGxq78YPn3oH44fV4clrT1LH9p2zZ6EqGsFybmxn3vU8tu3vxZdPm4pZ3NheWP4BXPG/r+DV7Ydw3sLx+OS8Mfjkz3Jje+q6k3HH3zbiyTc78MGj2rD8zBk4/Yfa2B5btxMPrtmOueOb8YvPLcCp3/87ugbSuPUjR+GSEyY6uuaCgMRNgNgtNlZOvaVYR3C+GebwIQls29+rq4jaqwYTFwYAR23G3HTlLTcNNaVlSgEldHN22S1VDLNxiu6Fdzq6MGdcc9F1+pEtxbtrWho0y01LfTX2dQ+go3NAfa86HkMqkzu3/LX9wpZ9OP9/1qivdZYbM3GTn7ujNuoXOcWJW4qPublh2XRs7si1RrDqlmKCyq2Ym0xWwWs7DgEAXnxvP8YNq82PAdi6rxtPvd0BAPjbm+1YNqtNPUfv7unGH17dCQB4ZdtBbGrvUq1PW/Z04+/v7AWQq+nz7p4eNZNyc0e32oPuQE8S7+7pwY4DOavAxt1dqjWhP5XF9gO9WK+O7QAmDMs9NCkK8N7ebqx6Kze2J9/swJmzRqlj27KnG4+ty6XAry0YWxeeE8Z2MD+2LXu6Vff3wd4U3t3Tg+0HctaOje2d6M+fo4F0bmzrtufGtvq9/eoDXW5sPViZH9vKtzpw1uxR6nHfsqcbf1yXO26vbj+Edzq61CzQzR3d+Mfm3Ni2H+jFlj3d2tg6utQ6X2xsrF7S27s71Rg8NrZX2dje3Y9JwtiefDM3tlVvdeDD3Ng27+nGn9bvAgCs234I77R3qWEGYtZjuUNuqQCxW6G4nHpL9UotN7lJbG+3NnF1D8gL+AH2Y27Yj0zMunKCU4HoVhE/5paSwbsZ7DRQ/NhPX7DkosiUEnMjGaN8G5y44bKdRjXlsp32cdcIb7nk13v3c+/q1lmbiGkxPybb99otxePERcwm76NGNeKqU6eosUEDFrOl0qrlzVk7FtHixze7ra6K4kCP9rqfsyZVxSK6zwbS+urHfKbkQCqj3iMAYPfhPu57WRzoTUo/S2cV3Xr2dmnXSU1VVPc9fmyxaES//XRW9zn/vYF0Vq29VTi2jC7In/8sk1V0n+nGFo/ptt/PCdVIBAVj4+OuxM96TI7bQYPjlhXGtq+Lf3iI6r4nWg4PCueNj5Nkx+24icN0luXBAImbALFfobh8ekvJRAubuNq5Hx37EctSt+1abtyMuXFeNp9l4pT20xk6xNhyY6W8v6Io0sBQK35x1XLjwPoUsWA5AfQT6HAuoFisdQQAFx4/Qf3brHJ0bVVMS0W34JaK+WG5cZBVzSw37BpgFjR+MjaL5VEtNw4DpsVrip/4slkYTuC1VTEc6NFe8+MFCifpbt0k3c99phcQfHD5QDqjWw//WUYYGy+Q66oEcSFM4Ae4ZQfSWXRxxUZ1Y0tlcYCr1aUfW9ZwbFlF0R1HPu6w6Nh0xy2jWqgLxmay/YF0FvsNPsttI8X9zY0tESs4b/xvi+3HsDrjh7FyhcRNgAyo4sZunZvg1Y1MtIxqypmzdx/SflhmlhseK/vEqhO7YbmRYaXejd2q0kYMH1IYZMvgXR12m02+tauz6LZLySaKwNp+8+Pji1TOmzBUt9zEliG4+gNTVNHEP9GKlpe6RNxSOQS2aTttNZziyC3FBUgDWmwQ75YyW21KsLxZzZJjiIHoB3v11pj9OsuF8SS5XyhjwLsaC60MxhPxrkPCBN5r9JnJ2CJ66ww/TgBo58eWyuj2uVBAaMvqtp/Siwv+s2Q6qxM0/DqjEb1V6aBQ6FTcvvFxyxhufyCd0X2v4Jhy+1Qwtt7iYxsqiZsrd0jcBIjq2pE8zcoop1RwsUszAIxuzlludnGWG/YUwioY80RtZkup63LBciPDSrdkvuFhKSTiUVz3wWnq62HczYMXBrJJfPfhPpzz89XS9W5s7yq6bdWtUUoqeJHzxRcYjEUjWPfND+KF5R/A+GF1uuXOmDUSVbGour4ldzyrfU8QJ9XxKBdUb7xtP91SjrKl8k/uNXlxI6tzYyZY1PYLedFoV1+JxR8PCS4bfpJsF90iPfLPxNeHepM6y45uPSnj9Zhtw+n3CsbWl9LFPbUXuKVSxp/1Gn1W5LgZfCa+PtyX0gl8d45bkX0y+Ix/PczE0lyukLgJEDUo16IlopyK+MnihVTLDfdkwPz5MkHitEKxG24pGcU6LAPuuaUA4OpTp+C7Hz86v15t2zpxIxnSl3+7Tg1kFOkpUhsHKLHOTf7/YtYCPl05Eolg6JAExjTXFtQoksUusfPAW15qqqKIRiPWUsGz/mVLOfktsomVWW7kXcGNv6+2XzDZP7Pfk2ihFN0SBw2sI6JbRLTq8K/NPuvsT6n3voJtpDKG2yiM1REsJxbH1m46trSJO814/0Wrim5smawuycLp2LoG0roaYoXuNPvHLZVRdO49o7ENM7E0lyskblwkncni7d2dlp/mZOnUZpRTbylZITFmudl9qF+d3FjmQTHLjaVsKTWg2JuniAFL4iYv6uKlT5zRaARLprbo1gsUt9yw7A0ZVvpkadk2zvfBquVGtA6JmW6yvmrMbcPrHubWtFJnhx0Cr7KlWrnsLyexW31cajsgt6CZipMCt1QhZtdyoeVGe3JPChOhGLvR0dVv/Fmn8Wf8a7HGEf+6N5lBJxdzwn8vaRKrU2z7boytJ5lWa/aI38tZZ+Sf+TG23lRadx71xy1retzaLYyNLDcVTCqTxbIf/QNn3vU87n1hq6Xv9AqF8IqhxtyUQWsYWdbQ6KZa1FbFkMxk8X6+yBcrs98oSd/m5x5L2VIuViiWYc1yw2qMuPPTYfFWfHBwuoi44R+8RYFiZbJNlxJzY1EwZAxcX+K5k1nAWLYGL06YCCqHbKnfXr6wYFt2YOKmJi9uYhKRaclyw4ShZFlzcaP/jHeZ9AoTuFPrjNPPxMmV/6wvldFV9PZ7bHxMUcHYkhmde6+sxpbKGMbxWN3GUAoorly27uvBu3tzE/oOkydrnm415mbwuaVkgbXRaATT2uoBAO/kYz80y42siJ/NCsUD3rqljJo58rhVxI/BJv9MVlEtfrzlT5YRxSancxeMww/OmSP9zIyMgVXFCppbyhwj15fonpQF0/eaihsLRfyYW8ojy82UEQ345LyxAJxlS7E04Zp8vJosq8tMvKkxNybnb8CkIasYUMxnRKUyis4qJgp+/rX42YDJZ258r5zHluaKVoZxbMMooLhy4RWvVdcAC7izHnOT+78sAooN6r1Ma2sAkCsqBWjWFpnlBoClGAoGi7lxo0KxDCtuKRavkHDBLQXo+4qxLJi0TtwUjolNTl88ZTI+dswY3Wf2LDcO9sGia1QVUML1IboUZSKRVbXmY2aYCLLSCJsdAkf7ZxE2bCcPGmJlcqlbyuT7ouVN9tux45ba0zVgsCRB5CBxU8HsPqQv9lSMXr6hpOVsqfLpLWVUzG7BEblU3yc2tENRFNUtZeRKslK3hNHlcUCxFXHjVldwBu+WkbUgkIkbsyrJ4lO5DLX5p6eWG7mAKnBLSSw3LBaN/ypbjr1lngqe+8zLTHC2X06ypcT2CTL3oLWu4Ma/HZkVki1P4oawC4mbCkaMQC8GezqNRSPSoEoZ5dRbik2i4uR05tGjkIhFsXlPN/59sM80oBiwV3WZBRS70X5BRhAxN7zAYKKDFzdJqVvKWGB5bbmxKkaNXF91iZha7BEwEDf5mBTercREYNSC6cbrgGJ+3U7q3KSF3l4ykWrmXdSsYsb7JxbYA7TfjZgttcegiSlBADmXMMvsG0yQuHEJfZnu4pOk2lU7EbMcpFlOvaWMWkc01lSpfZO6+tNcKrhckFiJoQD0pd69KuIXhFsqlyqd+5sdU37CTAljUhTFNGammEs0ncni3wdz12pp1qci2zEQUJFIBNefPl19LQsoZlbNiMxyY6mIXwluN4uUZLkRfjuycZq5abU6Rca9pUTLTXNdFY7NW1X/77Vd2JrvAJ3KZAuK8REEz/AhCV8KYroNiRuX0AXlGdzwHn9jN25+7A2kM1kuDdz6RB0twc/vNinhBstTXZV7ry+VUa0tsoBiwHp6ew/XC8YrcWOnzo1bbqlIJKKW0bfiluKtgrJCgsUsCQ+/skP925nlxtpyGZPO1Sz9HZC73VSrJm+5yQff2nFLeVnmphTLjfjbkbmlzXuK6VP5Zb+dAcFys3DiMNVy8+r2Q/ja718DoJXXj0Ujnv2uiMHNYKxODJC4cQVFUXRP/RkDt9SXHnwVD67Zjt+t/bd6A7clbsqotxSzKFRJ3AqsavGBnqQ6VkPLjYW6JYAWmFyXiLnyRM67RhhWxE3SZbcUwMVCSNxSheImW/A9HqNrj7G5o1v9e7iDm5Yac1Oszk3G2HrS1qgde9nnsmwplhlk5TfArClePm2WYkUVY2bGCVWbc+s1XjG7Bs26NPcLlpuqWFQnNF9+/yAAraBgXVUMNVXaNd3WqC/axr8e0WD8mfg9flmz77U2GH8vGtFfq36PraW+Wifq+WUjEaClPsixJXQi3ouxDcY0cIDETcm8+N5+zP3OKvzr3f3qe8XScXce7FMnazvippx6S7FJNiGZYJmrgVW+TMSjaql5EasxNyyY2K0aN7+5/PiCLrdWUsHddksBmkBMScWN/rjwAcMy61ExtxSblI+bOAwjGgsFXjHUmJsiy2muM/kt5r6Lj8VlJ07EB49qK/hMy5bit6v/37z9Qu5/LxtnqtlSDtSNWADziOFDCpYx+zkwEc6+L3VLCZabeDRiEICuBafzYmnW6Cb175b6BEZy18rssdpniXgUU0c0SL8HAEePaZL+LS47ra1eF3/FLzt1RIPu/mE2Nv57ro4tZjS2esOxDR+SwMh81faCscWiamZpKWObOqLB8LhNaa1HbUI+tmFDEmpF+WJjm5ov7zHYIHFTIp/75Uu6ypBA8QmGL8fdWm9dFdsJvvWalElQK3NL7cu76oysNoD1J2C1xo1LpvOJLUPU1geMINxSgCYAkvkUYbNU8BQnnGUun2LxXkzALZ483NFYLVtuigQtnzpjBL754aOkx1GLudG+y/7WVmfFLeWh5YarT2QXMdttTHNtwTJmv3H2ffY7ky0qWm6i0UhB09N0JqtLK+/kCuTxovOkaa26750+c6T69/GThutczsu4z44c1aizSvGfVcejOGZcs/r6lGkjdL8/fpunTG/VdfE+fSY3tqmtuuuE38bCicPQxCUy8J/NGNlgOLZEPIq547mxTW/VWeZPns6PbYSuXYNubNNadVYV3dgmDdMlWYhjGz9cPraqWKRgbHzwuH5srTp3vv64teh+m/w2jhOO2ynTR2AwQuKmRGTNFoul4ybTWezJlzEXzbFmlFNAcUp4euRh2V9785Ybo3gbwFrFWYBPA/euDHixgOLeZFqdeN3MHkioKbrF3VJ800uZ26WYsGb7aObSMEM9XxZ7S5ll9BjBHhZ4y0tE/d9CET8/UsFLyZYSrFqyjDGz/dOspsa3b5nlhm/ICOT6KPGtOPi+RWOGaoLrlOkj8Nq/D6uv+af8k6e14r188VIAOJGLpzp5Wquu2vEHjtQmyUWTh+uuoFO4SbmtsVpnoT15WquuJcOYZm3iP3l6K1779yH19exxwtj2GY+ts18+tuMn6YU/P7m3NhSOjX+4HTu0jvteK17njtscTsydPK0V73NjWzJNG9tJ01rVEhpA7kGAHxv/u+fH1lKf0ImSk6eN0PW7Gqcb2wi8zh23YzjBdPK0VmzbrxWiXThxGAYjnomb22+/HYsXL0ZdXR2am5uly2zfvh1nnXUW6urqMGLECNxwww1Ip4s3/it3irmlBtJZNQC5tcG6a6CcekvJKhQz2MTJ9tEsddtqtlT3gHEDzlL4xecWqH8Xs9y88v5BZJXck7YdUVoMO24psxo34ndlaN3cS/vpl2q5MWN3PjWZt14wYW+p/UL+NHrZOLOU+Le05Lez8isn4Z4L56uxYKlMFmf/9AVc/8hrBd9PmsS7MUShHotGdbW1gFwHanY98efpyFGNOnG0ZEqL7nv8uE+Zrp8I+Yl/8eThOnHRUq/9ZhZNGq7rkTZlRL3uM76Z44IjtMl1xsgGJDOaSFsytVV3Dng36CnTR2D7fm37vKVk0eThasZYsbFNFca2n2uCyep6ATn3FX8PWTK1Vfd7jAvHbTt33Hjr9qJJw9X2NYD+Afj4ScN135vGuYyOnzRc16Dz2Ina2KaMqNc9iC+Z2qK7t/APqblzqm3fKKSg3PFM3CSTSZxzzjn44he/KP08k8ngrLPOQjKZxL/+9S/86le/wv33349bbrnFqyH5hjXLTe7HKwZymWFVCPiBURE/gLPcqG4pY0FiVbCpHcFdzuj44FFtOHfBOADFY25efv8AgMKnp1Jh7iVZtpRoGdQ6Qst/usUtN7l9lFkLrGFtv0spFLj7UB9SmazuOLDdtdR+gaWCl61bilm1tHMwra0Bp88cqU4kq9/dj/U7DuEPr/674Pus9hEvUMXfT7/QfiEejeg6cQM5ccOXFfj8iRNRUxXFjz9zDOZNGIrmuiqcOWskhg5JYPmZMxCPRvDIFxZh7NA6jG6qwYIJQzGpZQi+/bGZiEUjuPuCeRiSiGPGyAZMGVGPRZOH45rTpiAejeA7Z88CAJw4pQUjG2vwmePG4/yF45GIR3Ht0qmIRCI4+5jRaKyJ48YzZ+D0o0ZiSCKGC47PLXP5komojkfx48/OxTHjcmM7Y+ZIDBuSwE35sT18xfEYO7QWY5prMX/CUExuHYJvfWwWYtEIfnb+PNRVxXDkqEZMbh2CxZNb8OUPTNWNbcnU3NjOWzge5y2cgEQ8imtOy43t43PHoLEmjuVnzsDSI9swJBHD+QvHozoewxUnTUJ1PIqffHYejhnXjKF1VVg2sw3DhiTw9Q/lxvYQN7Z545sxubUet300d9z++/x5qEvEcFR+bCdMacHVH8gft4/NVMfW1liNCxZOwHn54/bl/Ng+MXcMGmri+PqHjsTSI0dgSCKG8/Jj+0J+bP913lx1bKcf1Ybh9dW4+UNHIh6N4LeXH4/RzTUY01yLueObMWWENrafnjfP5tVdPniW+/etb30LAHD//fdLP1+5ciXeeustPPXUU2hra8MxxxyD73znO7jxxhtx2223IZEYnBHagLWYmz2q5caJW0rB/u4BDK93z3pgF7NKvYkCt1TxmJtiU0SnR+IG0J6oip031nxu7NDCGIlSYMdQarlJi24pc8tNsQDXUi03muXEfLlSLDeb93RjzrdW6p6YmTvKWvsF71PBS3NL5R8MJANkx1cUJzwpC+ew0HITQe+A3nLT2ZdSr714LIqbzzoSXztjhvr7fenrS1UrzZUnT8alJ0xUP/v7DaeiKpZzjZ6zYBw+dswY9bP/+/9ORCwSQTQawQdmtOGtb5+hfvarS4+DoiiIx6KYO34oNty2TP3sh+ceg1RGUV+vu+V09e+vf+hI3LBMPrYvnDwZl3Bje/arp6hj+9T8sfjonNHa2K4+AdH82E6dMUI3tvsv0cZ2zLhm3dju/PQcw7HddOYMfPX06errNdzYrjhpMi5eLB/bJ+ePxUe4sf2ZH9t047HNqdOP7f+ZjG35mTNwvcHYLj9pEi5afAR3Tk9R3d2fmDcWH549uoSHoOAJbOSrV6/G0UcfjbY2Lchp2bJl6OzsxJtvvhnUsFyhmFsqlc6qwbZ2LDdMR6zbfgjz/+Mp/GbNdsdjLBVzy03u6dNKQLHVqsteNs1k+1DsKbwvmdvnWovtMuxuXxU3fBE/gzo3sho3gPWYG6c3LTWg2GrMjcOU+d5kRhfnwa4TKy1I2CHwMhW8lGwps3PIRmwmbmR93cTD4cRyE4lEdNdFIh7VHUOrn1XFotK+YEBOZPH7zX8m277ZZ07GFqexmZ43o88GI4GNvr29XSdsAKiv29vbDb83MDCAzs5O3b9yQ+aW4m+CA+mM6rKxk47bLNQb+PpjbzgcYemYpUSzLA52MzUPKM5bo4okKqlNMz0IKGbWhWLCgE0YbpciZ09SmltK3wmZx+ypP/ddnwKKLVpunAQUy7erj7kxEsP878zLCsWllGUwi5ti4s0suN1SQLHMciPE3OzvHtD6XLl0ngiiXLAlbpYvX45IJGL6b+PGjV6NFQCwYsUKNDU1qf/GjRvn6facIJsk+RTevd0D6tNXi41UcNHK0+KyW8rOU6hphWJB8Zungue3bdVyU+1+cBuLCxF77ogwccMXO3ODQreU9pkYc6NNjM4sN6pbyuE+iOnERphVKHYCW02x3lb6IGRXNi2FCacil4wUtS+b5BzKxI1opUpJrG/i4RDjx2LRCM46epTuvff29ahFH2XNOwliMGPLxn/99dfj4osvNl1m0qRJltY1cuRIvPTSS7r3Ojo61M+MuOmmm3Ddddeprzs7OwMTOEamcdnTM2/NYb19muuqbD1Bi/E5RwwvrGzqlFv/tAFPbGjH3649yVIHWJlpnCGaM63E3BTDK6sJYD3mpk8VNx67pTghLAouzaVhZLmxVuem2mGdHj9ibqTbFf43tNxwb3vqlirBcpM2sZawIfPiJKsA/KLJjETcKAr4YG+xcWY8GsGXTp2CI0c1oqOzH//x17exuaNbrXdkZAkkiMGKLXHT2tqK1tbW4gtaYNGiRbj99tuxZ88ejBiRy9VftWoVGhsbcdRRRxl+r7q6GtXVwQXS8nQPyNPWZf1yeHHD6iLYibcBCl0Jbga2/mr1NgDAI6/swJUnTy66fMrMLSWM0yxbqthkBQA7DvRi5Vsd+e25/4TJnlqLxtx47JY62JNC++F+ncgSh2T21A8A73R0Y/2OQ7oCaTylWm4YD7+yHUOHTNbV9eBxGnPTUp/QpbMymAguloLNX0deuqVcyZaSHBsmyPhU7ExW0e1L0kLclCwVvKYqho/MGa3WN9m8p8t1EUoQ5YJntsjt27dj/fr12L59OzKZDNavX4/169ejuzvX2+b000/HUUcdhQsvvBCvvfYannzySXzjG9/AVVddVTbipRh/fm2X9H1ZzE1K8kRdaq0UL0zJZn58HitF/BjmAcXFU3uX3PEsV2zM/X0WU7GN6Et6Y7lhJdq//Ze3cPyKp9HRqdX4EEdkJUbi7J++YPiZGlAccxhzk///gRe346wf/9NwObPeUmbEohF8btGEwu0W1LmRf983t1R+3Y6ypUxqRLF3eHekKPxTEoErjqIgoJjb1sSWXLuHfd1JtTimF78rgggSz67oW265BXPnzsWtt96K7u5uzJ07F3PnzsUrr7wCAIjFYvjLX/6CWCyGRYsW4YILLsDnPvc5fPvb3/ZqSK7zz837pO/L3BsywTPCRgE/GW61YeCfPmUdimWYtl8ocEuZ1LnJL2q1MKE3lhv2FF68+CLgfrbUR+boYyHe3KllCYnHJW3y1G+FgVJjbjhXD199VoTPwrFDLBIxiEXR/2+UrcX/9Pxov+CkoGZKDbaW7Gf+LdFyw8OEDx/7JQ5DFlDMqK+Oq7/zXYf6Cj4niDDgmbi5//77oShKwb9TTjlFXWbChAl4/PHH0dvbi7179+IHP/gB4nHPSu+4Dl++m0eWCi5zVU1uLWyYV4z/vfQ4bjvGN9bt+3vx8+feNXSd8fB9W6xM3IqimKeCV4luqdJ7SzGsWpbsUGU15ibpjVtq3vihutd8lVdx0jJ76ucRr7eBdAa3/flNtRx7qRWKi+HU3SGmrjKuOnUKWwKA8fUiq2rsBWzdTtxS6jmUHBstoJiPudG2kckq6jbNrG8DklRwRiQSUV3iO/Pixq3Ab4IoF8gWWQJd/XLhkJFYaWQT5yyh26sVTprWim/nq1aaZTd96MfPY8UTG/GDJzdJP+9PZdQnb7MncBnprKJOujKxIQarmgUUq3VTArXc2Iu5cdstJa5P0f0tuCQsxrKwJ3LGIy/vwP3/el997bjOjcU5MOMwxTgaLRzb9z81G0fkXSnFKlrzv4lyzJbKZhVVmJnVueEtL/yzEi9aq7h4t4LrRBiYKPRYCQp2nVAqOBE2SNyUgJG4kcXXyNKMxVb2VmE3KrNigcxiw1oGiCz87tOY862V6EtmdOLGSmdsFhAdiciL6okuj3K33Gip4Nbq3LidCi7Cx/4YWW6KTUZ8bxxAa4XBKLXOTTGcWm5iEssNbwksdr345ZZymi2l6+ouzZbKvcfHzPBxPXwsDn8OxWGIl7K4LWa5efG93P3BqZuTIMqVweMDKkMMLTeSOy+fvvmp+WPRUl/tuH2CnafGIZJ2BT0DaVXQ7DjYqxM3xYJqAeBQvg1BU22VdPKa3Fqve20mBqx2BWd4YbnRUsGND2g2q2gxNx43kkvqapzoP0ubtL3gEVOBxevAa7eU02ypqCTmRtcdvJjlhnvfy67gLObGtrjhmxVKjg37OfUbxNzw7TjMXJOiVVf8nYrlHshyQ4QNkusl0CXE3LCJN5VRDANBW+ur8d2PH43rPjjN8XatBsACwBBJDM0e7ik+EYsK4qZwnZmsgvbD/errg3nLzdA6eT2cWWOasGhSrn7GjJENpvVG1CfVdBaHe4u7xzwRNxbSevu5GAi3A4pF+HNQ6JayVhxPvP5EceO8/YK1SdB5zE2h8OK1TrFeZGxSj0a8rXPDdstuzA1vwTWz3LR3ar+3rMRyw/oTGSGOS7xeTpmuL+lBAcVE2CBx45BkOluQkcBnGhXUJ3GxzLnWtK/4sjLLzR7uxpnOZouKmy/8+hUcv+Jp/OvdXHYYC0odWmfsbvr55+bj/kuOxW8vP950fOyeetG9L2HOt1eq6zbCy5gbM6vVq9sOqX/XOHTpmPHpBWPVv3kLkmgYMEvBN0MUDE4DSK3H3DjMlooWuqV0JQ+KVLRmvzsvXVKAc6sQf43Jjo3scPH7mkprljt+UfFwiMdHLBvxgRltmNSiJTQYdZkniMEKXdEOEa02AFDHuSsMGx668ITEBJKR5YY3Scu6aHdwlptURtGJm//469vY09WvW/6pt/cAAP7n+a0ANLeUkeUGyMXZnDJ9BIYWqXYsTkIvbZXHCDG8rHNj9hR+8x+1Pl5RD55yb/3ITPXvZNok5kZNwS9iuRFeiwHtTq0a/LcaTIozOq1zE5XE3EgtN0WypTwXNzAfhxFqbzADy4vMMsZfl7x722wXi1luAP35i5FbiggZJG4cIou34d0V4s3FLHXaLsXSULu5Bnl1CXPLTSarFGRLfeOxDdL1slToA0zcWGjTUAzxBl9sLvQkoNhCzM22/b2Gn7nBkOq4WhrA1C1VpCu4+j2DQGSgNOsXf77MYo/SDntLRSKRghRnXqiwv4qlgnttiFBjf4p0RxcpVqdIJlj4yzJpYLkTxyFabmSCnA/UpvYLRNggceMQmbjhU3rTuowXBRf+MtdHy40nymKWBj52ReYG26uz3GTRKYib9/f3SNfbm8/gOKTG3JTeoVscXTGLgheBsFYsN/Mn5GrR3PYR49YgpcKuDZ24cVjnRiTJXY/DSxCl/FbNTpUWc2PvfMUkqeC89Uez3BilguuX8xq7lhuzjuCAfNx8thTfEZy38hS6pfSvZSKTF6fUOJMIG3RFO6RroNAtxT9N8VaATk4I8YGCTinW16ZYDA1f2C+dVdDZpxdqRk/2r+04hF2H+lTx1GRSv8Yqdu+pQcXcsMnaqJeSG0jFjbCM1To34jd5y80Zs4wb09rBbGLPFGnwKcKsVh+ePdpU3Fhtv+B9zI1Tt5R5tpts2LpsKYduKZl7kBc3dsUyQZQ7JG4cIrPcxKIR9SbCxzjw8TlizxcnqJYGgxsrL15kAohPM01lsgVuKTPX2TUPrVOrp7pRzE6chDbv6dK9FlNaPYm5iRW33LBxeBmbwERrUrD68VitcyPCC6avLZvhdIiFpjYD0jYDiv/wxcX45UUL8PkTJ1pLBTdsv6BlS3kJW71dt5RquTEYoEyU8deA5pbSLyeOwkrMDe9Gp2wpImyQuHGITNxEIzAQN9qyvcnSxY1muZHHiPAuMVllZD6tOZ0pjLkxi2t5/d+H1aBGN1xEohvqjr9twlP5DuBAYWNCL1PBzdovqG4WH9KL0yaWm2JP/ur3xCyr/DVxwfHjS0plt7r37Nq0Omk21yVw2pFtiMeiltxSxWNu/MmWsm25KVKnSGq5kaaCCzE3wkAKs6UKV8zXn6L2C0TYIHHjEFm2VCQSUQPz+AlKjGkpFS1GRP45fzOUVUYWm/IViBsTAdFYW6V+3w2hIbulPrZup258PF7chGOScyaStWmJcAKbuMUifh2d/bjqwVex5r39RZ/81e8Jr90KaLeaZWXXcsMjimZeqGgBxQGngrNsKZvfKxZzIzu+erdU7m/72VKF5523vBYLUCeIwQZd0Q5h1hg+nTICbcLv49xPnQaVjJ2i1rmxkAous0bwTflkbikzy01TbZUuHbVUZHMfb3IXb9JeFGaLc72l3vj3YWmtHRZD5aVFgK1bH1Cs4Ibfv46/vrEb597zopZtY3MysmrxKYZ1yw1z49nfXoHlhjvn6vEPPOYGpuMwolhJCP5d9juwli2lx4rlRh9QTJYbIlyQuHEIi2vh+yZFIxGMaqoFAOw8qDUudNtyUyygmH9f1i+Jj/tJZRTDSssymmqr1OKFTvsT8cgmobguMNvus7F92FP0e/t68JH/+ifO+vHzBcuozQ49tdzk/tdZbgBsau9UX6sWmKIVioG3d3fi4Ze3Q1EUw1gNuygGf4uUYrkRiyTGbFhu2LVfrjE3Wp2b4rde9pDhJFuKAoqJSod6SzmECYKm2irszHfWjUaB8cPq8NbuTuzINy7MZBV0SlxYpVAsdZkXBLJl+MrKuw/3FcQvFBM3+/OWDTdqzsgesM0sN14gTsC7DxdmtPliuWFuKSEVnI/TYk/+VRasZmfelRNpw4dUc+6s0s6Z1R5gquXGwfEaXq9PVddnS5m7g9jw/LLcuB1zw4u2RDyKnmRG2n6hmFtK/NnINldDqeBEiCFx4xDmamqs1Q5hNBLBuGE5y82Og324+7l38cNV7+hutN/75NElb1u13FhoHigrTMdbbsTu0UBxtxSb/F2JuZFZbrgbrR/ixsoEzA6jH5abAZ3lRtGJG6vF8XiLwnv7utVJtdRzZvVslGK5GVIdx5BEDD35/ZalghvH3PhjuYFHMTeiuAH0bmZDC5xY58aCO7cmQZYbIryQuHEIi7nha71EIhGMG5arg7Ju+0G8uv2Q7jtfOGkSzj12fMnbVi03EpdTbzKN+17Yqr6Wu6W0yVMmbsysAtXxKJL5mB2vYm7iPlturLgIVMuNp9lS8oBimZtRNubjjhiGaBR48b0Duif3hpoqy4HIRbF4OuxmS4mMaKzB1n25YpL8MS/efiG/nG/ZUnbdUvnzZ2Ap4Z9F2DmW1bkp6C0lnBjxwUd23VLMDRFmyBbpEOaW0sfcaP2WRGED5DKN3EBtvyC5sX738bfx8vsH1dfFAopl4ka8zY1prlX/fujlHWorAjdSwWU3XaNiiHzqqptYubEzT5EbjU+NMHJL8Zg++Ue0LB4+jqqhJq4W//OiTpCMYm0GitHaUK3+HZfE3BhpCi3mxutsqfw4bH7PluXGLOYmHrXVFVy2JC9uKBWcCBskbhzSrWZLcZYbmN9U3RI3ZkXnnnyzQ/e6WBG/7ZKeSQVppMKNmAkmd9xShe/xN1r+SfZfy08reXsyrNzYVUuEl5ab/OHkJ27R/WL25M+P7EC3lvEVjUS4buLuBRSbUUrMDaB3zfGxIcXaLyg+uaUcVyguku3Gr48JUVm2lOg6LtYVXHZfmjGqQf271Fgsgig36Ip2CEv1rq/W33jNbqqNJl2U7aClgkuEi1AkUNZ+gY+5YZNlS311wXsMo/gGPy03jTVxDHOhUacMa5ab0iZrK1ixNpg9+UcjEVUs7ufS2dNZxVaWjhlW3TClxNwAwHFHDAWQc/u2NWrXphZzI/+eb26p/P92LTfqeTAYH2+lkcXWJTm3pNkeFpZQKFxmcms9fvzZuVh6ZBtOnTHCyvAJYtBAMTcOYU+WfFBehJtcZLhluZFVQWbw1YeBwpucoii6p2LGuGG12Nc9IP2OUbNssXuzE2SmdVm2lJeiQpzwZe6voMQNryUS8ajpkz//db5WTzqTVSfFUou12bbcOLQUXX7SJDTXJXDuseN010ixgGLf3FLF/GMGJIv03OL3i52qi+59CZtvPxNVsajOLcWjS9FXFEPxJ/LROaPx0TmjrS1MEIMIstw4hFk/+Joc0Yh5kTk+PqcU2AQrZkQAhc0fRQEkEzYA8JWl09S/ZYJIRrULMTDygGIuW0phosK7S1UULDKXjzYO77OlePhA0bpEzPTJPxLRJt0DouXGYTfxgvFYnDRLtdyMaKjBVadO0VkUAc4tZTg+v9xSMB2HEVpvMKNUcO1v3gX68vsHAOiL+PG3Gv43KjtHPjVJJ4iygcSNQ5hI4Pv05NxSZuLGJbdUkVRwHjEVnG+9wDOpdQi+/bGZufUWuKXk63alzo3kPX5CTBep6OoG4rplx9UPy41s3fyxP9SbUoPFZZNjNBJRA4p7k1zn94yiK/5WCrzYMrv82CTu9vEqlqXke/sFhzE3RueB3y/+QUnsGG8UUKwoCrbs7TYcL0FUCuSWckA2q1V85TMOIhHzJ0a33VJW0qTFVHDRbcUY0VCj3kBFQWTkAnAjoFg2+fFuIj9EhSgUZO6+tA/jkE1WAwbnq5gFhj/tmax7bimrZFTLjbvbK54K7o9bCqrlxp66SRWpU8T/1vg1s+W1gGL999my/2/lO/ivZ7cUrJePWyKISoDEjQP4VF0xk8M05sYtt5RJQLGIuIzMcjN8SAKJeNSw8rGh5cYFcTN+2JCC93gB4Yc7SJxoRHdfNquok6kfXcF5+pJyS5ssMDgaiSCbn+b4vmNBuqVct9zk/zeMuVG7gru6WcNxuJ0txcf/89chO47GXcFz/4vC5g9fXAQAGF5P4oaoLEjcOIDPNqpN6C03ZjE3btVpUWNulJwZ2mybhTE3hZaAEY01uvVaibmJRtxxFc0e22T6eabE2A0rFHNL8a+9TJmVCQH+WuORHY9IBOojPD9J8m6p0rOlrC3n1Xkr3n4hL6o8Dyh26pYyF5n8by0tETcpodJ0JGI+hpmjm3QPYARRKVDMjQNYnZhYNKK7SRWLuXGro7XOslHEeiO6mGQuF2ayZhkcVlLBixURs8rREnHDb4096XqZ2huPRfGVpdPwkXzWiKLoJxn+GDvN/rGC7HgaihtZthT3d6HlRkshdguztHC1TYTLx0vNljK47tlue9E9noddjkYWJCNSRdx1/Ppk+8iqgxd2BZePgyoPE5UKWW4cwKwf1XF9Z95oRB4gCwDH5ut2uAF/w0pnFZg15xZjbmRiqK0hZ7mJGri7ZN9xI5gYACYOL3RL8Y+i7GbvdQXVa5ZOxeHeFP7vtV0AcvssK5borVuqcN1GXdFlT/68W5T/Xi4V3B2xoUjOjQzPY26MtutXtpTDAN1ilhtdqw1OoLL3U0JAsmqsMzggXluwCKJcIXHjAGa5qamK6W6iMsvNNz98FEY31eC0I9tc2z4vbvgJRlawT5wcZcu0NeXEDZuIxO/I5rCEmaKygcwio7Pc+BDIq41Fv122i7xbytNsKRurlokG/tLL6iZJRX1d6mTHnxszo6FX542trqs/jZffP4BjjximH59PAcVOu4KnitS54dfHCx32Oxfr3ESK+KW8LmZIEOUKuaUcoLPc8DdRSbbUhGF1OPPoUa4E3zJEyw1DJlysWGGYW8qofo7sCd2N6sSMeeOb9dvjtl9qA0Y7GIlGvkFp0BWKGfyT/6UnTAQAXLt0mmpPSAsWAC8Cs83cUhmPUvh5i8k5d68u+Nz/CsX21I3W1b24Wyqt+x3k/h9IGwQUS9ZFuoaoZEjcOMDMciP6+r1otMg/fWd14qbwFicKHmnMTQOz3BjF3BSOwc0mlr/43ALda35zbPi+WG6448qLQF1JfA+HYSdOhI+5ueUjR2Hjd87ArDFa/JJouWHHsVSLBq9nTOvceJUtVWR1WoViVzcrGUjuP9uWmzSLfTJKBdf+1l2DqltK79Zia0mms/jRU+/o1kXxNkQlQ+LGAXrLjfZ+rkKxflkvsmuMLDdpC5YbMQYHAKaPbNCttzAVvPA7fJZYqQyvr8aHZ49SX+tN8+Z1QdzEqGEnX2vHy0BVO2FM4vFgGTHymBtFPYelTni8pcJSzI1HAcVG+FXnRi3iZ/N7qSI9vrIGgezGbqnc53c8uQk/emqzbl2e1/ohiDKGxI0DmOWmuipWUEVUvKF4YbmJcA06xSd0EfE9MXsKAMYOrQVglgpeOAa3AorVbUj+fvLNdry5qxOAPzdqvWjMqm4XPwoJAnbdUvLjz9YgTpLaPjgeHgCxY7nxcmmP3InFjpFaj8hrt5RqubHplrLRFVxmuTHqCs4C4XnIckNUMhRQ7AB9tpQGLzoYpRZNMyIWjSCbUYrG3IjWHFnMDRNoxkX8Cr/jukVK5+5QsG1/D77w67Xa9jxMwWawxqeKAnzzTxvw1q5O/PXLSzRh4FPtFCsUrVAsXBfZrDsWDX1AceF1oSgKnnyzXRU+bl8nVt1SXqeCazE39tC6s1vJlpK5pfJuLWa50fKlCqBMKaKSIXHjAD7mRuxYXBBz41HRt1g0glRG0d8MJS6nrJKz7kSFeJojRzXiE3PH4ANHjtCtM7dM8fYLbj8VHtFSp3t9sDclbM8fI2MsEkFaUfD4G+0AgMfW7cSJU1oAeO8as+WWMrLcRPTnGchbbrwIKJa894/N+3DlA6+qr/223LD99Ppcqb9zp9lSFgKKLVluTHaTMqWISobcUg7gLTf6gGJJzI1XlhtJTRqZywnQt4tgAqihOo7LT5qEya312jpttF9we7+uOnUKtz2lwOLkg+EGQOGEoEATCl5PFk6zpWSIgeYZtyw3goVN5M1dh3Wv3c+W0tOX1Bc59MuFWHpXcKOAYvnvmYk2o/YLMsgtRVQyJG4cwPozFRbxK4y5cbMiLI+sM7gsWwoAuge4DtEmlWNl6zSKKXA7BqYuEccn5o3Jb7MwVsgvy41sMvarkKCbMTd6y03WtYBi6AKKCz8dkc+8Y7ifLaVf34d/8rzuNdtvzy03+f9tx9xkrWdL8c8qWdUtxQKK9dlSMkjbEJUMiRsH8BkL+mypwpgbr56eZDVpZG4pAOju18SN2ZMtM5XzdV2Mgka92C8+A0W0HvmRLQXI4xT8aAEBuCRu8qvQxdzoAords9wYxdzwuH3exNW9u7dH9zqT/216XufGoeWG3TuM3FJKEctNSnVL6bPjZFC2FFHJkLhxgFprIqoXN5FI4Q2lyrOYm8JqwikDt1QXJ27SJsXV2FD5dRql+3oibtQMlELLjU+Gm4JJMQI/LTfurYu3vmUyilbczsWAYtmlwYrMMby23Ij4ZblhNhMjw83qd/fjiw+sxf7uAd37Wo+v4gHFspo3qlsqXnz/yC1FVDIUUOwALWNB74aSTRyexdzkJ3vdE3raQNwMaMG56s1f8uSvWm4siBsvJg8tA0XRNX6c1DIE5x473vXtyZBNCGmX4lWK4Y61oXDS5bPoSrfciGUC9F3peXEzqqnG9aylYsN3z/1mjma5kf8+PvuLFwEAtVUx3HnuMer7xS038u1lsgoURdHuPTE+W0oOWW6ISobEjUWue3g9dh3uw28+f7zuBiV0X/Clzg3bNmCcOsrTpXNLGRfFk8fcyLfvueUmfxOfN74Zj37pBNe3ZYRsv7wqSCfi1WTECw6304Ozij7Ym3UxP27iMNz1mWNc3RZQvGGl/zE3hZ/xndzf26d3m4mp3CJGDxPtnf3o7NN+x2IRPxlkuSEqGRI3FlAUBY+u2wkAeGt3py7mRme5iUYK3CdeuaXYavUBxXLLDR9zw26u8pibvLjhYm5kdXH4Zd2En7i86ipdDNnk71edGzcOqWyISU7cREot4ie8zioKYtx5Y0JqelsDRjXVlrYxGUWOUUa9vr29bphFSqZF3trdqf69fsch7OnqVwOt1To3Bic7YyBu7vjbJvziH++pr60U0SRxQ1QyFHNjAXGCT3F+c/72EYkUPln6arkxCCju6tfcUmx5WUCqVuemuFvKi4BNNh9ls1pxQr9v0HK3lD/NO92w3MjWwEoXAC50BRcuB/H6GMhbLdzsPcZjJLYZfltuZBzu09do4kVJsQrFZrvH137S3FLGkLYhKhkSNxbg68REIxHBLSWmguu/61UqONuOlTo3+lRwY9Egq3NjdLP1ZvIozJbyozIxj+yBnx1Wz8WNR+vXuaVc3oYodti2quPu9R7jaaqtQnNdleHnvte5kYh/8SFjT5cWVJwyKcVgtD6RWDRiaf/IckNUMiRuLMA6+QK5Gwa7eRWmghdmc3idCq4vs19azI3qlrJQ58avbCnfLTeyVHDfLDfWljMzvsg+60lq57/0In5iQLH+836PLTexaAQv3nSa4ef+WW40IV4wBsE9LMtWNHJX8w8Tv/n8QukyfKaVWcA2BRQTlQyJGwsMZPRVUDXLjeCWQqHlxqubbEwaUKy/qdbmO0V3SmJuTIv45TMzAGPLzZyxzc4GboIsW8qv+jYMmYAptyJ+dsex40Cf+rebdW4AiVvKY8sNoHVAl8Gum5jHFj9eiIukhB+NLObNyHIztC6h/r14Sgs+Mmd0wTJ8vI25W4rEDVG5kLixQEoIsE1xcSv6VHD9k1Q8GvGsgZ80FVyw3FTnn575gFKzQF19V2wmbgrv3h+YMQLnLBjncOTG8BOGWeCzl0hjbnwq4md1X82CrItlE7m9C+L1sf1ALwDvLDfFSPsU/M2QpYKLlpvOfr4Ug3njzPsvORZzxjbhN5fnrDayxZIGiQMi5JYiKhnKlrKAKA5YPZmquFjET2+58TJeRGq5EcQNmwT5LCozdw8f5JjJKqiKycXNpSdM9K1Csd/ZUrKnXb8sN1bnY7Prqtg6ShXb4tXAv37ijd1Yu+0gAG8tN2Zk/HJLmVhu2O9waF0VDvam9DFvRRpnzhrThD9dfaL6Wpb1xRr35gZiPEZqnElUMmS5sQAvDjKKwlUoLizix7/2Kg0c0J7o0iZuKfZ0yL9v1riPnxDYPspu3l7tFh+kWS7ZUpGIj0X8uPWb7bdXQepWKIi54S657/1to/p3tY+WG74FiRZQ7HEquFnMTZaJm5yLiY+5SRZpnClS7FSbrcXnWHyCKCtI3FhAb7nJ6txSPGJXcC/9/uzJL6urc6O/1bIJMpnmBZDxky2/P+wJU2a58WqSZ+tVFC7w2ec7tOy48HWNvIR3pZh1/TazSvgdZsFfH6Obtbo2flpu0hJx4/V1Y2q5yV+7Q4fkxE33QFoVhWkbXb2B0sQ9uaWISobEjQV4H3c6o3dL8abfqBBj46XfPyqJuRF9/SzwUGd5MnmyjUU1txr7TpGyIp6gQPGxR5AemSk/ZdKPy034y8Vs8mttqPZ0HGbIivgx+KJ9Xltu/ny1VrVaVqXbr1Rwme0mxbml2Pj6Uhlks1qPL6viptiDBGVLEYQcEjcWEGNu+Cqj4u2Dv6d66fO20n4hLnNLMauTwdhY3A2zTmUl6sarveKfhv2qNCsiE6Qpm0/bTuEnI7MKtHd9Zq7hZ8UCiktFtOTxl0dboya6+AwhL5g+skH9W9c92+9UcGnMTW48jTVV6v2guz+ta2xr3S1VTNwYf0bihqhkSNxYQIy5SXLN68xibry13Ghp27JxApow4Ov0sBuvkcuMTapsuR8+9U7BMl5lgPFxDOVkuUkLzQq9gp/I+G3VJTQXz+VLJmLKiHrjlQjDXzhxmGvjk8FnC/Hnik9p9gI+IDdIy41ZzE08FkF9dS5no7M/rQv4txqPV4pAIbcUUcmQuLEAb7lJc9lS8VhEUsRPe+3lzSVuQdwk8gImKXFLGYkG9kTJ1vXoqztdGnFxdJabgAKKzWJuzOJg3IDfdFVce8GLG7vC8pITJpY8Lh7RUsG/5os/njBluKvbFeGPlb5Ugz/1kdjaZUUu1TpYsSgaanKuqe4BvbhxzXJj8pnX1ytBlDMkbiygs9xkNLdUIiZWKI7o3AJeWoXZEx0/oRSkgjMrjMQtZZSKqqWPGwfbeLVf6oQRYMyNbDJJFekH5BYRXUBxVPp30XUIr11vtyC8/tnf38Wh3iQAgP1MLjtxomfWPUYkElEnb1k5BO8bZ+b+l/1KMpzrt6EmZ7np6k/p3VKWaxo5j7kJMquOIIKGrn4L8L15cqng+ZtXPFogZvh7kZdWB3bfSussN/pb7Sfmjcm9r8uWMs9CUtPHM4ph6wVfYm58qjQrIroBcgUF/Ym54a8XPuaGH5Pdo+H1/Hb/v97HV3/3OgAtHscva5vW6FUWMO/11o1jbngxzMRNN+eWslPck3eTsnVZpcrj7D6CKGfo6rdAQRE/rv0Cfx+PCDE3Xt7i1VRwSZ2bL50yGetv+SCmjsgFXaaEbC/AeAJiE3gyk9WJOh6vHsq1VPDysdxkFYVL3/XRLcWX2Oc3W2QI4qTpelCpZDJ/ZmMHAO1a9CuQlf0GeIulFnPjj+XmcF8KD720XSjUpz1AsJibrv40566yfnz4uL0ff2Yu5o1vxt0XzNfGYfJds6B0ggg7VKHYAinh5sk/yZtNJl6a5qPqU2uh5aY6HkNzXUKLn5E82RoFNKoZVibixrMaJpzlxi/3gog4MWez+gByL9G7pfSB6pbXIbx224oiq3vELkHmIvVLj8YkvwHfqklzfy9/9A28uv0g7vjUHN14qqJazE3XQJor/mn9OuItN1NG1OPRL52g+9zs0qCYG6KSIWlvgWRaa5yZyWZ12TMFD9UR4bVHsJs3b7lhT4+1idxpVevccG6pVJFAXXbjTWcVDKT1DUOvOW0qPr1gLGaObnRjFwooh2wpcXsZxbyqs5vEDGJudNZBm1eVXz2WAO1a9MstZR5z433MD89fXt+t/s1baOrzrqT/emYzepMZ9X2r8Lth1jBUBsXcEJUMWW4soM/G0LKPqmKCG0qIufFS3bDt8k+t77R3AQAmt9bnxycr4lck5iauZVglBcvNVz44zY2hGyKNuQnYLcW7yLw28/MP9Hw1ZPEaM0P83O1aS2Y1Hdml6Jdbip0rWZFKPy03ADCkWruV8rE1LE7mYG8K1z68HoA90cE/vMibkVJAMUHIoKvfAnwq9b+27FN7xVTFogX3Fv9ibvKWm7wZPpnOYvOenLiZMSpnWRHTugHNimN04+PjGIzcUl5RjtlSWUXR+gF57CKLGlhuSrEGup4tZRBkDvBuKX9jbjKSuDP/KhTn4NP1tTo3UTRwomfLnm4A9kRHMsOLm0LLjdmh9rpdCEGUM3T1W4C3YDy6Tqv7kivip19W7BLuFWrMTf7m99NntyCr5DIqRjfVAODbL2g3yM7+FADjzIsqPuYm5bO40Vlu/Em/FhEtWpks1w8o7u2EGTWIubGzVXFZP7Vh1rdMpRxaBe4AeksJR7ouwVluuFo7snuAnbHxDyZ2LTEUc0NUMiRuLJDMyCf5qljhzctvyw17Wn5iQ87n/+kF49QxydxSzOrEAh1F1O9IYm68RitpH5zlRgz2zPKp/wFZbiK23FLeZkuZuaWYsPCy7QiPGlDMZwP6nC3FqK/mLDdcTN7eroGC79q5psXCnAXjMPmM3FJEJUNXvwVSRllDVTEh2FO03Hg3JnZjZ7EpB3pyFplPzhurLsM/2TJ3QmdfbrlGA8uN2lsqbZwt5RXsWCrge0v5HFAsPO1mddlx3o6FXz8f38NvdSTXnNIK7ruljD9jBhS/gpjNqnQH6c7kA4pPnt5asKwd0VFU3JhmS9Htnahc6Oq3gNEkX18dLzBP6y033t1gNXGTs3QczFeJHTZE6+nD39xSGQWZrIKufEZVY62B5YYrjOa3uGF3akUJLltKnBCyir6cvpfwQalinZt7LpyPC4+fgM8cO850HYVuKX+O3z8378P2Az2+bjPOZfYBwN827MY7Hd2+jEFcfV+q0HpUFY3ilGmtuPuCebpl7bilkmkzW5k55JYiKhnKlrJAz0Bhh+N7LswV0hJjbHTJUj5YbrKKgs6+tPr02lyniZaETtxk0ZfSbpTGMTdanI6YLeU1fEBxUNlS4oSw6u12bNvXC8D7bCmduInrRfLpM0fi9Jkjba/Tr+N3wS/XqH/75ZaKC6ngVz7wasFnXiG6//qS2j2CieFYPubmhCktumXtBKYXd0sZ76fXAfAEUc6QuLFAtyBuZo9tUieagtRbv4r4sVTwjIIDeavNkERMl1HB3+DTGUUNJq6ORw0L8fEZVr7H3HABxWmfAkNFROvMhp2d3GfejmUIl3FjWKG4GMKyQXSG9uuUyVLBxc+8Qlx7X0r7raip4PkDIYoMOxaV0txSZLkhKheS9hboSerFTT33hC2av/2qmcangh/oybuk6hPSZYBcUDQTN0YuKUCbVNMZxf9sKbB94nsE+XuJmsUp+OmWcmolMnOT+oVvlhtJzI34mVcUuKXyBfqyWQWvbDsAQLuWRKHlpuXGDL/OA0GUI57drd9//31cdtllmDhxImprazF58mTceuutSCaTuuVef/11LFmyBDU1NRg3bhzuuOMOr4bkmO5+vbipMgj2BITMFg/HFOViYw4ycVOnFzeRSIRLB8+is49lShkb7NikkAog5kY7dEFmSxlvL+G55cZ9Q2oQlpugYm54vLfciG6pnLh5YM02rXFmNKL7n2GnoWWxVidme+lndWqCKDc8c0tt3LgR2WwWP//5zzFlyhRs2LABl19+OXp6evCDH/wAANDZ2YnTTz8dS5cuxd1334033ngDl156KZqbm3HFFVd4NTTbiG4pvjiWWZqul0aHOBdQzIKJmwRxA+RM48lMzhJzuC+3XKNBGjig3XhTaf9TwdVsKa7lge8xNyYTj9cxDHVcOjEvLO24NwvdpLn3zLKc3Mavc6ZlA2oZUpoo9jcVvDeVgaIoeODFbep77CEoGo3ozoEdq9zyM2dgy55uXHLCEUWXjUa0jDW2XYKoVDwTN2eccQbOOOMM9fWkSZOwadMm/OxnP1PFzYMPPohkMol7770XiUQCM2fOxPr163HnnXeWlbjpGdBP8vzNSUwF5/EnW0qzsNRIJubcDTaDZCaLx99oBwBMHVFvuN4gs6XYJB5ktpTZ9uw8cTuhSggAZ5RyBKL5TvUZH9WN740z85aSRDyKdN6C4nUWtLiLipITpGOH1qkZW3yMVjwa0RrbStsoyBk3rA5PfuUk43FwKuvrHzoSHz1mNI67/WkAZLkhKhtfA4oPHz6MYcOGqa9Xr16Nk046CYmEZnFYtmwZvve97+HgwYMYOnRowToGBgYwMKAVxurs7CxYxi2ue3g93t3Xg/09+kJcuuqxJjcQf+rc8BV0jcRNbrL8x+a9AIDPHDfecL0sruQnz2zB0DpjC4+XKFDUJoN1HrhqzDCLuTFzWbmNTtzY2Ky4aCwaQTQC+GmDC6r9QiIeVa+bfq/jxSS72JfMYEyzVoeId5fFo1GkMrmxVXukvKKRCJprtXup38H4BFFO+BatuWXLFvzkJz/BF77wBfW99vZ2tLW16ZZjr9vb26XrWbFiBZqamtR/48aZ1/1wyv7uATy6bide23FI174AKI/iWOypLJPNcnU1Cm9mWjsFRY0d4m/Ahctr+3awN+XaeK3AZ0v15l2BQ6rtdUIuFVNx42OvnmTaobiJFL72MmtPhm9uKTU+LHf9Z7jfabPHwlxmle1NZdBYq4nx/d1afCFvEbRjubE1poj+2AcRTE4Q5YLtX9ny5ctz9VxM/m3cuFH3nZ07d+KMM87AOeecg8svv7ykAd900004fPiw+m/Hjh0lrc+Il98/aPiZ1UnOy0lFtdwoWu8oWTYPm6x7k2lVBNVKGvBpywd3Q1TbLwDoCchyY/a066eLzKjlh13SGcXTwHYZfombGKtzk8lCURR057MaV3ziaIwdWufptmU/7b5kGvxpW3rkiIKxAu7WS+LHkXNBaq+DCCYniHLB9sxx/fXX4+KLLzZdZtKkSerfu3btwqmnnorFixfjnnvu0S03cuRIdHR06N5jr0eOlBcsq66uRnV1td1h2+alrQcMPzO8OYlp4W4OSICPuUmbtAdg7x3u06wwtQljcRPkDZEdvqyioDcZjOXGbOLx02LHV6a18wQuWhSqq6K+P8H7ZSnS4sNybkwWVvSxY0Z7vm3ZHvYls2rxyc+fOBHNXIA/L4zd7NYttnvhjz1ZbohKxra4aW1tRWtrYb8UGTt37sSpp56K+fPn47777kNUyGBYtGgRbr75ZqRSKVRV5czIq1atwvTp06XxNn7ywaPa8OCabdKgWqvWDX9ibhTVLC/LEGETcmfeJRWPRkxvrhNbhgDIFSr87sePxpd/uw7v7etxdexGsMOVTGdVa1Q5WW78EDenH9WGlW914NITjsBTb+eEvtPLaFpbPUY01PhWe4nhVyBrjEsFZ1XEoxFzy6RbyAQcbx0Vf2P8Q0Ox9G63xkSWG6KS8exuvXPnTpxyyikYP348fvCDH2Dv3r1ob2/XxdKcd955SCQSuOyyy/Dmm2/i4Ycfxl133YXrrrvOq2FZZtHk4fjv8+dJPzMSB37eSnTihmvUJ6KKm7zlptiN/6NzRuPZr56CR7+4GLPGNGFS6xA3h20Ke9Lk213UmViZvMAshbjah5ibuy+Yj5duPg3HTxquvekwFfy8fOC430/wfhm4+CJ+vBvTD8uR1HKTyqjBzaILk7+uXLXccCMRx+R3piFBlBOePRavWrUKW7ZswZYtWzB27FjdZ6xDdVNTE1auXImrrroK8+fPR0tLC2655ZaySQM3Ckq0+gTv5aSiBRQrnFtKZrnRu6XMXFJA7umPWW8AeB67oN927n9WVygRj/oevJ2IG58zrwJBeaLRCEY01Ki/EcCeaBZ7ndn9vhv45ZaKc8HyTOC7KRzMkMfcZFTLjVhZW2+58S7mhofq3BCVjGfi5uKLLy4amwMAs2fPxvPPP+/VMEqiqbawKB5gXdz4EnPDBRTL3GVx1S2VEzd2LSHXnT4Nh3qT+NjcMaUM1xbd+bpCQ3y22gDFLDf+jcesOKT9dZU4GJv45ZaKc3FnLLvMr4B4abZUMqNmbIlWVK9ibnRjym9ielsD3t3bjfkTgnXtE0SQUONME4wsN1azHfyJuclyFVoLx8XGyiw3NTbjERprqvCjz8wtZaiWiQhuKb/jbQDzmBs/3FIy7F1GhaLo2COG4emNe3Q90bzEt2yp/PWe4lyzvln6ZJabFG+5MY5/cdctpcE28dcvn4h0VrH9WyeIMEHixoQmgwaTRk+HBTVGPLTdRLl4g7SJ5Ya9x/pK+R3DYgc2+u6AatwA5sI1KHHjFHY87/jUbPzi+a349IKxpsu7hW+NM2Pab4BZL91MszZDtot9yYyaLSXGu3gVUKzvZZf7Ox6LwkcjI0GUJSRuTDB6CrRczM3Dezzfh8mszk2hW6p8T7kYcxOM5Sa4ruBGOO4tlX8xvL4ay8+c4fKojPEr1IMJiHTGPO7MC2TnxMxyw4/La7cUQRA+Vige7PBZRkY3UNFS4+W9Ri14p2il+mXZEQkhW6qcTdV8KjgQjOUmyCKGRtgKKPZsFNbxO+Ymnc2qRQ+rTALC3US2ld6kcbaUZwHF/N+kbghChcSNRcYO1VoWWL05eXmvUS03UNSYG7NsKSZuytktJboz/HIx8JRDaw0Rp9dRUFOdX24pvs6NFlQfXLbUns5+w2wpzwKKddlS7q2WIAY75XcnLzNOnpYrWPj1Dx2pvteXtNaG0MuYG62aL0xv7HGhiF85ixvxaAVRhKwca4PYuY7EirVB4FvjTLX9gv8BxbJz8ui6ndi+vzc3NjPLjYeNMwmCyFG+ARhlwv9ctADth/sxbphW72VP14B0WVnTQu9gbikt3sCsiB+LY2k0CJIuC8T2FQHcrMvRcuNUI3sprs3wu4hfKpvV6twEaLkBgE0dXQAKhTn/23SzXpIkxIogCJDlpihVsahO2AC51gRW8MctBa0ruETcJLj3IhHgE/P8q1djF3H0QRhRylHc2Iu5CX6G88uCwFfp9rvODY/M2ic+aPBuqkTMo2wpUjcEoUKWGxu8sPwD2NTeqbqqRMRbi7duqdy6s7qAYmO3FAA011ZhxshGz8ZUKuK9OQgze1kGFNsYUjm4pfxyJzIhGnTMzYxRDYhHo1i/45D6XoHlJuqN5UY3Jk/WShCDExI3NhjTXIsxzbXFF8zjh+UGSrE6N9qNtL6mvE+3KAYDETdlWMvGqUgOLKDYZ8tNOqO5pfw6f+I5EX97ZjE3brrO9EX8SN4QBKP87uQhwkszsT6g2KxCsTaG+uoyjreB3zFLchp8quJrB1uWG4ffcxPfAoolzWODiLmJICKpSKwfB/9pg4sPGeVgqSOIcqT87uSDmLamGt+2pda5gYJUPnlL9tTKu6XKceLmEb0ZQTyJlmPcgvNU8KACin223GQVrc6Nb72luL8jhQ8WouWml8uw9CqovwwT/QgiMMp7thsk/PKiBXhzVydOEWJxPC3ixwKKFWh1biR3N94tFURRPDsUuqUCGkiZYS8VPPiD5td5Y9d2JqsglfY75oYL5IV5LylAy1YE3B2j/toI/twTRLlA4sYFTjuyDacd2Vbwvpc3eT6gOG3SfoF/kq2vKW+3lHhvphiCEgnKLeWz5SbFx9z4VudGj2ipEV/z4sbVcVARP4KQQjE3HuLlU7TWW0pBKlu8zg0A37pCO6Ug2ywgcXPOfH8aTFplsFUo9rv9QkbXFdwnt5Tglypquen3Rtzox0TqhiAYJG48xFO3FNdbSs2WkgQU8+LGzUBGLxBvzkE9iX77Y7PQ1lite+/zJ04MZjA20QeYBnMA/atQrKWCJwNsnBlB4YOF+LrHI8sND1luCEKDxM0gRY25gfbUKrPc8O8NKeOO4ICsiF8wd+vaRAwfO0YrdvjM9Sfr2m/4TTk/kcv6JEk0tifwXcH9dkvxRCKF2VHi6y4fxE0ZXyYE4TskbjzEj1RwpUhvqcQgqnMjTop+TZIy2GQJAJNa632LI5FhZ8t8gKnXI97wrWV47EuLC94PIluKBRS72pTSBsVibryCKhQThBwSNx7i5b2GTWJZrrdUsSJ+zeXcVwqFWUFB3qw/vWAcAOC4icMCGwOjnCoU11blMu6aaqtQXx2X1pXxLeaGNc7MZn2PueGJoNDKKAq8ixZNAAB89rhxrm9b9jdBVDrl/Sg/yPHyZsP3lkpljbOlOvtT6t9LprZ4OKLSKWy/EMw4AODIUY1Y+42laK5LBDeIPE4Pgxca4/dfXIT/t/Id3LBsOgB5ZpRfopTVlkln/I+54YlEIkUtNzefdRTOPHoU5o0f6tk4KLuQIDRI3HiIp5abiBZQbPbUunjycMSiEZw4pQUjGv0rMugGQd+sh9dXF1/IB+yIBf2TvPvHb+boJtx78bHqa9k5CsItlfa5t1TBWAoaZepfJ+JRHD9puOvbpQrFBCGHxI2HeNs4M/d/KpOFkruvo1rSbXjC8CFY+42lqCvzYGJAli1Fd2ugvN0NMhdUkKngfrVf4IlAFnPjfxsI+r0QhEb5z3iDGD8aZ/antMBXo2DKcnCtWKGwzk0gwyg7yinmRkQ2h0f8ypaKaUX8kiYZg16Ty5YSLDcBxf4QBJGDAoo9xNvJJbfygbTWsyaoTBG3KIy5odt1DhtuKZ+PmcwF5dcImHUkw7mlZHFnXhNB8ZgbL7et/k2/F4JQGdyzYZnjpVuK3TsH8pabWLSwSupgQxQzg3x3XMNxhWIfJjupWyqAmJtM3jfrl0uscCxinRu/UsHlfxNEpUPixgMuXnwEAODLp031bBts4koGGGvgNuVSxG8w4/cR47OlxjTX4o5PzvYtvqtKTQVXkM1nDAYi8CPB1bnhod8LQWhQzI0H3PbRmbjpQzNQHfeuC7d47xzsLimg8MmTzOw5HKeCuzoKObyl5DPHjsOnj3W3jovptrnGmekAxU1Esl3fLDf83/RzIQiVwT8jlileChug0OUVBnEjTsfklspha9LyO6CY24ji/eZ08DE3WeaWCuBnEJFabnwaCHf86fdCEBphmBErEnHiCoVbigKKpdiJ3dK3X/D++AXZImNIde4BIp1VcLgvV6xSjH3xCzE7KohKyZQvRRAag39GrFAKxE0ILDeFMTeBDKPsKGeNF2QQe0NNFVryhRa37e/NjSeAgyVmS8lSw73btgb9XghCY/DPiBVKgVsqFJabiOnrSqWs69wEfI4mtQzRvQ4k5iaitxhVxaK+Xbv6802/F4JgDP4ZsUIRre9hsNyI81LQE2e54NS95EtAMXfSFL+DbgBMLANxA+hjboJ60CDLDUFoDP4ZsUIJY0BxOTXOLCvsWG74v304fjFdQLH/6mbGqAbd66ACinlR5We8jd4tRT8YgmAM/hmxQgllQLGYLUXqBoA9C4z+uvD++AU9n4qd7oMIKBZjbvxs3kmuKIKQM/hnxAoljHVuxLmY7ts5ynkCC3psk1vr0VRbpb4OJKC4wHITlFuqfK8TgvCbEMyIlUoI3VLCa7pZ57BX5kaftRN2IpEIaqu0mlKBxdxwrig/f4tUxI8g5Az+GbFCCaPlRrQCkFcqh+PeUu4OoyhBBBQDemERlLjRZ0v5GHPDbYoeBghCY/DPiBWKKASqQxFzo4du1jmcxtz47TIKSNvo4l2CCSgOLuZGP45ANksQZcngnxErFPE+FgbLjShmgo7nGIxU4hGLc2IimIDi4GJueDckWToJQmPwz4gViigEwiBuKBVcjlOR5/vhC8gvpbPcBCSIy6HOTWVKW4KQM/hnxAolnKngesgtlcPWUeCOWaUcPj7mJojWUgXZUnEfD7wu5sa/zRJEuTP4Z8QKRZy4qkJguRFncbpZ53EaUFwhx4/vwO1bN26OiLBdf91S3N+VcsIJwgIhmBErE/FGFlQQo5uIRfzoZp3DXlfwyoN3CQXVpbw86twEslmCKEsG/4xYoYj3saBiDdykMOZm8O+TGzhunOmz1AkqW4oXFoFYbiIR3Rj8dBEHeb4JopwhcTNIESf+uI+1NbxC3Cd6Es3h+DD4fPyCqnPDXzaBVCgGUB0PqM5NhRVtJAirkLgZpIg3sqCKl7kJWW7k2LLc8JOdB2MpR3Tp0AEFFPMtIKjODUEED4mbQYp4I4uHQdyIrwf/LrmCU3dDpcQs8bsZhFsKAJrrqoov5AFUoZgg5JC4GaSIEx5ZbsJHXSLXM+nUGSMsf6cSD5lugg/kjhZBQ40mbnpTGf+2HJH/TRCVTjzoARDOCKPlRrTdBJX5Ui7842un4p32LiyaPNzydyIGf/uBElBIMS+Cgwko1j9c9A6kfR8DQA8DBMFT4dPH4EW8kQVRdt5tyHKjp6W+GountDivUOzT4ZvYMgQAcOasUf5s0IQgNL64yZ6kj5abCoyxIggrkOVmkCLeyMJhudFTKTEjbhJEavAT1yzBvu4BjB1a58v2RPjrpByumT4/xU2AjVIJopwZ/I/7FUqh5Wbw39gUIZc4BLtUEdRUxQITNkDwFgtRU/Qkg3FLkbYhCA0SN4OVEKaCi3VSKt0t5QS9FSPAgfhI0PspWsj8FHr8b4Z+LwShQeJmkBLGOjfZAnETzDjCQqUcvnKZ1B/90mKcNXsUVnziaN+2yQdxl8dRIIjygGJuBikFFYpDoASygumGYghKpEIOX9C7yS7TeeOHYt55Q33dNlluCEIOWW4GKQW9pUIobuhmbZ9KPGRB73PQ21cpl3EQRBlA4maQEsbeUoUxN8GMIyxUTiPFStnPQvSWm+DGQRDlBombQUphzM3gP5VkuSmdSmykWCn7KYP/zZAblyA0Bv+MSAAIS8yN/jXdq+2jr3NTGQR96ZeLhSzo40AQ5QSJm0FKZdS5Gfz7FCSV8iQfuLgIcPP8Lybw40AQZQSJm0FKGHtLUZ2b0qnEIxb0ZRLo5rnfTNDHgSDKCRI3g5QwWm4KY24CGkhIqJTJrlL2UwZf54YeBghCg8TNICWcqeD615XiVnGTSoy5CdodE+R1qpDlhiCkkLgZpISxQvHRY5p0r0OwS75TidlSQau4IDfPPw+Q5YYgNEjcDFLEp8V4CFLBRzbV4LeXH6++jpK6ISxQyVcJH4RfyceBIEQG/4xYwfD6JgyWGwCY3DpE/Tsku+Qres1bGQcwaItFkJvXZUtVxukmCEuQuBnE8Df1MGRLAWJX63DsU1BUyuELej+D3PyezgFtHEEfCIIoIzwVNx/96Ecxfvx41NTUYNSoUbjwwguxa9cu3TKvv/46lixZgpqaGowbNw533HGHl0MKFfytLCyWG343wrFHwVEpx+/cBeMAAHPGNhVZ0huCFBU7D/UFtm2CKGc8FTennnoqHnnkEWzatAl/+MMf8O677+JTn/qU+nlnZydOP/10TJgwAWvXrsX3v/993Hbbbbjnnnu8HFZo4O+pYegtBQTvYhjsVOLT++IpLXjuhlPwyJWLgh4KQRBlQtzLlX/lK19R/54wYQKWL1+Os88+G6lUClVVVXjwwQeRTCZx7733IpFIYObMmVi/fj3uvPNOXHHFFV4OLRTkJrKc1z00lpuQ7Ec5UElCZ8LwIcUX8oggj3JtVQx9qQzOmDkywFEQRPnhW8zNgQMH8OCDD2Lx4sWoqqoCAKxevRonnXQSEomEutyyZcuwadMmHDx4ULqegYEBdHZ26v4R4ciWAiiIuFQiBn8THhLggf7dlYtwyQlH4Hufmh3cIAiiDPF8RrzxxhsxZMgQDB8+HNu3b8ef/vQn9bP29na0tbXplmev29vbpetbsWIFmpqa1H/jxo3zbvBlDp8GGgvJUzq5pdyDDqU/BFlEcNaYJtz6kZloqq0KbAwEUY7YFjfLly9HJBIx/bdx40Z1+RtuuAHr1q3DypUrEYvF8LnPfa6gQaIdbrrpJhw+fFj9t2PHDsfrGuxkuJK+sRDG3Di/SioXfYXicFwT5c7o5pqgh0AQhIDtmJvrr78eF198sekykyZNUv9uaWlBS0sLpk2bhiOPPBLjxo3Diy++iEWLFmHkyJHo6OjQfZe9HjlS7kOurq5GdXW13WGHEr5dQVhSwUPiXQuMcFwFg4P7Lj4WT2zYjS+eMjnooRAEIWBb3LS2tqK1tdXRxrLZLIBc3AwALFq0CDfffLMaYAwAq1atwvTp0zF06FBH26hUQhNQTL4U16BD6S2nzhiBU2eMCHoYBEFI8Ow5ec2aNfiv//ovrF+/Htu2bcMzzzyDz372s5g8eTIWLcqlbJ533nlIJBK47LLL8Oabb+Lhhx/GXXfdheuuu86rYYUWirkhgMrKkCIIgjDCM3FTV1eHRx99FKeddhqmT5+Oyy67DLNnz8Zzzz2nupWampqwcuVKbN26FfPnz8f111+PW265hdLAHRCWFGp+N0oIzSJAlhuCICoXz+rcHH300XjmmWeKLjd79mw8//zzXg2DGGSQ5aE06PARBEFQbymCCBX6OjekdAiCqExI3BBESCErDkEQlQqJG6LsmDu+GSMaqjFrTGPQQxl86LqqBzgOgiCIAPG0txRBOOEPVy5GVlEQj5H2JgiCIOxD4oYoO6LRCKIUL+IIirkhCIIgtxRBhApd+wXSNgRBVCgkbggipJC2IQiiUiFxQxAhgndFkeWGIIhKhcQNQRAEQRChgsQNQYQIvbWGTDcEQVQmJG5CQFg6ghOlo8uWosuCIIgKhcRNCCBxQ8igq4IgiEqFxE0IiJO4IfKQtYYgCILETSiI0YxGSKAO6wRBVCokbkJALEaTGJFDlwoe4DgIgiCChMRNCCC3FKFCFYoJgiBI3IQBCigmZFBvKYIgKhUSNyEgHqXTSOQgOUMQBEHiJhSQtiFkkFuKIIhKhabFEECWG4JBGVIEQRAkbkIBxdwQDKpQTBAEQeImFFC2FEEQBEFokLgJAWS5IRgRXSo4XRcEQVQmJG5CAIkbQgZdFQRBVCokbkIAiRuCEaEifgRBECRuwgDF3BAMKtxHEARB4iYUkOWGkEFChyCISoXETQigOjcEg9xSBEEQJG5CAVluCBl0VRAEUamQuAkBFHNDSKHLgiCICoXETQiIkrgh8lBtG4IgCBI3oWBSy5Cgh0CUIRRQTBBEpRIPegCEc357+fH46xu78OXTpgY9FKJMoN5SBEEQJG4GNYsmD8eiycODHgZRppC2IQiiUiG3FEGECLLWEARBkLghiFChKNrfFFxMEESlQuKGIEIKSRuCICoVEjcEEVLIcEMQRKVC4oYgCIIgiFBB4oYgQgrVuSEIolIhcUMQIULhX5C2IQiiQiFxQxAhhWJuCIKoVEjcEERIIW1DEESlQuKGIAiCIIhQQeKGIEIKFfEjCKJSIXFDECFC4UoUk7QhCKJSIXFDECGFDDcEQVQqJG4IgiAIgggVJG4IIqRQET+CICoVEjcEEVLILUUQRKVC4oYgCIIgiFBB4oYgCIIgiFBB4oYgQgq5pQiCqFRI3BBESKGAYoIgKhUSNwQRUshyQxBEpULihiBCCmkbgiAqFRI3BEEQBEGEChI3BBEiuNZS1DiTIIiKhcQNQYQUkjYEQVQqJG4IIqSQ4YYgiEqFxA1BEARBEKGCxA1BhBSKuSEIolIhcUMQIUKBUnwhgiCIkEPihiAIgiCIUEHihiBCBLVcIAiCIHFDEKGC3FIEQRAkbgiCIAiCCBm+iJuBgQEcc8wxiEQiWL9+ve6z119/HUuWLEFNTQ3GjRuHO+64w48hEUQoUchwQxAE4Y+4+drXvobRo0cXvN/Z2YnTTz8dEyZMwNq1a/H9738ft912G+655x4/hkUQBEEQRAiJe72BJ554AitXrsQf/vAHPPHEE7rPHnzwQSSTSdx7771IJBKYOXMm1q9fjzvvvBNXXHGF10MjiNBBpW0IgiA8ttx0dHTg8ssvx69//WvU1dUVfL569WqcdNJJSCQS6nvLli3Dpk2bcPDgQek6BwYG0NnZqftHEEQOcksRBEF4KG4URcHFF1+MK6+8EgsWLJAu097ejra2Nt177HV7e7v0OytWrEBTU5P6b9y4ce4OnCAIgiCIQY1tcbN8+XJEIhHTfxs3bsRPfvITdHV14aabbnJ1wDfddBMOHz6s/tuxY4er6ycIgiAIYnBjO+bm+uuvx8UXX2y6zKRJk/DMM89g9erVqK6u1n22YMECnH/++fjVr36FkSNHoqOjQ/c5ez1y5EjpuqurqwvWSRAEQRAEwbAtblpbW9Ha2lp0uR//+Mf4j//4D/X1rl27sGzZMjz88MNYuHAhAGDRokW4+eabkUqlUFVVBQBYtWoVpk+fjqFDh9odGkEQBEEQhHfZUuPHj9e9rq+vBwBMnjwZY8eOBQCcd955+Na3voXLLrsMN954IzZs2IC77roLP/zhD70aFkGEGgooJgiC8CEV3IympiasXLkSV111FebPn4+WlhbccsstlAZOEARBEIRjfBM3RxxxBBTJY+Xs2bPx/PPP+zUMgiAIgiBCDvWWIgiCIAgiVJC4IQiCIAgiVJC4IYgQQfHEBEEQJG4IgiAIgggZJG4IgiAIgggVJG4IgiAIgggVJG4IgiAIgggVJG4IgiAIgggVJG4IgiAIgggVJG4IIkTIqoATBEFUGiRuCIIgCIIIFSRuCIIgCIIIFSRuCIIgCIIIFSRuCIIgCIIIFSRuCCJEUDgxQRAEiRuCIAiCIEIGiRuCIAiCIEIFiRuCIAiCIEIFiRuCCBGTW4cEPQSCIIjAiQc9AIIg3GP+hGH40bnH4IgWEjkEQVQuJG4IImScPXdM0EMgCIIIFHJLEQRBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKgZ9V3BFUQAAnZ2dAY+EIAiCIAirsHmbzeNuMujFTVdXFwBg3LhxAY+EIAiCIAi7dHV1oampydV1RhQvJJOPZLNZ7Nq1Cw0NDYhEIq6uu7OzE+PGjcOOHTvQ2Njo6rrLDdrXcEL7Gk5oX8NJJe7rW2+9henTpyMadTdKZtBbbqLRKMaOHevpNhobG0N/oTFoX8MJ7Ws4oX0NJ5W0r2PGjHFd2AAUUEwQBEEQRMggcUMQBEEQRKggcWNCdXU1br31VlRXVwc9FM+hfQ0ntK/hhPY1nNC+usegDygmCIIgCILgIcsNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsQNQRAEQRChgsSNAT/96U9xxBFHoKamBgsXLsRLL70U9JBs849//AMf+chHMHr0aEQiEfzxj3/Ufa4oCm655RaMGjUKtbW1WLp0KTZv3qxb5sCBAzj//PPR2NiI5uZmXHbZZeju7vZxL6yxYsUKHHvssWhoaMCIESNw9tlnY9OmTbpl+vv7cdVVV2H48OGor6/HJz/5SXR0dOiW2b59O8466yzU1dVhxIgRuOGGG5BOp/3claL87Gc/w+zZs9VCX4sWLcITTzyhfh6W/RT5z//8T0QiEVx77bXqe2Ha19tuuw2RSET3b8aMGernYdpXANi5cycuuOACDB8+HLW1tTj66KPxyiuvqJ+H5f50xBFHFJzXSCSCq666CkC4zmsmk8E3v/lNTJw4EbW1tZg8eTK+853v6HpH+XZeFaKAhx56SEkkEsq9996rvPnmm8rll1+uNDc3Kx0dHUEPzRaPP/64cvPNNyuPPvqoAkB57LHHdJ//53/+p9LU1KT88Y9/VF577TXlox/9qDJx4kSlr69PXeaMM85Q5syZo7z44ovK888/r0yZMkX57Gc/6/OeFGfZsmXKfffdp2zYsEFZv3698qEPfUgZP3680t3drS5z5ZVXKuPGjVOefvpp5ZVXXlGOP/54ZfHixern6XRamTVrlrJ06VJl3bp1yuOPP660tLQoN910UxC7ZMif//xn5a9//avyzjvvKJs2bVK+/vWvK1VVVcqGDRsURQnPfvK89NJLyhFHHKHMnj1bueaaa9T3w7Svt956qzJz5kxl9+7d6r+9e/eqn4dpXw8cOKBMmDBBufjii5U1a9Yo7733nvLkk08qW7ZsUZcJy/1pz549unO6atUqBYDy7LPPKooSrvN6++23K8OHD1f+8pe/KFu3blV+97vfKfX19cpdd92lLuPXeSVxI+G4445TrrrqKvV1JpNRRo8eraxYsSLAUZWGKG6y2awycuRI5fvf/7763qFDh5Tq6mrlt7/9raIoivLWW28pAJSXX35ZXeaJJ55QIpGIsnPnTt/G7oQ9e/YoAJTnnntOUZTcvlVVVSm/+93v1GXefvttBYCyevVqRVFyYjAajSrt7e3qMj/72c+UxsZGZWBgwN8dsMnQoUOV//mf/wnlfnZ1dSlTp05VVq1apZx88smquAnbvt56663KnDlzpJ+FbV9vvPFG5cQTTzT8PMz3p2uuuUaZPHmyks1mQ3dezzrrLOXSSy/VvfeJT3xCOf/88xVF8fe8kltKIJlMYu3atVi6dKn6XjQaxdKlS7F69eoAR+YuW7duRXt7u24/m5qasHDhQnU/V69ejebmZixYsEBdZunSpYhGo1izZo3vY7bD4cOHAQDDhg0DAKxduxapVEq3vzNmzMD48eN1+3v00Uejra1NXWbZsmXo7OzEm2++6ePorZPJZPDQQw+hp6cHixYtCuV+XnXVVTjrrLN0+wSE85xu3rwZo0ePxqRJk3D++edj+/btAMK3r3/+85+xYMECnHPOORgxYgTmzp2LX/ziF+rnYb0/JZNJPPDAA7j00ksRiURCd14XL16Mp59+Gu+88w4A4LXXXsM///lPnHnmmQD8Pa+DvnGm2+zbtw+ZTEZ3IQFAW1sbNm7cGNCo3Ke9vR0ApPvJPmtvb8eIESN0n8fjcQwbNkxdphzJZrO49tprccIJJ2DWrFkAcvuSSCTQ3NysW1bcX9nxYJ+VE2+88QYWLVqE/v5+1NfX47HHHsNRRx2F9evXh2o/H3roIbz66qt4+eWXCz4L2zlduHAh7r//fkyfPh27d+/Gt771LSxZsgQbNmwI3b6+9957+NnPfobrrrsOX//61/Hyyy/jy1/+MhKJBC666KLQ3p/++Mc/4tChQ7j44osBhO8aXr58OTo7OzFjxgzEYjFkMhncfvvtOP/88wH4O++QuCFCx1VXXYUNGzbgn//8Z9BD8Yzp06dj/fr1OHz4MH7/+9/joosuwnPPPRf0sFxlx44duOaaa7Bq1SrU1NQEPRzPYU+3ADB79mwsXLgQEyZMwCOPPILa2toAR+Y+2WwWCxYswHe/+10AwNy5c7FhwwbcfffduOiiiwIenXf88pe/xJlnnonRo0cHPRRPeOSRR/Dggw/iN7/5DWbOnIn169fj2muvxejRo30/r+SWEmhpaUEsFiuIVu/o6MDIkSMDGpX7sH0x28+RI0diz549us/T6TQOHDhQtsfi6quvxl/+8hc8++yzGDt2rPr+yJEjkUwmcejQId3y4v7Kjgf7rJxIJBKYMmUK5s+fjxUrVmDOnDm46667QrWfa9euxZ49ezBv3jzE43HE43E899xz+PGPf4x4PI62trbQ7KuM5uZmTJs2DVu2bAnVeQWAUaNG4aijjtK9d+SRR6puuDDen7Zt24annnoKn//859X3wnZeb7jhBixfvhyf+cxncPTRR+PCCy/EV77yFaxYsQKAv+eVxI1AIpHA/Pnz8fTTT6vvZbNZPP3001i0aFGAI3OXiRMnYuTIkbr97OzsxJo1a9T9XLRoEQ4dOoS1a9eqyzzzzDPIZrNYuHCh72M2Q1EUXH311XjsscfwzDPPYOLEibrP58+fj6qqKt3+btq0Cdu3b9ft7xtvvKH7Ya1atQqNjY0FN+JyI5vNYmBgIFT7edppp+GNN97A+vXr1X8LFizA+eefr/4dln2V0d3djXfffRejRo0K1XkFgBNOOKGgVMM777yDCRMmAAjf/QkA7rvvPowYMQJnnXWW+l7Yzmtvby+iUb2siMViyGazAHw+ryUERoeWhx56SKmurlbuv/9+5a233lKuuOIKpbm5WRetPhjo6upS1q1bp6xbt04BoNx5553KunXrlG3btimKkkvJa25uVv70pz8pr7/+uvKxj31MmpI3d+5cZc2aNco///lPZerUqWWXaqkoivLFL35RaWpqUv7+97/r0i57e3vVZa688kpl/PjxyjPPPKO88soryqJFi5RFixapn7OUy9NPP11Zv3698re//U1pbW0tu5TL5cuXK88995yydetW5fXXX1eWL1+uRCIRZeXKlYqihGc/ZfDZUooSrn29/vrrlb///e/K1q1blRdeeEFZunSp0tLSouzZs0dRlHDt60svvaTE43Hl9ttvVzZv3qw8+OCDSl1dnfLAAw+oy4Tp/pTJZJTx48crN954Y8FnYTqvF110kTJmzBg1FfzRRx9VWlpalK997WvqMn6dVxI3BvzkJz9Rxo8fryQSCeW4445TXnzxxaCHZJtnn31WAVDw76KLLlIUJZeW981vflNpa2tTqqurldNOO03ZtGmTbh379+9XPvvZzyr19fVKY2OjcskllyhdXV0B7I05sv0EoNx3333qMn19fcqXvvQlZejQoUpdXZ3y8Y9/XNm9e7duPe+//75y5plnKrW1tUpLS4ty/fXXK6lUyue9MefSSy9VJkyYoCQSCaW1tVU57bTTVGGjKOHZTxmiuAnTvp577rnKqFGjlEQioYwZM0Y599xzdXVfwrSviqIo//d//6fMmjVLqa6uVmbMmKHcc889us/DdH968sknFQAF41eUcJ3Xzs5O5ZprrlHGjx+v1NTUKJMmTVJuvvlmXcq6X+c1oihc6UCCIAiCIIhBDsXcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKkjcEARBEAQRKv5/wK/L9g1JQ1kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m log_likelihood \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(token_ids[\u001b[39m0\u001b[39m])):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     log_likelihood \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log_probs[\u001b[39m0\u001b[39;49m, i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, token_ids[\u001b[39m0\u001b[39;49m, i]]\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Tokenize the input string\n",
    "token_ids = encode(input_string)\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "#token_ids = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "# Get logits from the model\n",
    "logits, _ = model(token_ids)\n",
    "\n",
    "# Convert logits to log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# # Compute log likelihood of the input string\n",
    "log_likelihood = 0.0\n",
    "for i in range(1, len(token_ids[0])):\n",
    "    log_likelihood += log_probs[0, i-1, token_ids[0, i]].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_log_likelihood(model, input_string):\n",
    "    # Tokenize the input string\n",
    "    start_ids = encode(input_string)\n",
    "    #token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "    token_ids = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "    \n",
    "    # Get the model's logits (predictions in log space)\n",
    "    logits, _ = model(token_ids)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Convert the tensor to a simple list\n",
    "    log_probs_list = log_probs.squeeze().tolist()\n",
    "    \n",
    "    # Compute the log likelihood of the input string\n",
    "    #log_likelihood = sum([log_probs_list[i][token_id] for i, token_id in enumerate(token_ids[0][1:])])\n",
    "    \n",
    "    #return log_likelihood\n",
    "    return log_probs_list\n",
    "\n",
    "# Example usage:\n",
    "log_likelihood = compute_log_likelihood(model, input_string)\n",
    "# print(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.950631141662598,\n",
       " -7.802065372467041,\n",
       " -13.482702255249023,\n",
       " -8.664899826049805,\n",
       " -8.303033828735352,\n",
       " -8.459487915039062,\n",
       " -8.544755935668945,\n",
       " -8.036310195922852,\n",
       " -8.63289737701416,\n",
       " -7.842120170593262,\n",
       " -13.253167152404785,\n",
       " -8.583927154541016,\n",
       " -9.431646347045898,\n",
       " -9.803473472595215,\n",
       " -9.308578491210938,\n",
       " -6.665337085723877,\n",
       " -8.988578796386719,\n",
       " -8.000788688659668,\n",
       " -10.165093421936035,\n",
       " -8.545632362365723,\n",
       " -9.102828979492188,\n",
       " -7.873270034790039,\n",
       " -10.757134437561035,\n",
       " -5.889583110809326,\n",
       " -7.671297073364258,\n",
       " -11.372381210327148,\n",
       " -6.976910591125488,\n",
       " -7.4980788230896,\n",
       " -7.716301918029785,\n",
       " -8.631196022033691,\n",
       " -11.004348754882812,\n",
       " -5.41005802154541,\n",
       " -10.959341049194336,\n",
       " -10.98293399810791,\n",
       " -7.680006504058838,\n",
       " -8.742183685302734,\n",
       " -11.81546688079834,\n",
       " -8.332889556884766,\n",
       " -8.378704071044922,\n",
       " -8.227534294128418,\n",
       " -0.15913772583007812,\n",
       " -8.025323867797852,\n",
       " -8.808104515075684,\n",
       " -9.910619735717773,\n",
       " -10.528595924377441,\n",
       " -10.060989379882812,\n",
       " -11.483312606811523,\n",
       " -8.316913604736328,\n",
       " -10.217599868774414,\n",
       " -7.987351417541504,\n",
       " -8.242897033691406,\n",
       " -9.658035278320312,\n",
       " -9.755733489990234,\n",
       " -9.022960662841797,\n",
       " -6.266380786895752,\n",
       " -10.658675193786621,\n",
       " -7.667608261108398,\n",
       " -9.16586685180664,\n",
       " -7.564515590667725,\n",
       " -8.931997299194336,\n",
       " -11.418488502502441,\n",
       " -6.427118301391602,\n",
       " -7.902199745178223,\n",
       " -9.170424461364746,\n",
       " -7.033017158508301,\n",
       " -11.820178985595703,\n",
       " -8.379581451416016,\n",
       " -9.338068008422852,\n",
       " -9.419045448303223,\n",
       " -5.3011579513549805,\n",
       " -7.680788040161133,\n",
       " -6.180191993713379,\n",
       " -7.5618414878845215,\n",
       " -8.13213062286377,\n",
       " -4.751554489135742,\n",
       " -8.532227516174316,\n",
       " -3.4638442993164062,\n",
       " -10.518830299377441,\n",
       " -8.767194747924805,\n",
       " -7.440390110015869,\n",
       " -10.94239616394043,\n",
       " -7.361095905303955,\n",
       " -7.775007247924805,\n",
       " -6.6193671226501465,\n",
       " -7.656801223754883,\n",
       " -6.481744766235352,\n",
       " -7.6984148025512695,\n",
       " -10.487643241882324,\n",
       " -6.422869682312012,\n",
       " -8.589497566223145,\n",
       " -8.803001403808594,\n",
       " -5.7674126625061035,\n",
       " -11.756107330322266,\n",
       " -9.224717140197754,\n",
       " -6.646552085876465,\n",
       " -8.821013450622559,\n",
       " -7.993476867675781,\n",
       " -9.321496963500977,\n",
       " -9.062085151672363,\n",
       " -5.764608383178711,\n",
       " -7.909576416015625,\n",
       " -7.212730884552002,\n",
       " -9.369081497192383,\n",
       " -10.498621940612793,\n",
       " -6.365996360778809,\n",
       " -8.668899536132812,\n",
       " -10.80052375793457,\n",
       " -7.366962432861328,\n",
       " -10.872920989990234,\n",
       " -9.644855499267578,\n",
       " -8.521404266357422,\n",
       " -9.799703598022461,\n",
       " -5.558866500854492,\n",
       " -5.132552146911621,\n",
       " -8.177952766418457,\n",
       " -10.477681159973145,\n",
       " -7.751117706298828,\n",
       " -9.92990493774414,\n",
       " -8.425687789916992,\n",
       " -8.837491989135742,\n",
       " -7.898916244506836,\n",
       " -6.543567657470703,\n",
       " -9.20556354522705,\n",
       " -5.296974182128906,\n",
       " -7.145254135131836,\n",
       " -8.526975631713867,\n",
       " -10.62103271484375,\n",
       " -9.84132194519043,\n",
       " -8.553725242614746,\n",
       " -8.044427871704102,\n",
       " -5.538974761962891,\n",
       " -10.11452865600586,\n",
       " -6.7956013679504395,\n",
       " -6.302101135253906,\n",
       " -7.75233268737793,\n",
       " -7.981353759765625,\n",
       " -10.290728569030762,\n",
       " -8.927913665771484,\n",
       " -10.584684371948242,\n",
       " -10.064664840698242,\n",
       " -11.330522537231445,\n",
       " -7.981705665588379,\n",
       " -8.67103385925293,\n",
       " -5.693831443786621,\n",
       " -7.559401035308838,\n",
       " -5.937882900238037,\n",
       " -13.435747146606445,\n",
       " -6.618038654327393,\n",
       " -8.750913619995117,\n",
       " -7.694700241088867,\n",
       " -5.092756271362305,\n",
       " -5.482172966003418,\n",
       " -6.30391788482666,\n",
       " -7.282887935638428,\n",
       " -12.149431228637695,\n",
       " -7.126071929931641,\n",
       " -10.010489463806152,\n",
       " -7.281721115112305,\n",
       " -13.187739372253418,\n",
       " -11.936113357543945,\n",
       " -8.24740219116211,\n",
       " -9.22597885131836,\n",
       " -7.936367034912109]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[-5.950631141662598,\n",
       "    -7.802065372467041,\n",
       "    -13.482702255249023,\n",
       "    -8.664899826049805,\n",
       "    -8.303033828735352,\n",
       "    -8.459487915039062,\n",
       "    -8.544755935668945,\n",
       "    -8.036310195922852,\n",
       "    -8.63289737701416,\n",
       "    -7.842120170593262,\n",
       "    -13.253167152404785,\n",
       "    -8.583927154541016,\n",
       "    -9.431646347045898,\n",
       "    -9.803473472595215,\n",
       "    -9.308578491210938,\n",
       "    -6.665337085723877,\n",
       "    -8.988578796386719,\n",
       "    -8.000788688659668,\n",
       "    -10.165093421936035,\n",
       "    -8.545632362365723,\n",
       "    -9.102828979492188,\n",
       "    -7.873270034790039,\n",
       "    -10.757134437561035,\n",
       "    -5.889583110809326,\n",
       "    -7.671297073364258,\n",
       "    -11.372381210327148,\n",
       "    -6.976910591125488,\n",
       "    -7.4980788230896,\n",
       "    -7.716301918029785,\n",
       "    -8.631196022033691,\n",
       "    -11.004348754882812,\n",
       "    -5.41005802154541,\n",
       "    -10.959341049194336,\n",
       "    -10.98293399810791,\n",
       "    -7.680006504058838,\n",
       "    -8.742183685302734,\n",
       "    -11.81546688079834,\n",
       "    -8.332889556884766,\n",
       "    -8.378704071044922,\n",
       "    -8.227534294128418,\n",
       "    -0.15913772583007812,\n",
       "    -8.025323867797852,\n",
       "    -8.808104515075684,\n",
       "    -9.910619735717773,\n",
       "    -10.528595924377441,\n",
       "    -10.060989379882812,\n",
       "    -11.483312606811523,\n",
       "    -8.316913604736328,\n",
       "    -10.217599868774414,\n",
       "    -7.987351417541504,\n",
       "    -8.242897033691406,\n",
       "    -9.658035278320312,\n",
       "    -9.755733489990234,\n",
       "    -9.022960662841797,\n",
       "    -6.266380786895752,\n",
       "    -10.658675193786621,\n",
       "    -7.667608261108398,\n",
       "    -9.16586685180664,\n",
       "    -7.564515590667725,\n",
       "    -8.931997299194336,\n",
       "    -11.418488502502441,\n",
       "    -6.427118301391602,\n",
       "    -7.902199745178223,\n",
       "    -9.170424461364746,\n",
       "    -7.033017158508301,\n",
       "    -11.820178985595703,\n",
       "    -8.379581451416016,\n",
       "    -9.338068008422852,\n",
       "    -9.419045448303223,\n",
       "    -5.3011579513549805,\n",
       "    -7.680788040161133,\n",
       "    -6.180191993713379,\n",
       "    -7.5618414878845215,\n",
       "    -8.13213062286377,\n",
       "    -4.751554489135742,\n",
       "    -8.532227516174316,\n",
       "    -3.4638442993164062,\n",
       "    -10.518830299377441,\n",
       "    -8.767194747924805,\n",
       "    -7.440390110015869,\n",
       "    -10.94239616394043,\n",
       "    -7.361095905303955,\n",
       "    -7.775007247924805,\n",
       "    -6.6193671226501465,\n",
       "    -7.656801223754883,\n",
       "    -6.481744766235352,\n",
       "    -7.6984148025512695,\n",
       "    -10.487643241882324,\n",
       "    -6.422869682312012,\n",
       "    -8.589497566223145,\n",
       "    -8.803001403808594,\n",
       "    -5.7674126625061035,\n",
       "    -11.756107330322266,\n",
       "    -9.224717140197754,\n",
       "    -6.646552085876465,\n",
       "    -8.821013450622559,\n",
       "    -7.993476867675781,\n",
       "    -9.321496963500977,\n",
       "    -9.062085151672363,\n",
       "    -5.764608383178711,\n",
       "    -7.909576416015625,\n",
       "    -7.212730884552002,\n",
       "    -9.369081497192383,\n",
       "    -10.498621940612793,\n",
       "    -6.365996360778809,\n",
       "    -8.668899536132812,\n",
       "    -10.80052375793457,\n",
       "    -7.366962432861328,\n",
       "    -10.872920989990234,\n",
       "    -9.644855499267578,\n",
       "    -8.521404266357422,\n",
       "    -9.799703598022461,\n",
       "    -5.558866500854492,\n",
       "    -5.132552146911621,\n",
       "    -8.177952766418457,\n",
       "    -10.477681159973145,\n",
       "    -7.751117706298828,\n",
       "    -9.92990493774414,\n",
       "    -8.425687789916992,\n",
       "    -8.837491989135742,\n",
       "    -7.898916244506836,\n",
       "    -6.543567657470703,\n",
       "    -9.20556354522705,\n",
       "    -5.296974182128906,\n",
       "    -7.145254135131836,\n",
       "    -8.526975631713867,\n",
       "    -10.62103271484375,\n",
       "    -9.84132194519043,\n",
       "    -8.553725242614746,\n",
       "    -8.044427871704102,\n",
       "    -5.538974761962891,\n",
       "    -10.11452865600586,\n",
       "    -6.7956013679504395,\n",
       "    -6.302101135253906,\n",
       "    -7.75233268737793,\n",
       "    -7.981353759765625,\n",
       "    -10.290728569030762,\n",
       "    -8.927913665771484,\n",
       "    -10.584684371948242,\n",
       "    -10.064664840698242,\n",
       "    -11.330522537231445,\n",
       "    -7.981705665588379,\n",
       "    -8.67103385925293,\n",
       "    -5.693831443786621,\n",
       "    -7.559401035308838,\n",
       "    -5.937882900238037,\n",
       "    -13.435747146606445,\n",
       "    -6.618038654327393,\n",
       "    -8.750913619995117,\n",
       "    -7.694700241088867,\n",
       "    -5.092756271362305,\n",
       "    -5.482172966003418,\n",
       "    -6.30391788482666,\n",
       "    -7.282887935638428,\n",
       "    -12.149431228637695,\n",
       "    -7.126071929931641,\n",
       "    -10.010489463806152,\n",
       "    -7.281721115112305,\n",
       "    -13.187739372253418,\n",
       "    -11.936113357543945,\n",
       "    -8.24740219116211,\n",
       "    -9.22597885131836,\n",
       "    -7.936367034912109]]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.unsqueeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(model, input_string):\n",
    "    # Tokenize the input string\n",
    "    token_ids = encode(input_string)\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Get logits from the model\n",
    "    logits, _ = model(token_ids)\n",
    "\n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Compute log likelihood of the input string\n",
    "    log_likelihood = 0.0\n",
    "    for i in range(1, len(token_ids[0])):\n",
    "        log_likelihood += log_probs[0, i-1, token_ids[0, i]].item()\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a tokenizer for your GPT model\n",
    "# log_likelihood = compute_log_likelihood(gpt_model, tokenizer, \"Your input string here\")\n",
    "# print(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m log_likelihood \u001b[39m=\u001b[39m compute_log_likelihood(model, input_string)\n",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X63sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m log_likelihood \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(token_ids[\u001b[39m0\u001b[39m])):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X63sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     log_likelihood \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log_probs[\u001b[39m0\u001b[39;49m, i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, token_ids[\u001b[39m0\u001b[39;49m, i]]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X63sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m log_likelihood\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "log_likelihood = compute_log_likelihood(model, input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len('-0123456789:ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyzÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāĂăĄąĆćĈĉĊċČčĎďđĒēĔĕĖėĘęĚěĜĝĞğĠġĢ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ 1.9307,  1.1828, -4.9455, -6.7650, -2.0055, -3.0020, -3.2038,\n",
    "          -2.1942, -3.4653, -5.3109, -1.3434, -6.4464, -4.3917, -1.7048,\n",
    "          -3.9408, -2.5360, -0.2636, -3.2607, -1.6355, -4.1500, -3.2615,\n",
    "          -1.8201, -3.7380, -5.2553, -2.0990, -2.5190, -4.0043, -1.3846,\n",
    "          -1.0009, -0.6828, -3.2038, -4.8597,  0.6266, -3.6161, -3.7216,\n",
    "          -3.5843, -2.2755, -3.9846, -1.2852, -5.2079, -2.0455, -1.4619,\n",
    "           7.0367, -1.5216, -2.4856, -1.1797, -4.4474, -2.2430, -2.2546,\n",
    "          -1.4891, -3.5570, -1.2047, -0.2842, -1.8798, -1.3267, -0.4516,\n",
    "          -0.3417, -1.5330, -0.9727, -1.6755, -1.9910, -0.4008, -3.8658,\n",
    "          -0.5314, -4.6410, -4.1227, -3.5439, -2.0581,  0.0547, -2.5001,\n",
    "          -2.9420,  1.0225, -3.5922, -1.3187, -2.2193, -2.8824, -0.0386,\n",
    "          -1.0355,  2.4982, -2.7982, -3.4882, -2.1288, -4.7673, -3.3740,\n",
    "          -4.7016, -1.5258, -2.0962, -0.2289, -2.7652, -3.6555, -1.3713,\n",
    "          -2.5181, -1.3434, -0.2759, -3.8927, -2.8960, -0.3261, -2.5964,\n",
    "          -3.0790, -1.6057, -1.8425,  0.1460, -0.9039, -1.5485, -2.4929,\n",
    "          -4.3711, -2.3016, -3.5627, -1.1182, -1.6007, -5.1231, -3.0806,\n",
    "          -3.0576, -3.7470, -0.2158,  0.8879, -3.2896, -3.9851, -1.7416,\n",
    "          -4.9267, -3.5209, -3.3572, -3.2785, -2.6327, -2.5645,  0.6000,\n",
    "          -2.1178, -2.6745, -3.9984, -2.7897, -5.0074, -2.7254, -2.7487,\n",
    "          -0.3466, -4.4992, -0.7854, -2.5838, -2.0815, -2.9000, -2.7879,\n",
    "          -2.9369, -3.7099, -3.8852, -4.8258, -2.3163, -0.2856, -4.1565,\n",
    "          -2.2483, -6.9151, -0.5414, -1.7882, -3.6384,  0.8883, -2.1491,\n",
    "          -1.3721, -2.1947, -5.2771, -1.9821, -2.4926, -1.6949, -6.6289,\n",
    "          -6.0050, -3.1536, -3.2256, -2.5102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.7558e-03, 2.7246e-03, 5.9401e-06, 9.6299e-07, 1.1237e-04,\n",
       "          4.1483e-05, 3.3901e-05, 9.3039e-05, 2.6101e-05, 4.1220e-06,\n",
       "          2.1785e-04, 1.3243e-06, 1.0335e-05, 1.5179e-04, 1.6223e-05,\n",
       "          6.6106e-05, 6.4140e-04, 3.2027e-05, 1.6268e-04, 1.3161e-05,\n",
       "          3.2000e-05, 1.3526e-04, 1.9872e-05, 4.3579e-06, 1.0234e-04,\n",
       "          6.7239e-05, 1.5225e-05, 2.0907e-04, 3.0684e-04, 4.2175e-04,\n",
       "          3.3900e-05, 6.4727e-06, 1.5622e-03, 2.2447e-05, 2.0200e-05,\n",
       "          2.3172e-05, 8.5773e-05, 1.5528e-05, 2.3091e-04, 4.5693e-06,\n",
       "          1.0796e-04, 1.9351e-04, 9.4975e-01, 1.8230e-04, 6.9526e-05,\n",
       "          2.5662e-04, 9.7757e-06, 8.8614e-05, 8.7593e-05, 1.8832e-04,\n",
       "          2.3814e-05, 2.5027e-04, 6.2835e-04, 1.2741e-04, 2.2153e-04,\n",
       "          5.3147e-04, 5.9324e-04, 1.8024e-04, 3.1563e-04, 1.5630e-04,\n",
       "          1.1401e-04, 5.5915e-04, 1.7487e-05, 4.9069e-04, 8.0545e-06,\n",
       "          1.3525e-05, 2.4127e-05, 1.0661e-04, 8.8177e-04, 6.8520e-05,\n",
       "          4.4045e-05, 2.3210e-03, 2.2989e-05, 2.2331e-04, 9.0739e-05,\n",
       "          4.6754e-05, 8.0328e-04, 2.9641e-04, 1.0152e-02, 5.0861e-05,\n",
       "          2.5511e-05, 9.9330e-05, 7.0991e-06, 2.8595e-05, 7.5813e-06,\n",
       "          1.8154e-04, 1.0262e-04, 6.6406e-04, 5.2568e-05, 2.1580e-05,\n",
       "          2.1186e-04, 6.7297e-05, 2.1786e-04, 6.3354e-04, 1.7023e-05,\n",
       "          4.6120e-05, 6.0257e-04, 6.2232e-05, 3.8408e-05, 1.6759e-04,\n",
       "          1.3226e-04, 9.6608e-04, 3.3811e-04, 1.7746e-04, 6.9015e-05,\n",
       "          1.0550e-05, 8.3565e-05, 2.3679e-05, 2.7290e-04, 1.6844e-04,\n",
       "          4.9734e-06, 3.8348e-05, 3.9238e-05, 1.9694e-05, 6.7284e-04,\n",
       "          2.0288e-03, 3.1115e-05, 1.5520e-05, 1.4631e-04, 6.0529e-06,\n",
       "          2.4689e-05, 2.9079e-05, 3.1462e-05, 6.0015e-05, 6.4251e-05,\n",
       "          1.5213e-03, 1.0043e-04, 5.7554e-05, 1.5316e-05, 5.1294e-05,\n",
       "          5.5838e-06, 5.4700e-05, 5.3440e-05, 5.9033e-04, 9.2820e-06,\n",
       "          3.8064e-04, 6.3018e-05, 1.0414e-04, 4.5936e-05, 5.1385e-05,\n",
       "          4.4272e-05, 2.0438e-05, 1.7151e-05, 6.6959e-06, 8.2349e-05,\n",
       "          6.2746e-04, 1.3075e-05, 8.8140e-05, 8.2873e-07, 4.8585e-04,\n",
       "          1.3963e-04, 2.1952e-05, 2.0294e-03, 9.7335e-05, 2.1169e-04,\n",
       "          9.2992e-05, 4.2636e-06, 1.1503e-04, 6.9039e-05, 1.5329e-04,\n",
       "          1.1034e-06, 2.0591e-06, 3.5647e-05, 3.3171e-05, 6.7831e-05]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9307,  1.1828, -4.9455, -6.7650, -2.0055, -3.0020, -3.2038,\n",
       "          -2.1942, -3.4653, -5.3109, -1.3434, -6.4464, -4.3917, -1.7048,\n",
       "          -3.9408, -2.5360, -0.2636, -3.2607, -1.6355, -4.1500, -3.2615,\n",
       "          -1.8201, -3.7380, -5.2553, -2.0990, -2.5190, -4.0043, -1.3846,\n",
       "          -1.0009, -0.6828, -3.2038, -4.8597,  0.6266, -3.6161, -3.7216,\n",
       "          -3.5843, -2.2755, -3.9846, -1.2852, -5.2079, -2.0455, -1.4619,\n",
       "           7.0367, -1.5216, -2.4856, -1.1797, -4.4474, -2.2430, -2.2546,\n",
       "          -1.4891, -3.5570, -1.2047, -0.2842, -1.8798, -1.3267, -0.4516,\n",
       "          -0.3417, -1.5330, -0.9727, -1.6755, -1.9910, -0.4008, -3.8658,\n",
       "          -0.5314, -4.6410, -4.1227, -3.5439, -2.0581,  0.0547, -2.5001,\n",
       "          -2.9420,  1.0225, -3.5922, -1.3187, -2.2193, -2.8824, -0.0386,\n",
       "          -1.0355,  2.4982, -2.7982, -3.4882, -2.1288, -4.7673, -3.3740,\n",
       "          -4.7016, -1.5258, -2.0962, -0.2289, -2.7652, -3.6555, -1.3713,\n",
       "          -2.5181, -1.3434, -0.2759, -3.8927, -2.8960, -0.3261, -2.5964,\n",
       "          -3.0790, -1.6057, -1.8425,  0.1460, -0.9039, -1.5485, -2.4929,\n",
       "          -4.3711, -2.3016, -3.5627, -1.1182, -1.6007, -5.1231, -3.0806,\n",
       "          -3.0576, -3.7470, -0.2158,  0.8879, -3.2896, -3.9851, -1.7416,\n",
       "          -4.9267, -3.5209, -3.3572, -3.2785, -2.6327, -2.5645,  0.6000,\n",
       "          -2.1178, -2.6745, -3.9984, -2.7897, -5.0074, -2.7254, -2.7487,\n",
       "          -0.3466, -4.4992, -0.7854, -2.5838, -2.0815, -2.9000, -2.7879,\n",
       "          -2.9369, -3.7099, -3.8852, -4.8258, -2.3163, -0.2856, -4.1565,\n",
       "          -2.2483, -6.9151, -0.5414, -1.7882, -3.6384,  0.8883, -2.1491,\n",
       "          -1.3721, -2.1947, -5.2771, -1.9821, -2.4926, -1.6949, -6.6289,\n",
       "          -6.0050, -3.1536, -3.2256, -2.5102]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the measure of error for produced message, used in NLP\n",
    "# change the size of window (\"last few chars\"): 5, 10, 20,? ****\n",
    "# display the matrix of logits\n",
    "# what to do to see how well can we do anomalies with that? feed some never seen sounds, or change \n",
    "# categories: music vs voice? \n",
    "# how is it at edge detection: silence vs noise? then prediction of what happens next? when the silence ends\n",
    "# example: baby cried longer than usual\n",
    "# trick where you use these numbers as an embedding (see word2vec, snip2vec): we project things\n",
    "# to that dimension, use it as a featurizer for a normal predictor (eg classification)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËËËËËËËËËËËËËËËËËËËËËbbbbbbËËËËËËËbbbbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbbËËËËËËËËËËËËËbbËËËbËËËËËÚËËËËËbbËËbËËËËËËËbËËËËËËËËËËËËËËbËËËËËËbËÔËËËbËËËËËbËËËËËbËËËËËËËËËËbbËËËËËËËËbËËËËbËËËËËËËËËËËËËËËËËËËËËËËËÆËËËËËËËËbbbËËËËËËbbbbËËËËËËËËËbbbbbËËËËËËËËËbbbbbbbbËËËËbbbbËËËbbbbËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËbËËËËËËËËËËËË\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËÉÉÉËbbbbËËbbbbbËËËËËËËËbbËËËËËËËbbbbbËËËËËbbbbbËÉËËËËËËËËËËËËËßËËËËËËËËËËËbbËËË\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "\n",
      "\n",
      "Aihnnnaee      : bbbbb\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbâbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbúÉbbbbbbbbbbbbbbbbËËËÉÉËËËbbbbËËËbbbbbbbbbbbbbËËËËËËËËËË\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbSbbbbbbbbttÔÔÔÔbbbbbbbbbbËËËËËbbËËËËËËËËÉËËËËÉtËËËËËËËËËËËËËËbbbbËËËËËËËbËÄðËËËËËËbËËËËËËËËbËbËËËËāËbËËbbbbbbbbËËËbbbbbbbËËËËbbbbbbËËËbËËËËbbbbbbbbbbbbbbbbËbbËËËËbËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËË\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbËbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËÉËËËËËËËËËËËËËbËËËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËbËËËËËËËËËËËËÉËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËbËËËËËËËËËËËËËËËËËËËËËbËËËËËËËËËËbbËËËbbËËËËËËËËËËËËËËËËËËÚËËËËËËËËËËËbËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbb\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "---------------\n",
      "abbbbbbbbbbbbbbbbbĖïbbbbbbbbbbÄbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbËËËËËÉbbbbbËbbËËbËËËËËËÉÉËtËËËËËËËËËËbËËËËËËËbËËËËËËËËËËËËËËËËËËbËËËËËðËËËËËËËËËËËËËËËbËËËËËËËËËËËËbÔËËËËËËËbbbËbbËËËËËËËËËËËËbbbbbËËbËbbbbËËËËËËËËËËËËËËbËËËËËbbbbbËËËËbËbËËËËËËËËËbbËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËÉËËËËËËËËËËËËËËËËËÉËËËËËËËËËËbËËËËËËbËËËËËËËbËËËËËbËËbËËËÉËËËËËËbËËËËËËËËËËËËßËËËËbËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËËbËËbËËËËËËËÓËËËËËbbËËbbbbbËbbbbbbbbËËËbËËbËËbbbbbbbbbbbbbbbbËËËËËËËËbbËËËb\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.wte.weight',\n",
       "              tensor([[-5.0581e-02, -4.2051e-02, -8.0691e-02,  ...,  4.8803e-02,\n",
       "                        2.3874e-02, -3.5259e-02],\n",
       "                      [ 4.7397e-05,  8.8309e-02,  1.1242e-01,  ...,  1.1734e-01,\n",
       "                        5.2131e-02,  3.6257e-02],\n",
       "                      [ 8.7407e-02, -1.4090e-02,  4.2185e-02,  ...,  1.2650e-01,\n",
       "                       -7.7602e-02, -8.1872e-02],\n",
       "                      ...,\n",
       "                      [-3.9811e-02,  4.0846e-02, -6.2443e-02,  ..., -7.2490e-03,\n",
       "                       -9.5569e-03, -6.4395e-02],\n",
       "                      [-4.8087e-02,  3.5425e-03, -8.7879e-02,  ..., -6.5736e-02,\n",
       "                        7.4864e-02,  4.1747e-02],\n",
       "                      [ 5.8965e-02, -3.6151e-02,  8.3334e-02,  ...,  9.0242e-03,\n",
       "                       -3.5748e-02, -5.6006e-02]])),\n",
       "             ('transformer.wpe.weight',\n",
       "              tensor([[-0.0238, -0.0416,  0.0019,  ..., -0.0441,  0.0381,  0.0108],\n",
       "                      [ 0.0102, -0.0258,  0.0144,  ...,  0.0562, -0.0063,  0.0133],\n",
       "                      [-0.0263,  0.0206,  0.0197,  ..., -0.0341,  0.0424, -0.0072],\n",
       "                      ...,\n",
       "                      [-0.0109,  0.0316, -0.0042,  ..., -0.0120, -0.0068,  0.0094],\n",
       "                      [ 0.0100,  0.0165,  0.0090,  ..., -0.0097,  0.0158,  0.0018],\n",
       "                      [-0.0129,  0.0324, -0.0130,  ..., -0.0023,  0.0211,  0.0118]])),\n",
       "             ('transformer.h.0.ln_1.weight',\n",
       "              tensor([0.9435, 0.9592, 0.9799, 0.9703, 0.9817, 0.9842, 0.9583, 0.9529, 1.0241,\n",
       "                      0.9953, 0.9690, 0.9683, 0.9797, 0.9929, 0.9589, 0.9843, 0.9681, 0.9562,\n",
       "                      0.9998, 0.9449, 0.9940, 0.9952, 0.9726, 0.9474, 0.9798, 0.9784, 0.9897,\n",
       "                      0.9622, 0.9523, 0.9598, 0.9691, 0.9794, 0.9942, 0.9635, 0.9525, 0.9788,\n",
       "                      0.9644, 0.9828, 0.9880, 0.9692, 1.0069, 0.9863, 1.0027, 0.9611, 0.9661,\n",
       "                      0.9988, 0.9486, 0.9973, 0.9845, 0.9917, 0.9265, 0.9441, 0.9475, 0.9846,\n",
       "                      0.9739, 0.9828, 1.0102, 0.9980, 0.9951, 0.9454, 0.9738, 0.9900, 0.9468,\n",
       "                      0.9730, 0.9666, 0.9562, 0.9621, 0.9690, 0.9634, 1.0085, 0.9871, 0.9903,\n",
       "                      0.9965, 0.9925, 0.9875, 0.9654, 0.9688, 0.9669, 0.9541, 0.9928, 0.9617,\n",
       "                      0.9747, 0.9783, 0.9642, 0.9791, 0.9693, 1.0045, 0.9504, 0.9768, 1.0337,\n",
       "                      0.9930, 0.9628, 0.9982, 0.9960, 0.9739, 0.9763, 0.9772, 0.9985, 0.9722,\n",
       "                      0.9974, 0.9868, 0.9698, 0.9850, 0.9453, 0.9526, 0.9909, 0.9650, 0.9778,\n",
       "                      0.9593, 0.9779, 0.9923, 0.9490, 1.0072, 0.9704, 0.9981, 0.9948, 0.9985,\n",
       "                      0.9584, 0.9740, 1.0015, 0.9879, 0.9689, 0.9841, 0.9684, 0.9754, 0.9755,\n",
       "                      0.9765, 1.0029])),\n",
       "             ('transformer.h.0.attn.c_attn.weight',\n",
       "              tensor([[-0.0199,  0.0394, -0.0145,  ..., -0.0196,  0.0555, -0.0761],\n",
       "                      [-0.0067, -0.0093,  0.0326,  ..., -0.0085, -0.0143, -0.0674],\n",
       "                      [ 0.0689,  0.0314,  0.0073,  ..., -0.0626, -0.0309,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0321, -0.0219,  ...,  0.0026,  0.0698,  0.0006],\n",
       "                      [ 0.0215, -0.0028, -0.0290,  ..., -0.0134,  0.0296, -0.0541],\n",
       "                      [-0.0253,  0.0100, -0.0141,  ...,  0.0111,  0.0143,  0.0471]])),\n",
       "             ('transformer.h.0.attn.c_proj.weight',\n",
       "              tensor([[-0.0054,  0.0011, -0.0185,  ...,  0.0043, -0.0036, -0.0141],\n",
       "                      [ 0.0122,  0.0006, -0.0025,  ..., -0.0033, -0.0060,  0.0093],\n",
       "                      [-0.0108,  0.0078, -0.0087,  ...,  0.0024,  0.0047, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0194,  0.0114, -0.0214,  ..., -0.0130, -0.0054, -0.0002],\n",
       "                      [ 0.0350,  0.0032, -0.0056,  ...,  0.0114,  0.0033,  0.0116],\n",
       "                      [-0.0224,  0.0174,  0.0057,  ..., -0.0187, -0.0195,  0.0125]])),\n",
       "             ('transformer.h.0.ln_2.weight',\n",
       "              tensor([1.0417, 1.0406, 1.0246, 1.0283, 1.0308, 1.0303, 1.0203, 1.0337, 1.0705,\n",
       "                      1.0268, 1.0277, 1.0751, 1.0393, 1.0247, 1.0320, 1.0176, 1.0639, 1.0248,\n",
       "                      1.0257, 1.0260, 1.0465, 1.0213, 1.0303, 1.0247, 1.0389, 1.0561, 1.0393,\n",
       "                      1.0424, 0.9962, 1.0372, 1.0164, 1.0716, 1.0582, 1.0173, 0.9995, 1.0135,\n",
       "                      1.0032, 1.0160, 1.0661, 1.0446, 1.0061, 1.0299, 1.0384, 1.0509, 1.0321,\n",
       "                      1.0686, 1.0221, 1.0444, 1.0774, 1.0315, 1.0360, 1.0381, 1.0295, 0.9988,\n",
       "                      1.0151, 1.0420, 1.0412, 1.0448, 1.0525, 1.0281, 1.0501, 1.0209, 1.0045,\n",
       "                      1.0113, 1.0245, 1.0460, 1.0794, 1.0707, 0.9978, 1.0355, 1.0468, 1.0203,\n",
       "                      1.0508, 1.0316, 1.0539, 1.0358, 1.0350, 1.0221, 1.0515, 1.0472, 1.0284,\n",
       "                      1.0191, 1.0247, 1.0400, 1.0345, 1.0485, 1.0690, 1.0260, 1.0363, 1.0768,\n",
       "                      1.0478, 1.0482, 1.0672, 1.0673, 1.0460, 1.0240, 1.0283, 1.0428, 1.0265,\n",
       "                      1.0507, 1.0100, 1.0510, 1.0779, 1.0258, 1.0286, 1.0290, 1.0397, 1.0282,\n",
       "                      1.0393, 1.0240, 1.0315, 1.0197, 1.0293, 1.0527, 1.0368, 1.0694, 1.0488,\n",
       "                      1.0315, 1.0345, 1.0689, 1.0187, 1.0291, 1.0261, 1.0496, 1.0547, 1.0371,\n",
       "                      1.0461, 1.0387])),\n",
       "             ('transformer.h.0.mlp.c_fc.weight',\n",
       "              tensor([[-0.0221, -0.0426,  0.0131,  ...,  0.0092, -0.0170,  0.0303],\n",
       "                      [ 0.0075, -0.0059, -0.0291,  ...,  0.0141,  0.0134, -0.0158],\n",
       "                      [-0.0321,  0.0124,  0.0839,  ..., -0.0226,  0.0512, -0.0397],\n",
       "                      ...,\n",
       "                      [ 0.0060, -0.0037, -0.0114,  ..., -0.0247,  0.0188,  0.0153],\n",
       "                      [-0.0109,  0.0228,  0.0220,  ..., -0.0020,  0.0041, -0.0391],\n",
       "                      [-0.0351, -0.0312,  0.0118,  ..., -0.0381,  0.0489,  0.0080]])),\n",
       "             ('transformer.h.0.mlp.c_proj.weight',\n",
       "              tensor([[-0.0089,  0.0101, -0.0005,  ...,  0.0237, -0.0044, -0.0256],\n",
       "                      [-0.0250, -0.0097,  0.0084,  ...,  0.0189,  0.0086, -0.0111],\n",
       "                      [ 0.0052, -0.0232,  0.0414,  ...,  0.0059, -0.0097,  0.0281],\n",
       "                      ...,\n",
       "                      [-0.0212,  0.0026,  0.0239,  ...,  0.0227,  0.0119, -0.0215],\n",
       "                      [-0.0046,  0.0225,  0.0456,  ...,  0.0016, -0.0136,  0.0399],\n",
       "                      [-0.0012, -0.0006, -0.0178,  ...,  0.0119,  0.0070,  0.0056]])),\n",
       "             ('transformer.h.1.ln_1.weight',\n",
       "              tensor([0.9565, 0.9850, 0.9371, 0.9716, 0.9722, 0.9659, 0.9781, 0.9510, 0.9333,\n",
       "                      0.9683, 0.9653, 0.9766, 0.9725, 0.9631, 0.9670, 0.9534, 0.9689, 0.9805,\n",
       "                      0.9984, 0.9957, 0.9495, 0.9770, 0.9628, 0.9461, 0.9650, 0.9898, 1.0038,\n",
       "                      0.9851, 0.9621, 0.9520, 0.9738, 0.9871, 0.9723, 0.9464, 0.9563, 0.9669,\n",
       "                      0.9872, 0.9825, 0.9565, 0.9674, 0.9357, 0.9672, 0.9524, 0.9597, 0.9621,\n",
       "                      0.9795, 0.9723, 1.0009, 0.9549, 0.9730, 0.9346, 0.9609, 0.9733, 0.9568,\n",
       "                      0.9766, 0.9920, 0.9542, 0.9491, 0.9749, 0.9366, 0.9751, 0.9682, 0.9912,\n",
       "                      0.9513, 0.9777, 0.9410, 0.9593, 0.9774, 0.9617, 0.9549, 0.9725, 0.9546,\n",
       "                      0.9809, 0.9719, 0.9653, 0.9558, 0.9841, 0.9695, 0.9807, 0.9830, 0.9652,\n",
       "                      0.9730, 0.9749, 0.9841, 0.9854, 0.9727, 0.9575, 0.9670, 0.9754, 0.9539,\n",
       "                      1.0065, 0.9733, 0.9619, 0.9708, 0.9656, 0.9475, 0.9683, 0.9552, 0.9732,\n",
       "                      0.9397, 0.9842, 0.9582, 0.9729, 0.9598, 0.9849, 0.9509, 0.9783, 0.9727,\n",
       "                      0.9938, 0.9926, 0.9805, 0.9465, 0.9615, 0.9718, 0.9982, 0.9472, 0.9509,\n",
       "                      0.9722, 0.9765, 0.9914, 0.9837, 0.9573, 0.9759, 0.9529, 0.9802, 0.9465,\n",
       "                      0.9667, 0.9549])),\n",
       "             ('transformer.h.1.attn.c_attn.weight',\n",
       "              tensor([[-0.0308,  0.0396,  0.0117,  ...,  0.0400,  0.0465,  0.0818],\n",
       "                      [ 0.0620, -0.0163,  0.0313,  ..., -0.0173, -0.0258, -0.0555],\n",
       "                      [-0.0507, -0.0125, -0.0530,  ...,  0.0011,  0.0242,  0.0191],\n",
       "                      ...,\n",
       "                      [ 0.0265,  0.0144, -0.0187,  ...,  0.0322,  0.0208,  0.0181],\n",
       "                      [ 0.0251, -0.0142, -0.0453,  ...,  0.0280,  0.0171, -0.0067],\n",
       "                      [-0.0231,  0.0050,  0.0115,  ...,  0.0488,  0.0260, -0.0062]])),\n",
       "             ('transformer.h.1.attn.c_proj.weight',\n",
       "              tensor([[-0.0171, -0.0082, -0.0019,  ...,  0.0355,  0.0244, -0.0113],\n",
       "                      [ 0.0026,  0.0080,  0.0086,  ...,  0.0197, -0.0038, -0.0011],\n",
       "                      [ 0.0031,  0.0030, -0.0038,  ...,  0.0049, -0.0008, -0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0203, -0.0147,  0.0034,  ...,  0.0058,  0.0140,  0.0147],\n",
       "                      [ 0.0162, -0.0024,  0.0017,  ..., -0.0114,  0.0126, -0.0034],\n",
       "                      [-0.0042,  0.0240,  0.0086,  ...,  0.0040, -0.0039, -0.0217]])),\n",
       "             ('transformer.h.1.ln_2.weight',\n",
       "              tensor([0.9672, 0.9613, 0.9569, 0.9594, 0.9501, 0.9528, 0.9512, 0.9706, 0.9951,\n",
       "                      0.9495, 0.9796, 0.9710, 0.9481, 0.9444, 0.9700, 0.9669, 0.9719, 0.9625,\n",
       "                      0.9369, 0.9560, 0.9706, 0.9460, 0.9587, 0.9472, 0.9446, 0.9692, 0.9474,\n",
       "                      0.9576, 0.9635, 0.9366, 0.9552, 0.9802, 0.9717, 0.9553, 0.9585, 0.9401,\n",
       "                      0.9599, 0.9537, 0.9837, 0.9659, 0.9514, 0.9542, 0.9641, 0.9738, 0.9335,\n",
       "                      0.9841, 0.9775, 0.9506, 1.0100, 0.9675, 0.9680, 0.9547, 0.9445, 0.9407,\n",
       "                      0.9638, 0.9648, 0.9576, 0.9410, 0.9638, 0.9854, 0.9772, 0.9516, 0.9412,\n",
       "                      0.9291, 0.9646, 0.9437, 0.9917, 0.9773, 0.9620, 0.9398, 0.9475, 0.9671,\n",
       "                      0.9300, 0.9363, 0.9475, 0.9955, 0.9726, 0.9604, 0.9734, 0.9546, 0.9278,\n",
       "                      0.9543, 0.9564, 0.9511, 0.9601, 0.9615, 0.9761, 0.9568, 0.9590, 0.9633,\n",
       "                      0.9781, 0.9344, 0.9844, 0.9748, 0.9671, 0.9346, 0.9552, 0.9769, 0.9467,\n",
       "                      0.9758, 0.9570, 0.9731, 0.9634, 0.9665, 0.9613, 0.9632, 0.9507, 0.9476,\n",
       "                      0.9712, 0.9614, 0.9779, 0.9462, 0.9532, 0.9737, 0.9763, 0.9666, 0.9657,\n",
       "                      0.9565, 0.9601, 0.9625, 0.9568, 0.9626, 0.9822, 0.9782, 0.9872, 0.9764,\n",
       "                      0.9759, 0.9647])),\n",
       "             ('transformer.h.1.mlp.c_fc.weight',\n",
       "              tensor([[ 1.0627e-02,  3.4408e-03, -1.3961e-02,  ...,  2.6035e-03,\n",
       "                       -9.7762e-03,  1.8813e-03],\n",
       "                      [ 3.0909e-02,  1.4197e-02,  1.1519e-03,  ...,  2.8625e-02,\n",
       "                        1.6084e-02, -2.0211e-02],\n",
       "                      [ 8.0256e-03, -4.6361e-02, -1.6036e-02,  ...,  2.0558e-02,\n",
       "                       -2.2286e-02, -1.1436e-03],\n",
       "                      ...,\n",
       "                      [ 4.3223e-03, -1.4391e-02, -7.1105e-03,  ...,  3.9449e-02,\n",
       "                       -2.8258e-02, -2.7791e-02],\n",
       "                      [ 2.9225e-03, -6.6936e-03,  2.0110e-02,  ..., -9.3072e-03,\n",
       "                        3.4455e-04, -9.2545e-03],\n",
       "                      [ 2.8413e-02,  1.5855e-02,  1.8239e-03,  ...,  3.1460e-05,\n",
       "                        2.5043e-03, -6.5527e-03]])),\n",
       "             ('transformer.h.1.mlp.c_proj.weight',\n",
       "              tensor([[-3.5245e-03, -4.2559e-06,  1.8280e-02,  ...,  1.0680e-02,\n",
       "                        1.1742e-02,  4.1718e-03],\n",
       "                      [ 1.4894e-02, -1.6198e-03,  6.4094e-03,  ..., -8.6810e-04,\n",
       "                       -7.4812e-03,  1.3919e-02],\n",
       "                      [-8.6863e-03,  3.9188e-03, -2.4556e-03,  ..., -1.1072e-02,\n",
       "                        5.1611e-03, -9.0114e-03],\n",
       "                      ...,\n",
       "                      [-8.6602e-03,  3.6243e-03,  1.0647e-03,  ..., -2.3237e-03,\n",
       "                       -1.1738e-02, -5.1085e-04],\n",
       "                      [-1.1318e-02, -7.0486e-04, -1.1512e-02,  ..., -1.3692e-02,\n",
       "                       -8.8559e-03,  2.1342e-03],\n",
       "                      [-3.4614e-03, -1.7972e-03,  1.7232e-02,  ..., -1.0305e-03,\n",
       "                       -2.0126e-03,  4.9307e-03]])),\n",
       "             ('transformer.h.2.ln_1.weight',\n",
       "              tensor([0.9706, 0.9786, 0.9379, 0.9611, 0.9652, 0.9545, 0.9578, 0.9539, 0.9618,\n",
       "                      0.9815, 0.9463, 0.9803, 0.9486, 0.9424, 0.9915, 0.9550, 0.9343, 0.9639,\n",
       "                      0.9568, 0.9779, 0.9453, 0.9653, 0.9263, 0.9362, 0.9450, 0.9698, 0.9532,\n",
       "                      0.9547, 0.9441, 0.9390, 0.9775, 0.9649, 0.9505, 0.9662, 0.9358, 0.9745,\n",
       "                      0.9630, 0.9431, 0.9605, 0.9679, 0.9706, 0.9754, 0.9554, 0.9585, 0.9506,\n",
       "                      0.9588, 0.9536, 0.9825, 0.9404, 0.9467, 0.9568, 0.9372, 0.9708, 0.9448,\n",
       "                      0.9654, 0.9658, 0.9732, 0.9504, 0.9768, 0.9604, 0.9675, 0.9500, 0.9770,\n",
       "                      0.9315, 0.9711, 0.9625, 0.9541, 0.9441, 0.9450, 0.9484, 0.9537, 0.9340,\n",
       "                      0.9640, 0.9786, 0.9572, 0.9648, 0.9860, 0.9819, 0.9707, 0.9777, 0.9457,\n",
       "                      0.9495, 0.9547, 0.9933, 0.9678, 0.9384, 0.9755, 0.9654, 0.9529, 0.9437,\n",
       "                      0.9896, 0.9491, 0.9333, 0.9710, 0.9653, 0.9415, 0.9541, 0.9557, 0.9407,\n",
       "                      0.9635, 0.9521, 0.9756, 0.9640, 0.9559, 0.9446, 0.9658, 0.9383, 0.9345,\n",
       "                      0.9537, 0.9740, 0.9454, 0.9756, 0.9735, 0.9755, 0.9465, 0.9605, 0.9400,\n",
       "                      0.9508, 0.9740, 0.9482, 0.9570, 0.9475, 0.9731, 0.9411, 0.9536, 0.9470,\n",
       "                      0.9586, 0.9381])),\n",
       "             ('transformer.h.2.attn.c_attn.weight',\n",
       "              tensor([[-0.0342, -0.0250, -0.0264,  ..., -0.0052, -0.0121,  0.0245],\n",
       "                      [ 0.0402, -0.0481,  0.0020,  ..., -0.0165, -0.0407, -0.0589],\n",
       "                      [ 0.0522, -0.0458, -0.0432,  ..., -0.0394,  0.0063, -0.0068],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0011,  0.0078,  ..., -0.0180,  0.0238,  0.0148],\n",
       "                      [-0.0174,  0.0198,  0.0094,  ..., -0.0060, -0.0355,  0.0273],\n",
       "                      [ 0.0126, -0.0283,  0.0019,  ..., -0.0010,  0.0072,  0.0196]])),\n",
       "             ('transformer.h.2.attn.c_proj.weight',\n",
       "              tensor([[ 0.0055,  0.0074,  0.0030,  ..., -0.0031,  0.0133, -0.0166],\n",
       "                      [-0.0050,  0.0087, -0.0008,  ..., -0.0050, -0.0035, -0.0034],\n",
       "                      [ 0.0031,  0.0006,  0.0065,  ...,  0.0005,  0.0051, -0.0084],\n",
       "                      ...,\n",
       "                      [-0.0265,  0.0209,  0.0110,  ..., -0.0125, -0.0263,  0.0018],\n",
       "                      [ 0.0018, -0.0133, -0.0174,  ...,  0.0095,  0.0035, -0.0070],\n",
       "                      [ 0.0041, -0.0008,  0.0005,  ..., -0.0122, -0.0071,  0.0053]])),\n",
       "             ('transformer.h.2.ln_2.weight',\n",
       "              tensor([0.9627, 0.9519, 0.9309, 0.9663, 0.9175, 0.9363, 0.9472, 0.9392, 0.9374,\n",
       "                      0.9290, 0.9610, 0.9454, 0.9509, 0.9413, 0.9443, 0.9529, 0.9480, 0.9308,\n",
       "                      0.9323, 0.9475, 0.9541, 0.9207, 0.9397, 0.9140, 0.9456, 0.9362, 0.9168,\n",
       "                      0.9580, 0.9372, 0.9060, 0.9625, 0.9746, 0.9296, 0.9434, 0.9212, 0.9355,\n",
       "                      0.9384, 0.9407, 0.9555, 0.9455, 0.9449, 0.9593, 0.9413, 0.9541, 0.9155,\n",
       "                      0.9634, 0.9508, 0.9464, 0.9578, 0.9300, 0.9353, 0.9424, 0.9416, 0.9237,\n",
       "                      0.9239, 0.9302, 0.9541, 0.9480, 0.9375, 0.9662, 0.9385, 0.9446, 0.9327,\n",
       "                      0.9143, 0.9464, 0.9246, 0.9598, 0.9660, 0.9418, 0.9430, 0.9308, 0.9395,\n",
       "                      0.9397, 0.9383, 0.9593, 0.9601, 0.9684, 0.9318, 0.9559, 0.9622, 0.9477,\n",
       "                      0.9194, 0.9392, 0.9502, 0.9455, 0.9192, 0.9599, 0.9412, 0.9657, 0.9309,\n",
       "                      0.9722, 0.9486, 0.9510, 0.9373, 0.9552, 0.9238, 0.9347, 0.9645, 0.9273,\n",
       "                      0.9445, 0.9462, 0.9500, 0.9282, 0.9547, 0.9425, 0.9384, 0.9342, 0.9322,\n",
       "                      0.9497, 0.9446, 0.9430, 0.9303, 0.9137, 0.9591, 0.9509, 0.9400, 0.9413,\n",
       "                      0.9490, 0.9656, 0.9368, 0.9403, 0.9383, 0.9494, 0.9764, 0.9568, 0.9599,\n",
       "                      0.9449, 0.9410])),\n",
       "             ('transformer.h.2.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0022,  0.0025, -0.0053,  ...,  0.0416, -0.0262, -0.0093],\n",
       "                      [-0.0026,  0.0045,  0.0149,  ..., -0.0583, -0.0286, -0.0029],\n",
       "                      [ 0.0112,  0.0100,  0.0161,  ..., -0.0109, -0.0272,  0.0473],\n",
       "                      ...,\n",
       "                      [ 0.0201, -0.0379,  0.0157,  ..., -0.0157, -0.0066, -0.0291],\n",
       "                      [-0.0083,  0.0067, -0.0102,  ...,  0.0043,  0.0226, -0.0111],\n",
       "                      [ 0.0161,  0.0164, -0.0120,  ...,  0.0458,  0.0182, -0.0111]])),\n",
       "             ('transformer.h.2.mlp.c_proj.weight',\n",
       "              tensor([[ 0.0019, -0.0065,  0.0092,  ..., -0.0003, -0.0110, -0.0007],\n",
       "                      [-0.0102, -0.0015,  0.0040,  ..., -0.0053, -0.0039,  0.0065],\n",
       "                      [ 0.0031,  0.0168,  0.0005,  ..., -0.0022, -0.0110, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0164,  0.0120,  ..., -0.0027, -0.0201,  0.0012],\n",
       "                      [ 0.0078, -0.0079, -0.0063,  ..., -0.0084,  0.0095, -0.0054],\n",
       "                      [ 0.0115,  0.0138,  0.0086,  ..., -0.0087,  0.0135,  0.0031]])),\n",
       "             ('transformer.h.3.ln_1.weight',\n",
       "              tensor([0.9687, 0.9385, 0.9867, 0.9685, 0.9937, 0.9586, 0.9515, 0.9712, 0.9557,\n",
       "                      0.9887, 0.9393, 0.9792, 0.9612, 0.9500, 0.9622, 0.9671, 0.9373, 0.9790,\n",
       "                      0.9535, 0.9847, 0.9383, 0.9591, 0.9318, 0.9357, 0.9591, 0.9876, 0.9265,\n",
       "                      0.9840, 0.9577, 0.9659, 1.0042, 0.9729, 0.9318, 0.9388, 0.9346, 0.9652,\n",
       "                      0.9819, 0.9511, 0.9551, 0.9439, 0.9708, 0.9703, 0.9651, 0.9836, 0.9795,\n",
       "                      0.9420, 0.9671, 0.9614, 0.9556, 0.9585, 0.9395, 0.9406, 0.9784, 0.9666,\n",
       "                      0.9326, 0.9773, 0.9751, 0.9716, 0.9540, 0.9518, 0.9598, 0.9785, 0.9847,\n",
       "                      0.9348, 0.9761, 0.9608, 0.9818, 0.9532, 0.9412, 0.9626, 0.9575, 0.9672,\n",
       "                      0.9819, 0.9618, 0.9486, 0.9513, 0.9493, 0.9491, 0.9628, 0.9768, 0.9620,\n",
       "                      0.9546, 0.9594, 0.9797, 0.9656, 0.9781, 0.9733, 0.9498, 0.9993, 0.9749,\n",
       "                      0.9591, 0.9699, 0.9530, 0.9681, 0.9527, 0.9582, 0.9883, 0.9820, 0.9485,\n",
       "                      0.9296, 0.9619, 0.9981, 0.9824, 0.9685, 0.9597, 0.9231, 0.9623, 0.9449,\n",
       "                      0.9684, 0.9691, 0.9402, 0.9976, 0.9602, 0.9865, 0.9099, 0.9368, 0.9635,\n",
       "                      0.9720, 0.9603, 0.9485, 0.9589, 0.9636, 0.9982, 0.9726, 0.9569, 0.9667,\n",
       "                      0.9712, 0.9250])),\n",
       "             ('transformer.h.3.attn.c_attn.weight',\n",
       "              tensor([[-0.0276, -0.0273,  0.0024,  ...,  0.0169,  0.0904, -0.0020],\n",
       "                      [ 0.0562, -0.0165,  0.0153,  ..., -0.0093, -0.0216,  0.0202],\n",
       "                      [ 0.0307, -0.0248,  0.0600,  ...,  0.1294, -0.0126,  0.0214],\n",
       "                      ...,\n",
       "                      [ 0.0251,  0.0131, -0.0027,  ..., -0.0232, -0.0430,  0.0097],\n",
       "                      [ 0.0053,  0.0073,  0.0349,  ...,  0.0218,  0.0123, -0.0335],\n",
       "                      [ 0.0038,  0.0182, -0.0090,  ..., -0.0240,  0.0153, -0.0219]])),\n",
       "             ('transformer.h.3.attn.c_proj.weight',\n",
       "              tensor([[ 0.0175, -0.0180, -0.0007,  ..., -0.0107,  0.0282,  0.0106],\n",
       "                      [ 0.0054,  0.0107, -0.0045,  ...,  0.0081, -0.0006, -0.0070],\n",
       "                      [ 0.0014, -0.0080, -0.0079,  ..., -0.0014, -0.0054, -0.0017],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0086, -0.0242,  ...,  0.0068, -0.0023, -0.0011],\n",
       "                      [ 0.0048,  0.0009,  0.0130,  ...,  0.0063, -0.0039, -0.0032],\n",
       "                      [-0.0124,  0.0137, -0.0132,  ...,  0.0029, -0.0085, -0.0028]])),\n",
       "             ('transformer.h.3.ln_2.weight',\n",
       "              tensor([0.9455, 0.9562, 0.9307, 0.9457, 0.9461, 0.9631, 0.9370, 0.9545, 0.9717,\n",
       "                      0.9372, 0.9807, 0.9481, 0.9447, 0.9246, 0.9700, 0.9461, 0.9676, 0.9349,\n",
       "                      0.9403, 0.9773, 0.9873, 0.9388, 0.9532, 0.9277, 0.9691, 0.9545, 0.9430,\n",
       "                      0.9486, 0.9268, 0.9422, 0.9619, 0.9611, 0.9380, 0.9572, 0.9292, 0.9673,\n",
       "                      0.9393, 0.9297, 0.9592, 0.9481, 0.9617, 0.9588, 0.9396, 0.9668, 0.9504,\n",
       "                      0.9792, 0.9620, 0.9560, 0.9844, 0.9941, 0.9464, 0.9611, 0.9680, 0.9687,\n",
       "                      0.9592, 0.9208, 0.9555, 0.9170, 0.9461, 0.9470, 0.9133, 0.9682, 0.8971,\n",
       "                      0.9105, 0.9588, 0.9188, 0.9905, 0.9725, 0.9588, 0.9897, 0.9217, 0.9415,\n",
       "                      0.9513, 0.9538, 0.9724, 0.9665, 0.9957, 0.9600, 0.9574, 0.9589, 0.9434,\n",
       "                      0.9368, 0.9290, 0.9243, 0.9573, 0.9700, 0.9982, 0.9497, 0.9743, 0.9540,\n",
       "                      0.9806, 0.9562, 0.9549, 0.9561, 0.9439, 0.9369, 0.9630, 0.9758, 0.9559,\n",
       "                      0.9535, 0.9819, 0.9631, 0.9450, 0.9503, 0.9605, 0.9603, 0.9185, 0.9920,\n",
       "                      0.9778, 0.9698, 0.9527, 0.9379, 0.9537, 0.9565, 0.9566, 0.9732, 0.9775,\n",
       "                      0.9138, 0.9628, 0.9317, 0.9448, 0.9201, 0.9684, 0.9656, 1.0009, 0.9417,\n",
       "                      0.9554, 0.9695])),\n",
       "             ('transformer.h.3.mlp.c_fc.weight',\n",
       "              tensor([[ 0.0293, -0.0070,  0.0042,  ...,  0.0177, -0.0080,  0.0059],\n",
       "                      [ 0.0185, -0.0160, -0.0292,  ..., -0.0099, -0.0193, -0.0168],\n",
       "                      [ 0.0164,  0.0075, -0.0180,  ...,  0.0061,  0.0119, -0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0008, -0.0115,  ...,  0.0338, -0.0156, -0.0252],\n",
       "                      [ 0.0284, -0.0195,  0.0573,  ...,  0.0151, -0.0545,  0.0196],\n",
       "                      [ 0.0067, -0.0293,  0.0162,  ..., -0.0132, -0.0326,  0.0292]])),\n",
       "             ('transformer.h.3.mlp.c_proj.weight',\n",
       "              tensor([[-0.0054, -0.0095, -0.0029,  ...,  0.0015,  0.0126,  0.0040],\n",
       "                      [-0.0093,  0.0246, -0.0012,  ..., -0.0015,  0.0176,  0.0141],\n",
       "                      [-0.0056,  0.0103, -0.0088,  ...,  0.0148,  0.0005,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0015,  0.0257,  0.0018,  ...,  0.0055, -0.0060, -0.0060],\n",
       "                      [-0.0080, -0.0120, -0.0037,  ..., -0.0074, -0.0068,  0.0039],\n",
       "                      [ 0.0072,  0.0085,  0.0097,  ...,  0.0082,  0.0005, -0.0076]])),\n",
       "             ('transformer.ln_f.weight',\n",
       "              tensor([1.2194, 1.1178, 1.2444, 1.1676, 1.2084, 1.2661, 1.2404, 1.2022, 1.2694,\n",
       "                      1.2187, 1.2195, 1.2374, 1.2316, 1.2455, 1.2190, 1.2697, 1.2484, 1.1124,\n",
       "                      1.2064, 1.1850, 1.2699, 1.2325, 1.2252, 1.2365, 1.2506, 1.2516, 1.2361,\n",
       "                      1.2163, 1.2078, 1.2282, 1.2192, 1.1812, 1.2775, 1.2427, 1.2395, 1.2379,\n",
       "                      1.1293, 1.1639, 1.2010, 1.2167, 1.2069, 1.1480, 1.2291, 1.2423, 1.2032,\n",
       "                      1.2570, 1.1878, 1.2420, 1.2514, 1.2549, 1.2512, 1.2239, 1.2313, 1.1742,\n",
       "                      1.2395, 1.2279, 1.2648, 1.2360, 1.1995, 1.2258, 1.2006, 1.2388, 1.1762,\n",
       "                      1.2153, 1.1923, 1.2210, 1.2142, 1.2206, 1.2216, 1.1945, 1.2125, 1.2251,\n",
       "                      1.2323, 1.2172, 1.2334, 1.1986, 1.2497, 1.2386, 1.2274, 1.2182, 1.1788,\n",
       "                      1.1925, 1.1798, 1.2033, 1.2313, 1.2137, 1.2671, 1.1985, 1.2040, 1.2651,\n",
       "                      1.2137, 1.1521, 1.2354, 1.2064, 1.1763, 1.1876, 1.2359, 1.2072, 1.2612,\n",
       "                      1.2466, 1.2318, 1.2274, 1.2482, 1.2120, 1.1183, 1.2273, 1.2323, 1.2383,\n",
       "                      1.1287, 1.2122, 1.2163, 1.2643, 1.2418, 1.2464, 1.2457, 1.2286, 1.2422,\n",
       "                      1.1723, 1.1783, 1.2694, 1.1970, 1.2599, 1.1492, 1.2024, 1.2546, 1.2662,\n",
       "                      1.2269, 1.2297])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-5.0581e-02, -4.2051e-02, -8.0691e-02,  ...,  4.8803e-02,\n",
       "                        2.3874e-02, -3.5259e-02],\n",
       "                      [ 4.7397e-05,  8.8309e-02,  1.1242e-01,  ...,  1.1734e-01,\n",
       "                        5.2131e-02,  3.6257e-02],\n",
       "                      [ 8.7407e-02, -1.4090e-02,  4.2185e-02,  ...,  1.2650e-01,\n",
       "                       -7.7602e-02, -8.1872e-02],\n",
       "                      ...,\n",
       "                      [-3.9811e-02,  4.0846e-02, -6.2443e-02,  ..., -7.2490e-03,\n",
       "                       -9.5569e-03, -6.4395e-02],\n",
       "                      [-4.8087e-02,  3.5425e-03, -8.7879e-02,  ..., -6.5736e-02,\n",
       "                        7.4864e-02,  4.1747e-02],\n",
       "                      [ 5.8965e-02, -3.6151e-02,  8.3334e-02,  ...,  9.0242e-03,\n",
       "                       -3.5748e-02, -5.6006e-02]]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('transformer',\n",
       "               ModuleDict(\n",
       "                 (wte): Embedding(165, 128)\n",
       "                 (wpe): Embedding(64, 128)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "                 (h): ModuleList(\n",
       "                   (0-3): 4 x Block(\n",
       "                     (ln_1): LayerNorm()\n",
       "                     (attn): CausalSelfAttention(\n",
       "                       (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
       "                       (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "                       (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                       (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (ln_2): LayerNorm()\n",
       "                     (mlp): MLP(\n",
       "                       (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
       "                       (gelu): GELU(approximate='none')\n",
       "                       (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (ln_f): LayerNorm()\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=128, out_features=165, bias=False))]),\n",
       " 'config': GPTConfig(block_size=64, vocab_size=165, n_layer=4, n_head=4, n_embd=128, dropout=0.0, bias=False)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(165, 128)\n",
       "    (wpe): Embedding(64, 128)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=165, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drill_app_2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/dev/otosense/nanoGPT/model.py:171\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, idx, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 171\u001b[0m     device \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    172\u001b[0m     b, t \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39msize()\n\u001b[1;32m    173\u001b[0m     \u001b[39massert\u001b[39;00m t \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot forward sequence of length \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m, block size is only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the snips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import FilesOfZip\n",
    "# https://zenodo.org/records/2552860#.Xhdyj78o-V4\n",
    "s = FilesOfZip('/Users/Sylvain/Dropbox/kaggle/freesound/FSDKaggle2018.audio_train.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FSDKaggle2018.audio_train/00044347.wav',\n",
       " 'FSDKaggle2018.audio_train/001ca53d.wav',\n",
       " 'FSDKaggle2018.audio_train/002d256b.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import wrap_kvs\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import soundfile as sf\n",
    "\n",
    "def deserialize_wrt_suffix(self, k, v, assert_sr=44100):\n",
    "    if k.endswith('.csv'):\n",
    "        return pd.read_csv(BytesIO(v))\n",
    "    elif k.endswith('.wav'):\n",
    "        wf, sr = sf.read(BytesIO(v), dtype='int16')\n",
    "        assert sr == assert_sr, f\"samplerate of {k} was not {assert_sr}\"\n",
    "        return wf\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KaggleMeta = wrap_kvs(FilesOfZip, name='KaggleMeta', postget=deserialize_wrt_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = KaggleMeta('/Users/Sylvain/Dropbox/kaggle/freesound/FSDKaggle2018.audio_train.zip')\n",
    "metadata = KaggleMeta('/Users/Sylvain/Dropbox/kaggle/freesound/FSDKaggle2018.meta.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FSDKaggle2018.meta/test_post_competition_scoring_clips.csv',\n",
       " 'FSDKaggle2018.meta/train_post_competition.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(metadata.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>manually_verified</th>\n",
       "      <th>freesound_id</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00044347.wav</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "      <td>28739</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001ca53d.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "      <td>1</td>\n",
       "      <td>358827</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002d256b.wav</td>\n",
       "      <td>Trumpet</td>\n",
       "      <td>0</td>\n",
       "      <td>10897</td>\n",
       "      <td>Creative Commons 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0033e230.wav</td>\n",
       "      <td>Glockenspiel</td>\n",
       "      <td>1</td>\n",
       "      <td>325017</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00353774.wav</td>\n",
       "      <td>Cello</td>\n",
       "      <td>1</td>\n",
       "      <td>195688</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9468</th>\n",
       "      <td>ffec59fb.wav</td>\n",
       "      <td>Fireworks</td>\n",
       "      <td>0</td>\n",
       "      <td>343090</td>\n",
       "      <td>Creative Commons 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>fff37590.wav</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "      <td>33136</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>fff44ac6.wav</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>0</td>\n",
       "      <td>133674</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9471</th>\n",
       "      <td>fff6a13d.wav</td>\n",
       "      <td>Chime</td>\n",
       "      <td>0</td>\n",
       "      <td>14640</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9472</th>\n",
       "      <td>fff81f55.wav</td>\n",
       "      <td>Cough</td>\n",
       "      <td>1</td>\n",
       "      <td>19117</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname         label  manually_verified  freesound_id  \\\n",
       "0     00044347.wav        Hi-hat                  0         28739   \n",
       "1     001ca53d.wav     Saxophone                  1        358827   \n",
       "2     002d256b.wav       Trumpet                  0         10897   \n",
       "3     0033e230.wav  Glockenspiel                  1        325017   \n",
       "4     00353774.wav         Cello                  1        195688   \n",
       "...            ...           ...                ...           ...   \n",
       "9468  ffec59fb.wav     Fireworks                  0        343090   \n",
       "9469  fff37590.wav        Hi-hat                  0         33136   \n",
       "9470  fff44ac6.wav      Laughter                  0        133674   \n",
       "9471  fff6a13d.wav         Chime                  0         14640   \n",
       "9472  fff81f55.wav         Cough                  1         19117   \n",
       "\n",
       "                 license  \n",
       "0            Attribution  \n",
       "1            Attribution  \n",
       "2     Creative Commons 0  \n",
       "3            Attribution  \n",
       "4            Attribution  \n",
       "...                  ...  \n",
       "9468  Creative Commons 0  \n",
       "9469         Attribution  \n",
       "9470         Attribution  \n",
       "9471         Attribution  \n",
       "9472         Attribution  \n",
       "\n",
       "[9473 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['FSDKaggle2018.meta/train_post_competition.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omodel.core.spectro import get_wf_to_spectr_func\n",
    "_spectr_of_wf = get_wf_to_spectr_func()\n",
    "spectr_of_wf = lambda wf: _spectr_of_wf(wf).T\n",
    "\n",
    "def wf_tag_gen(df, name_to_wf, name_field='fname', tag_field='label'):\n",
    "    for name, tag in zip(train_df.fname, train_df.label):\n",
    "        yield name_to_wf(name), tag\n",
    "    \n",
    "def spectr_tag_gen(wf_tags, spectr_of_wf=spectr_of_wf):\n",
    "    for wf, tag in wf_tags:\n",
    "        for x in spectr_of_wf(wf):\n",
    "            yield list(x), tag\n",
    "            \n",
    "wf_tags = wf_tag_gen(train_df, lambda name: ka[f'audio_train/{name}'])\n",
    "spectr_tags = spectr_tag_gen(wf_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wf_tag_gen(df, name_to_wf, name_field='fname', tag_field='label'):\n",
    "    for name, tag in zip(train_df.fname, train_df.label):\n",
    "        yield name_to_wf(name), tag\n",
    "    \n",
    "def spectr_tag_gen(wf_tags, spectr_of_wf=spectr_of_wf):\n",
    "    for wf, tag in wf_tags:\n",
    "        for x in spectr_of_wf(wf):\n",
    "            yield list(x), tag\n",
    "            \n",
    "wf_tags = wf_tag_gen(train_df, lambda name: ka[f'audio_train/{name}'])\n",
    "spectr_tags = spectr_tag_gen(wf_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KvDataSource' from 'slang' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m \u001b[39mimport\u001b[39;00m KvDataSource, Snipper, ClassificationSnipper, fixed_step_chunker, ddir, dflt_snips_to_str\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassificationSnipper\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KvDataSource' from 'slang' (unknown location)"
     ]
    }
   ],
   "source": [
    "from slang import KvDataSource, Snipper, ClassificationSnipper, fixed_step_chunker, ddir, dflt_snips_to_str\n",
    "\n",
    "from slang import ClassificationSnipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snipper = ClassificationSnipper(wf_to_chks={'chk_size': 2048}, \n",
    "                                chk_to_fv={'n_components': 2}, \n",
    "                                fv_to_snip={'n_clusters': 5})\n",
    "snipper.fit_fv_to_snip(train_XX, train_yy)\n",
    "\n",
    "train_snips = np.array([snipper.fv_to_snip(fv) for fv in train_XX])\n",
    "test_snips = np.array([snipper.fv_to_snip(fv) for fv in test_XX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MyType' from 'atypes' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sylvain/Desktop/dev/otosense/nanoGPT/slang_try.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39modat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmdat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfreesounds\u001b[39;00m \u001b[39mimport\u001b[39;00m mk_dacc\n",
      "File \u001b[0;32m~/Desktop/dev/otosense/odat/odat/mdat/freesounds.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m@add_ipython_key_completions\u001b[39m  \u001b[39m# adds tab-completion of keys in ipython\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m@add_prefix_filtering\u001b[39m(\n\u001b[1;32m     29\u001b[0m     relativize_prefix\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )  \u001b[39m# adds the ability to filter by key prefix\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m@wrap_kvs\u001b[39m(postget\u001b[39m=\u001b[39mfreesounds_dataset_postget)  \u001b[39m# adds bytes-to-data decoding\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFreesoundsDataset\u001b[39;00m(FilesOfZip):\n\u001b[1;32m     33\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A base store for the freesounds audio dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m \u001b[39mimport\u001b[39;00m KvDataSource\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmk_dacc\u001b[39m(\n\u001b[1;32m     40\u001b[0m     raw_store\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, audio_key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maudio_train/\u001b[39m\u001b[39m'\u001b[39m, annots_key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_post_competition.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m raw_store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/dev/otosense/slang/slang/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"Signal transformers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mTools to transform signals into a structured language.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m KvDataSource, Snipper\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m snips_to_str \u001b[39mas\u001b[39;00m dflt_snips_to_str\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m ddir\n",
      "File \u001b[0;32m~/Desktop/dev/otosense/slang/slang/core.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_property\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Any, Optional, Mapping\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     Waveform,\n\u001b[1;32m     11\u001b[0m     Chunk,\n\u001b[1;32m     12\u001b[0m     Chunker,\n\u001b[1;32m     13\u001b[0m     Featurizer,\n\u001b[1;32m     14\u001b[0m     Quantizer,\n\u001b[1;32m     15\u001b[0m     Snip,\n\u001b[1;32m     16\u001b[0m     Snips,\n\u001b[1;32m     17\u001b[0m     FVs,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunkers\u001b[39;00m \u001b[39mimport\u001b[39;00m DFLT_CHUNKER\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeaturizers\u001b[39;00m \u001b[39mimport\u001b[39;00m DFLT_FEATURIZER, DFLT_QUANTIZER\n",
      "File \u001b[0;32m~/Desktop/dev/otosense/slang/slang/stypes.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TypeVar, Optional, Iterable, Sequence\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39matypes\u001b[39;00m \u001b[39mimport\u001b[39;00m MyType\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39matypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     FixedSizeSeq,\n\u001b[1;32m     16\u001b[0m     VarSizeSeq,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     Snipper,\n\u001b[1;32m     34\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MyType' from 'atypes' (unknown location)"
     ]
    }
   ],
   "source": [
    "from odat.mdat.freesounds import mk_dacc\n",
    "'/Users/sylvain/Dropbox/kaggle/freesound/freesounds_audio_dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/sylvain/Dropbox/kaggle/freesound/freesounds_audio_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config2py import config_getter\n",
    "\n",
    "zip_filepath = config_getter('freesounds_audio_dataset_local_zip_filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drill_app_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
